[
  {
    "objectID": "2024-Lecture-01.html",
    "href": "2024-Lecture-01.html",
    "title": "(2024) Lecture 1",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Lecture Upload Plans and Transition to Logistics\n\n\n\n\n\n\nLecture Materials & Uploads\nThe lecture content will be uploaded, including items that are already prepared. References to additional material will also be provided.\n\n\n\n\n\n\n\n\n\n\nLogistics Reminder\nLet’s transition to logistical discussions in the final 15 minutes of the session.\n\n\n\n\n\nUploads: The lecture content and references will be shared.\n\nLogistics: Administrative details will be addressed shortly.\n\n\n\n\n\n\n\n\nSupporting Context\nWhile this segment focuses on logistics, nuclear physics formulas like those below may appear in technical sections of the lecture:\n\nNuclear binding energy: E_b = (Zm_p + Nm_n - m_{nuc})c^2\n\nSemi-empirical mass formula: E_B = a_V A - a_S A^{2/3} - a_C \\frac{Z(Z-1)}{A^{1/3}} - a_A \\frac{(A-2Z)^2}{A} + \\delta(A,Z)\n\nRadioactive decay law: N(t) = N_0 e^{-\\lambda t}\n\n\n\n\n\n\nClarity: Separating logistical announcements from technical content improves readability.\n\nPreparation: References and uploads ensure access to critical materials.\n\n\n\n0.2 The Evolution of the Universe: From Electroweak Epoch to QCD and Hadron Formation\nRight.\nWe are living in a universe made of hardened physics, but it was not always the case. Essentially, I like thinking of the evolution of the universe on a timescale. Here is the origin of the universe where everything started. There was a Big Bang first, and then several epochs followed. It started with the Planck epoch, then there was the electroweak epoch, quarks, and others. What has been happening during this is part of the physics that we also go through in the courses.\n\n\n\n\n\n\n\nHiggs Mechanism and Symmetry Breaking\nThe Higgs potential—the field giving mass to particles and ensuring protons don’t decay so life can exist—obtains a condensate during the electroweak epoch. The potential is given by:\n  \nV(\\phi) = \\mu^2 |\\phi|^2 + \\lambda |\\phi|^4  \n\nwhere \\phi is the Higgs field, \\mu^2 &lt; 0 triggers symmetry breaking, and \\lambda is the self-coupling constant.\n\n\n\nEssentially, let me start with the electroweak epoch, where the Higgs potential forms a condensate. The field that drives electroweak physics fills the entire space, creating a condensate of the Higgs field. Through this condensate, particles are slowed down, gain inertia, and obtain mass—this is the Higgs mechanism. The process of the potential evolving is called spontaneous symmetry breaking, where one of the minima is peaked. That’s how I like thinking about the electroweak stage.\n\nThen, particles have masses, the universe starts evolving, and at some point, we arrive at the quark epoch, where the universe is filled with a quark-gluon plasma. Essentially, quarks and gluons make an unstructured soup of objects—quarks, gluons, and more. There is nothing about structure formation yet; it’s just quarks disturbing. There is no scale to discuss yet. Quarks and gluons are together, and the universe is extremely hot. From that moment, it starts cooling down. At some point, the temperature is low enough that this medium starts separating, and we arrive at the quantum chromodynamics (QCD) epoch.\n\n\n\n\n\n\n\nNuclear Size Scaling\nThe radius R of a nucleus scales with the number of nucleons A as:\n  \nR \\approx r_0 A^{1/3}  \n\nwhere r_0 \\approx 1.2 \\, \\text{fm} (Fermi).\n\n\n\nThis medium starts forming small volumes where quark and gluon interactions bind to a scalable level. If you think of this, you’d need two years to learn it in detail. This comes in particle physics, introductory nuclear physics, and specialized courses on QCD or statistical mechanics. You could have a special course on QCD at finite temperature, since it’s a heated medium with different laws applied to what we’ll discuss for strong interactions.\n\nHydrogen is what we’ll focus on now. But if you think of how quickly this happened, you’ll be slightly surprised. Essentially, this is… do you have a guess? 10^{-36} seconds (Planck), 10^{-12} (electroweak), 10^{-6}, then 10^{-3}, and finally 1 second. Imagine there was nothing, and then the time it takes for sound to reach you when I clap—about 10 milliseconds—by that time, we’d already passed through all these stages and everything had formed. That’s how quickly the universe evolved.\n\nThe rest of the timeline to the present 13.8 billion years has other interesting physics, but not as intense from the perspective of hadron physics. Hadrons are objects where strong interactions are confined to a scale of a few femtometers (10^{-15} \\, \\text{m}). There’s essentially nothing around—just small chunks of volume where quarks and gluons interact. By colliding hadrons, we study their internal structure, but there’s no natural scale smaller than this.\n\n\n\n\n\n\n\nProton Mass Decomposition\nThe proton mass m_p is dominated by QCD energy (gluon condensate):\n  \nm_p \\approx 938 \\, \\text{MeV}, \\quad \\text{with} \\quad m_{\\text{quarks}} \\sim 9 \\, \\text{MeV} \\quad (\\text{only} \\sim 1\\%)  \n\nThe remainder arises from strong interaction energy.\n\n\n\nConcerning empty space, let’s sketch one of the most common elements on Earth. From the periodic table, what is it? Oxygen. Oxygen-16 (or 18) has 8 protons and 8 (or 10) neutrons. The geometry of this atom is a core of protons and neutrons, with electrons compensating the charge. The nucleus scales as the number of nucleons to the power of 1/3 (volume scaling). Packing 16 nucleons gives a radius of about 3 fm. Electrons, however, have wave functions 3 orders of magnitude larger (~50 picometers). This shows how empty space is—a tiny core surrounded by vast electron space with no strong interactions.\n\nNow, the fundamental hardware: the Standard Model describes most known interactions (excluding neutrinos’ open questions). The part responsible for strong interactions is quantum chromodynamics (QCD). The participants are quarks and gluons, the only particles carrying color charge—similar to electric charge but distinct. Color charge is a quantum number; we describe it via color wave functions.\n\n\n\n\n\n\n\nQCD and QED Lagrangians\nThe Lagrangian density for QCD is:\n  \n\\mathcal{L}_{\\text{QCD}} = \\bar{\\psi}_i (i \\gamma^\\mu D_\\mu - m) \\psi_i - \\frac{1}{4} G^a_{\\mu\\nu} G_a^{\\mu\\nu}  \n\nwhere D_\\mu = \\partial_\\mu - i g_s A_\\mu^a T^a and G^a_{\\mu\\nu} is the gluon field strength.\nFor QED:\n  \n\\mathcal{L}_{\\text{QED}} = \\bar{\\psi} (i \\gamma^\\mu D_\\mu - m) \\psi - \\frac{1}{4} F_{\\mu\\nu} F^{\\mu\\nu}  \n\nwhere D_\\mu = \\partial_\\mu - i e A_\\mu.\n\n\n\nOther Standard Model particles (electrons, muons, taus, photons, neutrinos) don’t carry color charge and don’t interact strongly. Quarks come in six types: up (charge +2/3), down (-1/3), charm, strange, top, bottom. They’re grouped by generation (light: u/d; heavier: s/c; heaviest: t/b). Most matter is made of first-generation quarks. Strange quarks may affect neutron star equations of state. Charm and bottom are studied in high-energy collisions; the top quark’s lifetime is too short for hadronization.\n\nHadrons are classified as mesons (quark-antiquark) or baryons (three quarks). At the chiral symmetry breaking scale, gluons condense, dressing quarks and giving them effective mass. The proton’s mass is ~1 GeV, but the Higgs contributes only ~1% (3 MeV per quark); 99% comes from QCD energy (gluon condensate).\n\n\n\n\n\n\n\nNatural Units Conversion\nIn natural units (\\hbar = c = 1):\n  \n1 \\, \\text{GeV}^{-1} \\approx 0.197 \\, \\text{fm}, \\quad 1 \\, \\text{GeV} \\approx 1.783 \\times 10^{-27} \\, \\text{kg}  \n\nRelating energy, mass, and length scales.\n\n\n\nNow, quantum chromodynamics (QCD) and quantum electrodynamics (QED) share a Lagrangian framework. QED’s Lagrangian is:\n  \n\\mathcal{L}_{\\text{QED}} = \\bar{\\psi} (i \\gamma^\\mu D_\\mu - m) \\psi - \\frac{1}{4} F_{\\mu\\nu} F^{\\mu\\nu},  \n\nwhere D_\\mu = \\partial_\\mu - i e A_\\mu. Maxwell’s equations in covariant form are \\partial_\\mu F^{\\mu\\nu} = j^\\nu. QCD’s Lagrangian adds color charge:\n  \n\\mathcal{L}_{\\text{QCD}} = \\bar{\\psi}_i (i \\gamma^\\mu D_\\mu - m) \\psi_i - \\frac{1}{4} G^a_{\\mu\\nu} G_a^{\\mu\\nu},  \n\nwith D_\\mu = \\partial_\\mu - i g_s A_\\mu^a T^a and G^a_{\\mu\\nu} as the gluon field strength.\nNatural units (\\hbar = c = 1) simplify conversions: 1 \\, \\text{GeV}^{-1} \\approx 0.197 \\, \\text{fm}; 1 \\, \\text{GeV} \\approx 1.78 \\times 10^{-27} \\, \\text{kg}.\n\n\n0.3 Einstein Notation, Gauge Invariance, and Fermion Interactions in Field Theory\nAnother thing to note is that here I am explicitly using Einstein notation and summing over repeated indices.\n\n\\mu here, \\mu there—they are repeated.\n\nThis means that if I were precise, I should have written here a sum from \\mu=1 to 4 over all components.\n\nThe \\nu is a repeated index again, so we sum.\n\n\n\n\n\n\n\nSummation Example: For F_{\\mu\\nu}F^{\\mu\\nu}, expanding the sum yields 4 \\times 4 = 16 terms (4 components per index). If multiplied by another term with 4 components, the total becomes 16 \\times 4 = 64 terms.\n\n\n\n\nThe equation of motion for this Lagrangian is obtained using the Euler-Lagrange equation.\n\nYou might remember this from classical mechanics where we had dots, and the first term was the time derivative, the second was the derivative of the field.\n\nEssentially, we treat the Lagrangian as a function of the field derivatives and the fields themselves as independent objects.\n\nFirst, we differentiate with respect to the derivatives, and then with respect to the fields.\n\n\n\n\n\n\n\nKey Insight: In classical mechanics, these would be the momentum terms, but here they are field variables. The index \\mu is summed over—a simple summation.\n\n\n\nThe derivative is with respect to A_\\rho (another index convention). If I take this Lagrangian, where everything is defined, and use the Euler-Lagrange equation, I get the equation of motion describing how the field propagates through space. We know this gives Maxwell’s equations.\n\nNow let me introduce the second part. This was the gauge field. Let me label it 1, 2, 3, 4.\n\nThis is a less intuitive concept that you might not have seen before, but we will need it.\n\nThere are degrees of freedom that are not fixed for photons in the electromagnetic field, and there is an arbitrary gauge choice we can make.\n\n\n\n\n\n\n\nGauge Transformation:\n  \nA_\\mu \\rightarrow A'_\\mu = A_\\mu - \\partial_\\mu \\chi  \n\nwhere \\chi is an arbitrary scalar field. Under this, F_{\\mu\\nu} remains invariant: F'_{\\mu\\nu} = F_{\\mu\\nu}.\n\n\n\nThis is a symmetry of the Lagrangian—the gauge symmetry. If you derive Maxwell’s equations from the Lagrangian, you’ll get them up to a gauge term. By fixing the gauge, you recover the standard Maxwell equations.\n\nNow we introduce fermion fields.\n\nIn electrodynamics, our leptons are described by the Dirac field \\psi, a four-component spinor (not a four-vector).\n\nThe Lagrangian is a scalar quantity—no free indices—and is constructed from objects of different dimensionality.\n\n\n\n\n\n\n\nSpinor Properties:\n\n\\psi describes spin-1/2 particles with four components (particle + antiparticle).\n\nThe adjoint spinor is \\bar{\\psi} = \\psi^\\dagger \\gamma^0.\n\nThe Dirac Lagrangian:\n  \n\\mathcal{L}_{\\text{Dirac}} = \\bar{\\psi} (i \\gamma^\\mu \\partial_\\mu - m) \\psi  \n\n\n\n\n\nThe equation of motion is the Dirac equation: (i\\not{\\partial} - m)\\psi = 0 (using Feynman slash notation).\n\nTo introduce interactions, we use gauge invariance.\n\nFor fermions, gauge invariance means changing the phase of \\psi locally at every space-time point.\n\nThe tricky part is that \\partial_\\mu \\psi introduces extra terms under a space-dependent phase transformation.\n\n\n\n\n\n\n\nCovariant Derivative Fix:\nReplace \\partial_\\mu with D_\\mu = \\partial_\\mu + ieA_\\mu. Under simultaneous transformations of \\psi and A_\\mu, the Lagrangian remains invariant.\n\n\n\nThis is the essence of QED: interactions arise from requiring invariance under local phase transformations.\n\nThe same principle applies to other gauge theories (e.g., SU(2) for electroweak, SU(3) for QCD).\n\n\n\n\n\n\nKey Difference in QCD:\n\nGluons self-interact due to non-Abelian structure (terms like f^{abc} in G^a_{\\mu\\nu}).\n\nCoupling runs with energy: asymptotic freedom (weak at high energy) vs. confinement (strong at low energy).\n\n\n\n\n\nFinally, in perturbation theory, we compute scattering amplitudes using Feynman diagrams.\n\n\n\n\n\n\nCross-Section and Decay Width:\n\nCross-section:\n  \n\\sigma = \\frac{1}{2E_A 2E_B |v_A - v_B|} \\int |\\mathcal{M}|^2 \\, d\\Pi_{\\text{phase space}}  \n\nDecay width:\n  \n\\Gamma = \\frac{1}{2m} \\int |\\mathcal{M}|^2 \\, d\\Pi_{\\text{phase space}}  \n\n\n\n\n\nThese connect field theory to measurable quantities in experiments.\n\n\n0.4 Course Logistics, Exam Details, and Summer School Opportunities\n\n\n\n\n\n\nLecture & Exercise Schedule\nIt’s really nice to have a watch today. We are three minutes out, but let me quickly say logistics for the course.\n\nI have notes: 11 lectures and 11 exercises.\n\nA bit fewer exercises because we will have some breaks.\n\nFor Moodle, we put a calendar listing anticipated dates for lectures and exercises.\n\nCloser to the dates, you may discuss shifting exercise sessions to other days, but for now, we keep the schedule as it is and we’ll discuss later.\n\n\n\n\n\n\n\n\n\n\n\nAdmission & Format\nFor the exam, we would like to offer during the semester that we will check the points you gain through exercise sheets, and then 50% will admit you to the examination.\n\nThe exam is designed to be easy: you’ll receive problem sets in advance (e.g., 1–2 weeks before).\n\nYou’ll bring your solutions to the exam, and we’ll have a discussion.\n\nFocus: Working on exercise sheets during the semester is most important.\n\n\n\n\n\n\n\n\n\n\n\nHow to Submit & Receive Feedback\nNow, let me show you these beautiful exercise sheets.\n\nEach sheet has a barcode to scan and classwork (optional for now).\n\nHomework: 1–2 problems per assignment.\n\nSubmission: Write solutions, hand them in at the lecture, and retrieve marked sheets during Thursday exercise sessions.\n\nA box will be provided for submissions (include your name!).\n\n\n\n\n\n\n\n\n\n\n\nKey Timings\n\nLectures: Tuesday\n\nExercise Sessions: Thursday\n\nExcursion: Fully booked, but 1–2 spots may open.\n\nOffice Hours: Thursday, 9–10 AM (Office 13B, Building B).\n\n\n\n\n\n\n\n\n\n\n\nApply Soon!\nFor our field, several summer schools are happening this summer:\n\nZurich, Switzerland: 8 weeks\n\nKrakow, Poland: 4 weeks\n\nBochum: 2 weeks (for advanced students)\n\n\n\nApplications are open for a few more weeks.\n\n\n\n\n\n\n\n\n\n\nCourse Description Access\nFor the course description… Well, in order to proceed, just write an email, and I can forward you the announcement. I think I sent it to Fast Shaft, but I’m not sure it was propagated. These are super exciting events if you want to know more.\n\n\n\n\n\n\n\n\n\n\nNo physics formulas were discussed in this segment. The focus was on administrative and organizational details.\n\n\n\n\n\n0.5 Collaborative Problem-Solving, Resource Use, and Course Logistics\n\n\n\n\n\n\nCollaboration is encouraged! Feel free to discuss and solve the sheets together. Use any resources you find helpful—ChatGPT, books, or other materials. The problems are standard and designed for learning, not assessment. How you learn is up to you.\n\n\n\n\nThe lecturer follows several key texts:\n\nHudson Martin (first reference)\n\nMartin Schemer\n\nThomson’s Particle Physics (general particle physics)\n\nMartin Spearman (dedicated to Hadron Physics)\n\n\n\n\n\n\n\n\n\nExercise groups start this week with three class problems.\n\nSheets distribution: Two pages per sheet, 15 printed. Take one for yourself and extras for friends.\n\n“Before leaving, please take the sheets. We should stamp them together, but make sure you don’t miss one.”\n\n“Do you have a quarter of the sheet? Leave me this quarter before leaving.”\n\n\n\n\n\n\n\n\n\n\n\nSystem description:\n\nA spark, a balloon, and a pendulum with a sliding hanging point.\n\nLagrangian: Kinetic term minus potential term (\\mathcal{L} = T - V).\n\n\\dot{X}^2 term for the moving part.\n\nPendulum interaction and potential energy terms.\n\nGoal: Derive the Dirac equation and Maxwell’s equations.\n“This is a fun problem—I just dropped it on this table.”\n\n\n\n\n\n\n\n\n\n\n\n\n“Thank everyone who came today. Hope we survive to the end of the semester!”\n\nExercises are not obligatory, but “everyone has to do the work.”\n\n“You’re gonna have troubles to get people to the board because this is not private.”\n\n\n\n\n\n\n\n\n\n\n\nUpcoming focus:\n\nReintroducing the QED Lagrangian:\n\\mathcal{L}_{QED} = \\bar{\\psi}(i\\gamma^\\mu D_\\mu - m)\\psi - \\frac{1}{4}F_{\\mu\\nu}F^{\\mu\\nu}\nWhere D_\\mu = \\partial_\\mu + ieA_\\mu (covariant derivative).\nComparing QED (electromagnetic interactions) and QCD (added complexity).\n“Starting with a simpler case and explaining how to calculate terms is a good starting point.”"
  },
  {
    "objectID": "2024-Lecture-02.html",
    "href": "2024-Lecture-02.html",
    "title": "(2024) Lecture 2",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Symmetries in Hadron Physics and Estimating Hadronic Cross Sections\n\n\n\n\n\n\nRecap of Previous Lecture\nThe homework problems last week were deemed too easy, so we’ll increase the problem level this week. Some problems recur across courses—like Clebsch-Gordan coefficients, which appear in nuclear physics, particle physics, and quantum field theory. This week’s homework will include them, but they should be manageable.\n\n\n\n\nWe’ll explore:\n\nQCD with the SU group (as previously discussed).\n\nThe phenomenology of the SU flavor group, including:\n\nIsospin\n\nSU flavor by interchanging u/d and squarks.\n\nConnection to the Lorentz group, where SU(2) is a subgroup responsible for rotations.\n\nYou’ll see how spin addition relates to quark addition inside hadrons. This builds on quantum mechanics material you’ve already seen, and I hope it’s easy to follow.\n\n\n\n\n\n\nThis is a particularly enjoyable section because the spin algebra can be derived independently—just isolate yourself from books and the internet, and you can work through it elegantly.\n\n\n\n\nLet’s start with a question from last lecture:\nHow would you estimate the typical cross section of hadronic reactions?\nThe exact approach involves:\n\\sigma = \\frac{1}{F} \\int |\\mathcal{M}|^2 \\, d\\Phi\nwhere:\n\n\\sigma = cross section\n\nF = flux factor\n\n\\mathcal{M} = matrix element\n\nd\\Phi = phase space differential\n\nThis gives the exact value in inverse GeV (convertible to cm²).\n\nBut what’s the scale? We’re dealing with barns (1\\,\\text{barn} = 10^{-24}\\,\\text{cm}^2).\n\n\n\n\n\n\nKey Insight: The cross section represents the effective interaction area. For a proton (radius ~1\\,\\text{fm} = 10^{-13}\\,\\text{cm}):\n\\sigma \\approx \\pi r^2 = \\pi (10^{-13}\\,\\text{cm})^2 \\approx 3 \\times 10^{-26}\\,\\text{cm}^2 \\approx 30\\,\\text{mb}\n\n\n\nThis is the typical scale for hadronic reactions. The strong interaction is powerful but short-range—particles must overlap to interact.\n\n\n0.2 Charge Definitions in QED and QCD, and Introduction to SU(2) Group Theory\nThe second question addressed is how to define charge in QED and QCD.\n\n\n\n\n\n\nIn QED, the electric charge is physical and conserved. It corresponds to a symmetry, as per Noether’s theorem, which guarantees a conserved current. The charge is defined as the integral of the zero component of this current:\n Q = \\int d^3x \\, J^0 = \\int d^3x \\, \\psi^\\dagger \\psi. \n\n\n\nFor a Dirac field, \\bar{\\psi}\\psi involves conjugation with \\gamma^0, yielding \\psi^\\dagger \\psi—a positive quantity. This implies that if \\psi represents an electron, it has a positive charge. However, this is a convention:\n\n\n\n\n\n\nThe observed charge Q is matched to -e for electrons to align with experimental conventions. The current in QED is:\n J^\\mu = -e \\bar{\\psi} \\gamma^\\mu \\psi. \nThe -e ensures consistency with standard definitions, but the conservation of the quantity is the key takeaway from field theory.\n\n\n\n\nThe approach in QCD is analogous but involves color indices and generators of SU(3):\n\n\n\n\n\n\nThe QCD current for color charge is:\n J^\\mu_a = \\bar{\\psi} \\gamma^\\mu T_a \\psi, \nwhere T_a are the Gell-Mann matrices (generators of SU(3)). Unlike QED, quarks have eight color charges, corresponding to the gluon probes.\n\n\n\nFor example, given a quark field \\psi_{\\text{color}} = (1, 2, i\\sqrt{3}), its charge is an eight-component vector—each component depends on the gluon used to probe it.\n\n\n\n\n\n\nThis structure arises from group theory: QED uses U(1), while QCD uses SU(3). Today, we’ll also discuss SU(2), which shares similar principles.\n\n\n\n\nThe SU(2) group is fundamental in particle physics. It is a special unitary group of 2 \\times 2 matrices with determinant one:\n\n\n\n\n\n\nFor U \\in SU(2), the conditions are:\n U U^\\dagger = I \\quad \\text{and} \\quad U^\\dagger U = I. \nThe determinant is restricted to 1 (the “S” in SU(2)).\n\n\n\n\nInverse: If g \\in SU(2), then g^{-1} \\in SU(2).\n\nClosure: For g_1, g_2 \\in SU(2), g_1 g_2 \\in SU(2).\n\nIdentity: The identity matrix I is included.\n\nAny SU(2) element can be written as:\n U = \\exp\\left(-i \\alpha_a \\sigma_a / 2\\right), \nwhere \\sigma_a are the Pauli matrices:\n \\sigma_1 = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\sigma_2 = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}, \\quad \\sigma_3 = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}. \n\n\n\n\n\n\nFor nilpotent matrices, the exponential simplifies. For example:\n \\exp\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = I + \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}. \n\n\n\n\nIn quantum mechanics, spin operators J_i satisfy:\n [J_i, J_j] = i \\epsilon_{ijk} J_k. \nThe raising/lowering operators are:\n J_\\pm = J_1 \\pm i J_2, \nacting on states |j, m\\rangle as:\n J_\\pm |j, m\\rangle = \\sqrt{j(j+1) - m(m \\pm 1)} \\, |j, m \\pm 1\\rangle. \n\n\n\n\n\n\nThe eigenvalues are:\n J^2 |j, m\\rangle = j(j+1) |j, m\\rangle, \\quad J_3 |j, m\\rangle = m |j, m\\rangle. \n\n\n\nFor quark states (e.g., \\Xi particles), flavor symmetry treats u and d quarks similarly due to nearly equal masses (m_u \\approx m_d \\approx 3 \\text{MeV}).\n\n\n\n\n\n\nThe \\Xi_{cc} (with ccu or ccd) shows a puzzling mass difference, violating the expected SU(2) symmetry. This remains a hot topic in particle physics.\n\n\n\n\nWe’ve covered:\n\nCharge definitions in QED and QCD.\n\nSU(2) group theory and its generators.\n\nApplications to spin and flavor symmetry.\n\nNext, we’ll delve deeper into SU(2) representations and their role in particle physics.\n\n\n0.3 Symmetries of the QCD Lagrangian and Quark Mass Hierarchy\nWhen we speak about the symmetries of the Lagrangian of QCD, we have in mind the Lagrangian itself. It consists of two key terms:\n\nGluon term: Describes the dynamics of the gluon fields.\nQuark term: The Dirac term, which includes a mass parameter for the quarks.\n\n\n\n\n\n\n\nThe QCD Lagrangian is given by: \\mathcal{L}_{\\text{QCD}} = -\\frac{1}{4}G^{a}_{\\mu u}G^{a\\mu u} + \\sum_{f}\\bar{\\psi}_f(i\\gamma^\\mu D_\\mu - m_f)\\psi_f where:\n\nG^{a}_{\\mu u} is the gluon field strength tensor,\n\\psi_f represents quark fields of flavor f,\nm_f are the quark masses,\nD_\\mu is the covariant derivative.\n\n\n\n\n\nIf the quark masses are not the same, the mass term will break the symmetry. For the u and d quarks, since their masses are almost identical (m_u \\approx m_d), the symmetry is not broken by the mass terms, and this remains a good symmetry.\n\n\n\n\n\n\nThe mass term in the Lagrangian is: \\mathcal{L}_{\\text{mass}} = -\\sum_{f=u,d,s} m_f \\bar{\\psi}_f \\psi_f When m_u \\approx m_d, this preserves an approximate SU(2) isospin symmetry.\n\n\n\n\n\n0.4 Combining Isospin and Spin Representations in SU(2)\nGroup.\nThis is so ud. Let me sketch this like this: U to D. Rotation in the space is a good scene. Then my cascade particle would be in that case, if I consider U and D to be the same particle as the SSL where L is the number in the ISIS spin wave function. It’s clear that cascade.\n\n\n\n\n\n\n\nThe cascade particle is historically named due to its decay pattern. While its original name is difficult to pronounce (except by Greek speakers), it’s universally referred to as “cascade” in particle physics.\n\n\n\nLet me also introduce the language that you might not be familiar with: Cascade is the same as this guy. It’s just historically people saw this guy decaying in the cascade of particles. That’s why. And no one can actually pronounce this correctly except Greek people. So if you want to make sure you spell it correctly, call it cascade. Everyone calls it cascade.\nThe cascade particle is now a single particle in our consideration with respect to the strong interaction and has ISIS spin wave function. This is very similar to the spin. We again have a two-dimensional vector space in which we can act with the group elements.\n\n\n\n\n\n\nThe SU(2) Rotation Operator for isospin transformations is given by:\n  \nU = e^{-i \\alpha \\cdot \\mathbf{G}}  \n\nwhere \\mathbf{G} are the generators of SU(2) and \\alpha are rotation parameters.\n\n\n\nSo U is equal to e^{-i\\alpha G}. Then what is the charge of the group of this vector? Take it in right? You put it to the equation for the charge and then we get essentially now it’s clear what the spin stands for.\nIsospin is the same way as the spin would give one half for this and minus one. You get it right? Acting well, it’s a doublet corresponds to the two components. The dimension of the representation is 2, it corresponds to the spin or isospin 1/2. We just counted.\n\n\n\n\n\n\nThe Isospin Doublet Representation for u and d quarks is:\n  \n\\begin{pmatrix} u \\\\ d \\end{pmatrix}, \\quad \\text{isospin } I = \\frac{1}{2}  \n\n\n\n\nSo if I get a multiplet of four dimensions, what would be the isospin? Okay, let’s come back. Four dimensions is three half. If it’s 95 dimensions, it’ll be 47.\nNow you might wonder, wait a second, how do we get more than two? We’ve got two quarks, right? U and D. How do we get more dimensions if we are not…\nI mentioned here that in grouping mock quarks actually changes the group. We are going to consider a different group. It will be SU. We are not talking about SU. Now we stick with the SU and we only permute two unique quarks.\nHow do we get higher representations, higher dimensions, more particles? Exactly. The next word: combining particles, combining representations.\n\n\n\n\n\n\n\nThe Clebsch-Gordan Series for combining two spin-1/2 particles is:\n  \n\\frac{1}{2} \\otimes \\frac{1}{2} = 0 \\oplus 1  \n\n\n\n\nThe higher dimension of representations appear when I start combining considering particles more than one quark. Because in that case I have to deal with the polling situation. There is one system. There is another system with spin one half. But I want to consider them as a system, as a combined system. And I want to characterize the total system by the total either spin or isospin and the projection.\nIn that case for SU, we are still sitting with the SU2 group. You can think of the system as a system of two vectors. The total spin of the system has again constantly. And the total J is one of the numbers from…\nDon’t remember that. Just think of the vector you have in one hand, one vector with J1, another hand, another vector with G2. And you want to obtain some of them in our regular vector space. In our regular vector sense, you want to take one and then you stick the other one right.\nThe resulting vector can have different lengths depending on the angle between these two. But there are minimal and maximal values. And all values in between are possible, but they have to contain them. Essentially that’s what’s written here.\n\n\n\n\n\n\nThe General Spin Addition Rule is:\n  \nj_1 \\otimes j_2 = |j_1 - j_2| \\oplus \\cdots \\oplus (j_1 + j_2)  \n\nExample: For j_1 = 3, j_2 = 2:\n  \n3 \\otimes 2 = 1 \\oplus 2 \\oplus 3 \\oplus 4 \\oplus 5  \n\n\n\n\nThe minimum you obtain by combining in this direction and then maximum combining and all intermediate values are possible. What we have here is combining the 1/2. Let’s know the notations that we often use. That tells you you combine two vector spaces coming from the cross product to the irreducible representations.\nYou combine spin 1/2 and spin 1/2. The total system can have spin 0 or 1. Let’s do exactly another example. We combine spin 3 and spin 2. What do I get? Just anyone output 5 or 55 again? Yes, 5, 4, 3, 1, 2. Put plus.\nWhat amazed me is that the dimension on the left and right is the same. Can we count that? This would be common dimensional 3. Spin 3 is 7, spin 2 is 5. Here is 3, 5, 7, 9, 11. Can someone tell me if it works or not? It might.\n\n\n\n\n\n\nDimension Check for Representations:\nFor the combination 3 \\otimes 2:\n  \n\\text{Dim}(3) \\times \\text{Dim}(2) = 7 \\times 5 = 35  \n\n  \n\\text{Dim}(1) + \\text{Dim}(2) + \\text{Dim}(3) + \\text{Dim}(4) + \\text{Dim}(5) = 3 + 5 + 7 + 9 + 11 = 35  \n\n\n\n\nRight, 20, 35 on the left, 35 on the right. Isn’t it amazing? We just, I mean probably formally prove that by counting all intermediate states from one to another. But that tells you the way to check yourself if you want to combine spins and write it with recombinations.\nYou can always count dimensions and see what essentially we do when we map this to that. Here we have a vector. The basis in this dimension is the… Let’s say 1, 1. This is an example of the 7 into 7 and then this is 5. This is one unit vector in that dimension.\nIn that case I have a 35 dimensional vector which has blocks. The first block is 3, 3 is the second block, 3. The second block is 5, 7, 9, 11. So I have blocks. We map this to that.\nMoreover, what group theory tells you is that these blocks did not talk to each other. Once you put this exponent and then consider matrix that acts on these 35 dimensional spaces, it will rotate different components within the box. They won’t talk to each other. That’s called representation.\nFor us it’s different sectors that give a total spin. If I have a system with total spin equals to say three, there is no rotation that would change the spin to four. That’s super clear, right? Under rotations the spin is preserved.\nThe implication in mathematics is that there is a representation that has a block diagonal form of the interaction where the blocks don’t talk to each other. Now we know how to combine.\nBefore doing a few practical combinations, let’s introduce part in charge conjugation quickly. I think we are good.\n\n\n\n\n\n\n\nIs it clear that what we discussed about spin is the same applied to isospin? Often in particle physics you deal with both and one shouldn’t mix them up.\n\n\n\nWe deal with isospin when you construct the quark wave function. But when you combine particles with spin and want to understand angular dependence, you’re dealing with spin. With rotational group, the same algebra, the same methods, everything is the same.\nJust keep in mind whether you’re talking about flavor SU(2) or Lorentz part of the Lorentz Group rotational SU(2).\n\n\n0.5 Parity, Charge Conjugation, and Composite Systems in Particle States\nNumber 3, 4, 5 in our discussion: the parity operator performs space inversion with respect to the origin. Both parity and charge conjugation act on a state and produce another state.\n\n\n\n\n\n\nThe parity operator \\mathcal{P} inverts spatial coordinates:\n\\mathcal{P} \\psi(\\vec{r}) = \\eta_P \\psi(-\\vec{r}), \\quad \\eta_P = \\pm 1\nwhere \\eta_P is the intrinsic parity eigenvalue.\n\n\n\nThe parity answers how the wave function changes when flipped through the origin. Intuitively, flipping a vector through the origin reverses all spatial components. For any particle, you can determine its parity. Acting twice with parity returns the original state, so the eigenvalue is modulo 1 (conventionally \\pm 1).\n\nCharge conjugation flips all charges, converting a particle to its antiparticle.\n\n\n\n\n\n\nThe charge conjugation operator \\mathcal{C} acts as:\n\\mathcal{C} |\\psi\\rangle = \\eta_C |\\overline{\\psi}\\rangle, \\quad \\eta_C = \\pm 1\nFor neutral eigenstates (e.g., \\pi^0), \\eta_C is defined, but charged particles (e.g., \\pi^+) are not eigenstates since \\mathcal{C} \\pi^+ = \\pi^-.\n\n\n\nThe prime notation here indicates the antiparticle. By convention, charge conjugation is defined for the neutral member of a multiplet (e.g., \\pi^0). The Particle Data Group lists J^{PC} (spin, parity, charge conjugation) for all particles. For \\pi^0, J^{PC} = 0^{-+}; for \\pi^+, charge conjugation is assigned by analogy to \\pi^0.\n\nTo find J^{PC} for a composite system:\n\nTotal J: Use spin algebra.\n\nParity: Multiplicative, including (-1)^L for orbital angular momentum L.\n\nCharge Conjugation: Multiplicative (for neutral systems, C = (-1)^{L+S}).\n\n\n\n\n\n\n\nParity of a composite system:\nP = \\eta_{P1} \\eta_{P2} (-1)^L\nwhere \\eta_{P1}, \\eta_{P2} are intrinsic parities.\n\n\n\nExample: For L = 0, combine spins multiplicatively (e.g., 1/2 + 0 = 1/2). For L = 1, add one unit of angular momentum, with parity (-1)^{L+1}.\n\nIn the quark model, heavy quarks (e.g., charm) are treated without gluons. The u and d quarks form isospin multiplets:\n\nIsospin 1: Symmetric combination.\n\nIsospin 0: Antisymmetric combination (matched with antisymmetric spin wave function).\n\n\n\n\n\n\n\nFor baryons like \\Lambda_c (isospin-0), excitations include radial (n=1s, 2s) or orbital (P-wave, D-wave) modes. The Particle Data Group lists ~7 \\Lambda_c states, including ground and excited configurations.\n\n\n\n\nKey skill: Determine allowed L in decays.\n\n\n\n\n\n\nDecay selection rules: For A \\to B + C, allowed L satisfies:\n|J_B - J_C| \\leq L \\leq J_B + J_C \\quad \\text{and} \\quad P_A = P_B P_C (-1)^L.\nExample: 0^+ \\to 1^- + 2^- requires L=2 (D-wave) to conserve J^P.\n\n\n\n\nNeglecting spins, sketch the energy spectrum for n \\leq 3 (1s, 2s, 2p, 3s, 3d). Recall L &lt; n (no f-waves)."
  },
  {
    "objectID": "2024-Lecture-03.html",
    "href": "2024-Lecture-03.html",
    "title": "(2024) Lecture 3",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Recording and Transcribing Lectures with Whisper\nMy experiment with recording lectures has been relatively successful, so I could recover what I was speaking.\n\n\n\n\n\n\nSpeech-to-Text Technology: There is an open large language model from OpenAI called Whisper that can translate audio to text. Since the architecture is known, it runs even on CPUs in parallel, such as on a Mac. You just:\n\nDownload it\nExecute it And then you have a transcript of your speech.\n\n\n\n\n\nImportant notes about recording:\n\nThe lecture is recorded\nI do it for myself\nIt might be converted to a document\n\n\n\n\n\n\n\nCollaboration Opportunity: If someone would volunteer to:\n\nType the questions\nAdd the equations\nCheck them That would be helpful.\n\n\n\n\n\nI don’t know what to do with that yet, but just for fun and for exploring the technologies, I will keep the recordings.\nImportant limitation: It records only me in the sense that it mostly captures my voice. Don’t hesitate to talk back, as this will not appear in the recordings anyway.\n\n\n0.2 Isospin Assignments and Matrix Representations for Hadrons\nToday we will discuss structure functions, the structure of hadrons, their internal composition, and how we study them. Before diving deeper, let’s start with a recap and address a few questions. These questions will remain on the board for later discussion.\n\nQuestion 1: Without consulting the PDG, determine the isospin of these particles based on their quark content:\n\nc \\bar{s}\nb q\nb s\nq \\bar{q}\n\nQuestion 2: What is the dimensionality of the isospin matrix acting on a system of three quarks with two possible charges? If we treat the quarks as a wave function in two dimensions—where spin-up corresponds to the u-quark and spin-down to the d-quark—what is the size of this matrix?\nQuestion 3: What are the irreducible representations of this matrix’s action on the quarks? In other words, how does it decompose into independent blocks that do not interact?\n\nWhen discussing isospin, we focus on light quarks. The first meson, c \\bar{s}, contains no light quarks, so its isospin is 0.\nFor the B-mesons:\n\nIf the quark q is light (e.g., u or d), the isospin is \\frac{1}{2}.\nIf q is heavy (e.g., c or b), the isospin is 0.\n\nThe s-quark belongs to the light quarks in SU(3) but is excluded in isospin discussions. For the b s state, if s is the only light quark, the isospin is \\frac{1}{2}.\nFor a pentaquark with three light quarks, the isospin matches that of the \\Delta-baryons. The \\Delta states (\\Delta^{++}, \\Delta^+, \\Delta^0, \\Delta^-) have isospin \\frac{3}{2}.\n\n\n\n\n\n\n\nClarification: When assigning isospin to a particle, we refer to the length of the isospin vector, not its projection. For example, the B-meson’s isospin \\frac{1}{2} implies two charge states:\n\nB^+ = b \\bar{u}\nB^0 = b \\bar{d}\n\nThe antiparticles are fully conjugated:\n\nB^- = \\bar{b} u\n\\bar{B}^0 = \\bar{b} d\n\nThese form a separate triplet in the antiparticle space.\n\n\n\n\n\nIsospin assignments for mesons: \nI = \\begin{cases}\n0 & \\text{(no light quarks)} \\\\\n\\frac{1}{2} & \\text{(one light quark)} \\\\\n1 & \\text{(two light quarks)}\n\\end{cases}\n\nIsospin of the \\Delta-baryon: \nI_\\Delta = \\frac{3}{2}\n\nDimensionality of the isospin matrix for three quarks: \n\\text{dim}(\\mathbf{I}) = 8 \\times 8\n\nIrreducible representations for three quarks in SU(2): \n\\mathbf{2} \\otimes \\mathbf{2} \\otimes \\mathbf{2} = \\mathbf{4} \\oplus \\mathbf{2} \\oplus \\mathbf{2}\n\n\n\n\\mathbf{4}: Symmetric (spin-\\frac{3}{2}, like \\Delta).\n\\mathbf{2}: Mixed-symmetry (spin-\\frac{1}{2}).\n\n\nCharge states of B-mesons: \nB^+ = b \\bar{u}, \\quad B^0 = b \\bar{d}, \\quad B^- = \\bar{b} u, \\quad \\bar{B}^0 = \\bar{b} d\n\n\n\n\n0.3 Spin and Charge Combinations in Quark Systems\nNow, Cascade B has spin 1/2.\n\n1/2 means that the dimensionality is 2J + 1 = 2.\nThere are two particles of this type: Cascade B and Cascade B.\n\n\nNow, the charges. The quark charges are:\n\nLower row (D, S, B): Q = -\\frac{1}{3}\nUpper row (U, C, T): Q = +\\frac{2}{3}\n\nWhen I combine three lower-row quarks like DDD, the total charge is -1. If I combine three upper-row quarks like UUU, the total charge is +2. Intermediate combinations give other values.\n\nHere, I combined B and S.\n\nI almost have the full bottom row: B and S.\nIf I had another lower-row quark, the total charge would be -1, but instead, I have an upper-row quark, so the total charge is zero.\nThat’s why it’s zero here.\nIf you replace it with a lower-row quark, you get -1.\n\n\nNow we come to the last case, which has three quarks and several possible combinations.\n\nWhat is the spin of this state?\nWhat are the possible combinations for spin 1/2 (P-C plus and P-C zero)?\nFor spin 3/2, I will have P-C.\nThe 1/3 means four particles in the multiplet, like Delta plus-plus.\n\n\n\n\n\n\n\nThe dimensionality of the isospin matrix acting on three quarks is 2^3 = 8. We have an 8 \\times 8 matrix, but we can split it into irreducible representations.\n\n\n\n\nAn interesting question: What is the antiparticle for P-C plus?\n\nFor P-C plus, the antiparticle is not P-C minus—it’s P-C plus bar.\nFor baryons, it’s important to distinguish baryons from antibaryons because they have different quark components.\n\n\nNow, let’s discuss the irreducible representation of the three quarks. Combining three spin-1/2 particles:\n\nFirst, combine two spin-1/2 particles to get spin-0 or spin-1.\nThen, combine the result with the third spin-1/2 particle.\nThe dimensionality check: 2 \\times 2 \\times 2 = 8 matches 2 + 2 + 4.\n\n\nTo construct the basis for the highest isospin state, start with |uuu\\rangle and apply the lowering operator: \nJ_- |J, J\\rangle = \\sqrt{2J} \\, |J, J-1\\rangle\n For J = 3/2, this gives: \nJ_- |3/2, 3/2\\rangle = \\sqrt{3} \\, |3/2, 1/2\\rangle\n The normalized state is: \n|3/2, 1/2\\rangle = \\frac{1}{\\sqrt{3}}(|uud\\rangle + |udu\\rangle + |duu\\rangle)\n\n\nFor the spin-1/2 states, we construct orthogonal combinations: \n|1/2, 1/2\\rangle = \\frac{1}{\\sqrt{6}}(2|uud\\rangle - |udu\\rangle - |duu\\rangle)\n Applying the lowering operator gives: \n|1/2, -1/2\\rangle = \\frac{1}{\\sqrt{6}}(2|ddu\\rangle - |dud\\rangle - |udd\\rangle)\n\n\nThe same logic applies to higher spins.\n\nFor example, combining two spin-1 particles gives spin-0, spin-1, or spin-2.\nThe dimensionality check: 3 \\times 3 = 1 + 3 + 5.\n\nThis is standard spin algebra, applicable to both integer and half-integer spins. The key is to start with the highest state and use lowering operators to construct the rest. Normalization and orthogonality are ensured by Clebsch-Gordan coefficients.\n\nFor practical calculations, you can derive these coefficients using ladder operators. For example, the spin-1 triplet states are: \n|1, 1\\rangle = |\\uparrow \\uparrow\\rangle, \\quad |1, 0\\rangle = \\frac{1}{\\sqrt{2}}(|\\uparrow \\downarrow\\rangle + |\\downarrow \\uparrow\\rangle), \\quad |1, -1\\rangle = |\\downarrow \\downarrow\\rangle\n The singlet state is: \n|0, 0\\rangle = \\frac{1}{\\sqrt{2}}(|\\uparrow \\downarrow\\rangle - |\\downarrow \\uparrow\\rangle)\n\n\nThis method works for any spin combination. For three quarks, the irreducible representations are:\n\nA quartet (spin-3/2)\nTwo doublets (spin-1/2)\n\nThe same principles apply to isospin and flavor SU(3), though SU(3) is more complex. For SU(2), it’s straightforward spin addition. You need to be comfortable with:\n\nDimensionalities\nLowering operators\nOrthogonal combinations\n\n\nThis is foundational for:\n\nQuantum mechanics\nParticle physics\nGroup theory\n\nPractice is essential—start with simple cases and build up.\n\n\n0.4 Symmetries and Construction of the Proton Wave Function in SU(4) Space\nIn order to proceed and talk about structure functions, today in the lecture we need to get the proton wave function, because that’s something from which the understanding of the internal structure of the hadrons comes. In the homework, you will have other questions related to the delta’s internal structure.\n\nI would like to start with the symmetries of the baryon wave function. We have to operate in the SU(4) space. One of them is color, which we’ve already discussed. The baryon wave function would have color indices and must be color-neutral, as all hadrons are.\nWe have to work with:\n\nA space wave function that essentially describes how the wave function is distributed in space—its dependence on x and time t.\nIsospin\nSpin\n\n\n\n\n\n\n\nIt’s important to realize that, in the general case, they are not factorizable. You cannot say you have to deal with the product of the functions. The wave function lives in this product of the four spaces and can mix the spaces. You need to have a sum of the components in these spaces. It would be unfair to write, in the general case, isospin times the rest—not true in general.\n\n\n\n\nThe color wave function is a singlet, meaning it’s a number, a scalar; it doesn’t have dimensionality in components. Therefore, the color wave function can be factored out.\nThere is a good argument why the space wave function is a scalar and can be factored out as well. I haven’t convinced myself of that, but one should find the argument for it in the literature. That’s something I ask you to take as given—I don’t have a good justification.\nSo the wave function for coordinate dependence will also factorize. What remains is spin and isospin, and these two do not factorize. That’s a large-dimensional representation.\n\nWe are going to deal with baryons made of three quarks, and every quark has a spin. So we are still working with the same group, but now we have many dimensions. Every quark is the product of two: flavor and spin. And since it’s a product of three quarks, we are going to deal with a basis in six dimensions. It’s the same thing as before.\nTo build the representation of particles in these six dimensions, we are going to act with the lowering operator, starting with something we know—one we know with no ambiguities: the delta, which has spin \\frac{3}{2}.\nWhen the delta has spin \\frac{3}{2}, the only combination we can come up with is |\\!u\\!\\!\\uparrow u\\!\\!\\uparrow u\\!\\!\\uparrow \\rangle. The \\Delta^{++} with J_z = \\frac{3}{2} is obtained by acting with the lowering operator.\n\nWhat happens if we act with the lowering operator in the isospin space? G_- acting on |\\!u\\!\\!\\uparrow u\\!\\!\\uparrow u\\!\\!\\uparrow \\rangle.\n\nIf we act with the lowering operator in the spin space, we reduce the projection.\nIf we act with the lowering operator in the flavor space, we reduce the charge and obtain a different particle: the \\Delta^+.\n\nThen you can think about how to construct \\Delta^-, \\Delta^0 with J_z = \\frac{1}{2}. We need to act twice with the lowering operator in the flavor space and once with the lowering operator in the spin space.\n\nNow, the proton appears as a wave function that is orthogonal. So, \n\\frac{1}{2} \\otimes \\frac{1}{2} \\otimes \\frac{1}{2} = \\left( \\frac{3}{2} \\right) \\oplus \\left( \\frac{1}{2} \\right) \\oplus \\left( \\frac{1}{2} \\right)\n\nThis is 4 + 2 + 2 dimensions. The proton is the particle made of the same quarks as the delta but has isospin \\frac{1}{2} and spin \\frac{1}{2}. Therefore, we have to construct a wave function that is orthogonal in both spin and isospin spaces.\nThat’s one way—we’re not going to do that because it’s not super clear to you yet. I think it’s just technically complicated, but you would be able to do it if you sit and work on it for a few hours.\n\nWhat we are going to do is explore the symmetry. The baryon wave function in the symmetric configuration is symmetric. This is a new symmetry—we haven’t discussed this yet. It’s not related to SU(2) directly; it’s something new we’ve introduced.\nWe are now going to swap dimensions—just take one particle with its spin and isospin and swap it with another. This is sort of an external look at those symmetries. We have a basis function that we construct, and now we can examine whether they have certain permutation symmetry. We didn’t intend to construct them with built-in symmetry, and we will see that most of them have no symmetry—they are not eigenstates of permutation.\n\nBut before doing that, let’s see what we actually demand from the function. The total \\psi must be symmetric under permutation. So, \\psi_{\\text{antisymmetric}}(1,2,3) = -\\psi(2,1,3). Here, we essentially swap the dimensions of particle 1 and particle 2.\nThe color wave function is antisymmetric. To see this, you need to examine its form because this is a different group—color transforms under SU(3).\nIn three dimensions, there are three colors, and in that case, it’s not as simple as spin algebra. The representations of SU(3) are more complicated: there are decuplets, octets, and that comes from combining three quarks.\n\n\\mathbf{3} \\otimes \\mathbf{3} \\otimes \\mathbf{3} = \\mathbf{1} \\oplus \\mathbf{8} \\oplus \\mathbf{8} \\oplus \\mathbf{10}\n\nThese are dimensions—not spins. Three times three times three equals 1 + 8 + 8 + 10 (27 total).\nThe singlet is 1, and the wave function for the singlet can be constructed similarly to what we discussed. But you see that it’s easiest to start with the highest-dimensional representation (the 10) because for them, you would know.\nFor the 10, you would start with |RRR \\rangle and act with the lowering operator to get the full multiplet. Then you construct the octets—there are two of them—and for components like |RGB \\rangle, |RBG \\rangle, etc., you need to find combinations that match.\nBut the answer is essentially that the totally antisymmetric combination is: \n\\psi_{\\text{color}} = |RGB\\rangle - |RBG\\rangle + |GRB\\rangle - |GBR\\rangle + |BRG\\rangle - |BGR\\rangle\n\nIt’s easy to write down if you start with one term and add all permutations, where the sign depends on whether the permutation is odd or even. All even permutations have a plus sign, odd permutations have a minus sign. From this, you find that the color wave function is antisymmetric under permutation—if you swap two, you get a minus sign.\nThe space wave function is symmetric—we are talking about ground-state baryons, which are in the simplest symmetric configuration. Now, the spin + isospin wave function must be symmetric.\n\nLet’s do an example of the combinations we are allowed to work with. In order to construct a wave function for a proton, we have to combine it in the representation that has isospin one-half and spin one-half.\nLet’s examine some of these basis functions. Is this symmetric or antisymmetric under permutation? Well, it depends on which particles I pick. If I pick particle one and two and swap them, this doesn’t change. This one becomes D U U, and that one becomes U D D. The plus sign remains, and there is a plus D U D, D U U, and minus. So essentially, the function is different—it doesn’t have a definite symmetry under permutation; it just goes to different wave functions.\nHowever, it’s symmetric under permutations of quarks 2 and 3. If I take these two and permute them, these terms stay the same, and this series remains unchanged. So we construct a spin wave function that is symmetric only under the permutation of 2 and 3. And that’s what one observes.\nThe proton wave function can be essentially guessed by combining spin: combining \\frac{1}{2}, \\frac{1}{2} spin, \\frac{1}{2} in flavor—symmetric flavor and symmetric spin. Here we have symmetric, and that was antisymmetric. You see, under the permutation of the screen, if I swap 2 and 3, the wave function becomes that but with a minus sign. Therefore, it’s antisymmetric. Symmetric under 2-3, antisymmetric under 1-2.\nIn order to build the proton wave function, I take the flavor representation from here and combine it with the spin wave function. The spin wave function is from here again, but instead of U and D, I’m going to write up and down.\nWhat does this tell us? That it is not symmetric in the first two particles? It tells us that it’s illegal to use that. We cannot use it. It does not represent a valid baryon wave function. If we use just this term without that, it would be illegal. The point is that the wave function transforms into another wave function.\nWhen we combine three quarks, we get the spin-\\frac{3}{2} representation and two spin-\\frac{1}{2} representations. If I apply the permutation operator to this, it leaves the representation. So the matrix that permutes is no longer decomposable in this representation. Since this permutation operator is external to the group (it doesn’t belong to the rotation group), it’s not the same as rotating. Therefore, we effectively see that this multiplet mixes with this one. To construct the proton, we need both of these spin-\\frac{1}{2} dimensions, and that’s essentially what we do.\nOne more comment on the same line: The delta lives here. The delta is a valid baryon. If I apply the permutation to this multiplet, I stay within the same multiplet. But for the proton, it lives in the mixture of these two.\nNow, one has to do the algebra to find the answer, and then it’s going to be… So the proton wave function is going to be—I’ll write these components—the proton is spin-up, where one diagonal is going to be U U D, U D U, D U U, and off-diagonally done. If you guys are satisfied with that, you should recover it.\nThe other combination is up-up-down, up-down-up, down-up-up. The way I—this is what I just worked out, and that’s the answer for the matrix. I show the coefficients in the basis of U and D, in the basis of three quarks of different flavors with different spin orientations. Along the rows, I put the same quark content; among the columns, I have the same spin orientation. So here it’s going to be U D—same spin up-up-down, but then particle number is going to be U D U.\nAnd now, the normalization part quickly: all of them are orthogonal to each other, so the normalization just comes by summing these numbers straight: 4, 4, 4, 12 plus 6, 18. We have it—we have the wave function of the proton. Now we can evaluate the cool properties of the proton. That came as a surprise.\n\n\n\n\n\n\n\nThe proton wave function construction involves combining symmetric and antisymmetric components in spin and flavor spaces to satisfy the overall antisymmetry requirement due to the color wave function. This ensures the Pauli exclusion principle is obeyed for the quarks inside the baryon.\n\n\n\n\n\n0.5 Probing Proton Structure: Elastic Scattering, Form Factors, and the Three-Quark Model\nNow, let’s discuss three studies of structure. One of the ways we study and examine the structure is to use something we know very well. We take actually one of the simplest and first questions we can answer: What is the charge distribution inside the hadron? That’s done with the electron probing the charge.\nWhen we scatter an electron off a hadron in this reaction, almost all variables are fixed. Essentially, the center-of-mass energy is fixed, and then there is just one variable remaining that describes the entire kinematics: the angle. When I scatter two particles and the energy is fixed, there is just one angle.\n\nTherefore, the experiment on the structure functions—the experiment that helps us understand the proton structure—is electron-proton scattering, observing the probability of the electron going in different directions. With the scattering experiment, we measure the angular distribution. From this, we gain insights into the proton’s charge distribution and magnetic moment. That’s called a scattering experiment.\nIt’s important to realize we will write many variables, like the Q^2 momentum transfer, but it’s all encoded in one. Essentially, you need to know the probability for the electron to go forward slightly. The kinematics in the lab frame look like this: the proton collides with the electron, and then the electron goes in a certain direction.\n\nThis is a simple two-body scattering matrix, and the only variable that determines everything is the angle. So, what’s the probability for the electron to scatter? Will it have almost no change in direction, or will it go perpendicular? That determines the proton’s internal structure. It’s amazing to think this way.\nThose who took nuclear physics before can easily tell me: what is the most probable outcome? Where will the electron go—perpendicular, 30 degrees, 20 degrees? Do you have a feeling?\n\n\n\n\n\n\nThe Rutherford scattering formula for point-like particles is: \\frac{d\\sigma}{d\\Omega} \\propto \\frac{1}{\\sin^4(\\theta/2)} This explains the huge peak at small angles in the scattering probability.\n\n\n\nSomething you might remember is the \\frac{1}{Q^4} Rutherford scattering term, \\frac{1}{\\sin^4(\\theta/2)}. There’s a huge peak at around zero angle in the probability. Most of the time, the electron prefers to go straight with zero scattering angle. The deviation from that behavior tells us about the proton’s structure.\n\nThat process is called elastic scattering—where initial particles are the same as the final state particles. In contrast, inelastic scattering is another important process where we probe the proton’s structure, and the proton dissociates. Here, the proton stays intact in elastic scattering, but in inelastic scattering, the electron and proton produce many different particles.\nThe result for the differential cross-section is derived from non-relativistic scattering, and you get the same even for point-like particles. An example of a point-like particle is the electron—it has no internal structure. If you scatter electrons or muons, you get an ideal cross-section of \\frac{1}{\\sin^4(\\theta/2)}.\n\nElastic scattering has this huge peak at zero angle in the probability. If you calculate the diagrams for QED, you write the matrix element as \\bar{u}\\gamma^\\mu u, with propagators and spinors. For this course, we analyze it generally, so we won’t go into details, but it’s important to understand dimensionalities and the objects themselves. The matrix element is a scalar—it has no dimensionality, just a number obtained by contracting structures like spinors and gamma matrices.\nWhen dealing with the proton, we extend the vertex function with form factors. What’s the dimensionality of this object? It’s a simple function of Q^2. It’s convenient to introduce combinations of these form factors: the electric and magnetic form factors, G_E and G_M.\n\nIn non-relativistic theory, they have a straightforward interpretation—they show the charge and magnetic distributions in momentum space. We can transform these to coordinate space via Fourier transform. The normalization of these form factors is fixed:\n\nG_E(0) gives the total charge (1 for the proton),\nG_M(0) gives the magnetic moment.\n\nThe magnetic moment is a quantity that reacts to a magnetic field. For an electron, it’s \\frac{e}{2m_e} (spin 1/2). For the proton, it’s different due to its structure. Experimentally, the proton’s magnetic moment is about 2.79, not 1 as naively expected.\n\nThis discrepancy arises because the proton is made of quarks. In the quark model, we account for quark charges (\\frac{2}{3}, -\\frac{1}{3}) and masses. The proton’s magnetic moment comes from the wave function structure, and the result matches experiment surprisingly well. For the neutron, it’s -1.91, again showing internal structure.\nIn summary, just knowing SU(2) and spin algebra, we can build the proton wave function and understand its magnetic moment—a clear sign the proton isn’t point-like. In the homework, you’ll work with the Δ baryon’s wave function and magnetic moment operator.\nApologies for running late, but thank you for your attention. We spent a lot of time on these foundational concepts, but they’re crucial. I hope to discuss more in future lectures.\n\nAnother interesting thing here was the way to understand that there are three parts inside a proton. I remember from color? No, from scattering. From scattering, yeah. So how do you address three colors? Not three colors, but three quarks. But not three colors. Exactly. Three quarks.\nBecause—let’s think of the foil. So if protons they—correct. I think it’s also coming from various form factors. Form factors? From experiment. And you see the distribution of these form factors.\nNice. I don’t think the mission is the magnetic moment. I think because it really comes from some experimental distribution. But I’m not sure which exactly observable it was.\nI remember three generations of particles comes from the widths of the W boson. But this is—it’s completely different. Three colors? It’s the spectroscopy of the mesons. So that’s the Eightfold Way. Because at this time, I think it was only part red. But now three quarks in the problem.\nWhy does it—I think it’s related to colors because only with three you can get an octet and decuplet. Yeah, but I’m talking about experiments. Yeah, at the time they were not great. Good point. Good question.\n\n\n\n\n\n\n\nKey experimental values for magnetic moments:\n\nProton: \\mu_p \\approx 2.79 \\frac{e}{2m_p}\nNeutron: \\mu_n \\approx -1.91 \\frac{e}{2m_p} These values arise from the proton’s quark structure and wave function."
  },
  {
    "objectID": "2024-Lecture-04.html",
    "href": "2024-Lecture-04.html",
    "title": "(2024) Lecture 4",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Magnetic Moment of the Omega Baryon and Excitation Patterns of the Sigma Baryon\nToday’s lecture is dedicated to experiments in spectroscopy and the computations of kinematics for these experiments. But before proceeding, I would like to recap the last lecture and pose two questions:\nQuestion 1: Compute the magnetic moment in the current model of the Omega baryon.\nQuestion 2: Compute the lowest 1s multiplet excitation of the Σ particle parameter.\nFor the second question, I’m referring to isospin fermions. Let’s address this quickly because it’s straightforward.\n\nFirst, what’s the quark content of the Ω⁻? It consists of three strange quarks. What is the wave function? The total wave function is always a product of four parts, though not necessarily a direct product. It lives in the tensor product space of:\n\nColor coordinates\nIsospin\nSpin\nSpatial coordinates\n\nFlavor is trivial here, so we focus on spin. Two quarks should be spin-up, and one spin-down to match the half-spin behavior of the particle. But what is the total spin? The books state it’s 3/2, not 1/2. The parity is +, so for L=1, the spin would be 1/2. But in reality, J is 3/2 precisely.\nThe Ω⁻ exists in a J=3/2^+ state. What’s the multiplicity? For spin-3/2, how many projections are there?\n\n\n\n\n\n\nThe spin multiplicity for J=3/2 is given by: \n\\text{Multiplicity} = 2J + 1 = 4 \\quad \\text{(for } J=\\tfrac{3}{2}\\text{)}\n\n\n\n\nTo obtain all components, act with a lowering operator on the maximal projection state. For the magnetic moment calculation, we only need one component because the magnetic moment is proportional to spin.\nThe magnetic moment operator is: \n\\hat{\\mu}_z = \\sum_{i=1}^3 \\mu_{iz} = \\sum_{i=1}^3 g_i \\left(\\frac{e \\hbar}{2 m_i c}\\right) S_{iz}\n Here, the g-factor is the charge number. Since all quarks have charge -1/3, the total is -1. The magnetic moment is then: \n\\mu_{\\Omega^-} = -3 \\left(\\frac{e \\hbar}{2 m_s c}\\right)\n where m_s is the strange quark mass (~500 MeV).\nThe Ω⁻ mass is roughly three times the strange quark mass: \nM_{\\Omega^-} \\approx 3m_s \\approx 1500 \\text{ MeV}\n The magnetic moment is similar to the electron’s but 1000 times smaller due to the larger mass.\nKey takeaway: For simple wave functions, calculating the magnetic moment is straightforward. You only need the maximal projection state.\n\nThe ground state of the Σ baryon has:\n\nA light diquark in a spin-1 configuration\nA spin-1/2^+ heavy quark\n\nThe isospin multiplicity is: \n\\text{Multiplicity} = 2I + 1 = 3 \\quad \\text{(for } I=1\\text{)}\n This gives three charge partners: +1, 0, and -1.\nFor the excitation pattern, we combine the spins of:\n\nThe light diquark (spin-1)\nThe heavy quark (spin-1/2)\n\nFor the S-wave, we get two states: 1/2^+ and 3/2^+.\nFor the P-wave (L=1), combining spins gives: \nJ = \\tfrac{1}{2}^-, \\tfrac{3}{2}^-\n The multiplicity for the P-block is 5, while the S-block has 2 states.\nThe hyperfine splitting energy scale is suppressed by the heavy quark mass: \n\\Delta E_{\\text{hyperfine}} \\propto \\frac{1}{m_Q}\n\nThese states correspond to physical particles listed in the PDG. The Σ baryon (300) was discovered roughly 10 years ago.\n\n\n0.2 Heavy Quark Spin Effects, Exotic Hadrons, and Electron-Positron Collider Experiments\nNow one word about energy splitting. The energy splitting between this and that state is caused by the spin of the heavy quark. The radial excitation is the scale given by the quark dynamics, and this is the order of \\lambda_{\\text{QCD}}. So that’s one of the first lectures where we introduced that it’s where the strong coupling, the very strong coupling, diverges. It’s roughly a few hundred MeV, and that gives you the separation of the different blocks.\nWithin the block, the splitting between levels comes from the dynamics of the spin of the heavy quark. This quark is heavy, so all spin effects are suppressed inversely proportional to its mass. In the Lagrangian—and in all effective theories—the spin-orbit interaction enters in terms like \\frac{\\vec{S}_Q \\cdot \\vec{L}}{m_Q}, where the mass of the heavy quark is in the denominator. The mass of the b quark is about 4 GeV, so it gives an order-of-magnitude suppression.\n\\lambda_{\\text{QCD}} is a few hundred MeV, and the quark mass is a few GeV, so the ratio between them is roughly 1:10 or 1:15. That’s the difference between excitation energies: the general quantum numbers versus the splitting between states. For example, the energy difference between \\Sigma_b with spin 3/2 and \\Sigma_b with spin 1/2 is on the order of 10 MeV. Can someone check that? Does the PDG list \\Sigma_b states?\n\nToday’s lecture is dedicated to kinematics and experimental techniques. I’ll start by overviewing what we do in experimental physics. This field has been extremely vibrant and fruitful in the last 10 years. It began around 2004 when the first exotic particles appeared. Since then, every few months, new observations of states that don’t fit simple meson or baryon models have been reported. Many experiments have dedicated part of their programs to studying hadrons, especially exotic ones.\nSeveral large labs worldwide study hadron collisions. One key question is understanding how hadrons form: which combinations are possible, and what rules govern their excitation patterns and properties. Quantum chromodynamics (QCD) describes strong interactions well where we can compute, but we still can’t classify or predict large multiplets of exotic hadrons. This isn’t just due to computational limits—lattice QCD works well for ground states but struggles at this scale—but also because the theory is inherently complex.\nThe phenomena in QCD involve emergent degrees of freedom. The Lagrangian has quarks and gluons, but these aren’t the relevant degrees of freedom for hadrons. We’re seeing a transition between configurations where quark degrees of freedom dominate and those where hadrons themselves form larger structures, like atoms. In spectroscopy, you face this borderline: a mix of compact quark-based hadrons and sparse hadronic molecules.\nLattice QCD can’t yet help much here, so experiments must provide new data. We study hadrons by measuring their properties and decays, either in new decay configurations or from different production mechanisms.\n\nMany labs explore different production mechanisms:\n\nElectron-positron colliders:\nBelle (Japan) and BES (China) collide electrons and positrons, producing intermediate states that decay.\nCERN’s colliders study proton-proton collisions, producing long-lived particles like B and D mesons.\nThese travel millimeters before decaying, letting us distinguish primary and secondary vertices.\nHadronic production:\nFixed-target experiments like GlueX (Jefferson Lab, photon beam) and COMPASS (CERN, pion beam), both using hydrogen targets.\nLattice QCD computations:\nTheoretical approach to complement experimental data.\n\n\nLet’s overview experiments, starting with BES (Beijing Spectrometer). It’s at the Beijing Electron-Positron Collider (BEPC), colliding electrons and positrons. BES studies hadrons in the charmonium and tau regions. The collision produces a virtual photon, which couples to hadrons.\n\n\n\n\n\n\nQuantum number selection rules for e^+e^- \\to \\text{hadrons}: Electrons and positrons have spin-1/2, so their collision can only produce spin-1 states (not spin-0). The parity is also fixed, so BES explores hadrons with J^P = 1^-.\n\n\n\nBES operates in two modes:\n\nResonance peak: Collecting data at specific energies (e.g., J/\\psi peak).\nEnergy scan: Measuring cross-sections by tuning beam energy.\n\nThe cross-section \\sigma(e^+e^- \\to \\text{hadrons}) peaks at resonances like J/\\psi or \\psi(2S). Analyzing final states (e.g., \\pi^+\\pi^-\\pi^0) reveals hadron properties.\n\nBelle II (Japan) is another e^+e^- collider, originally for CP violation studies but also useful for spectroscopy. It’s at the SuperKEKB accelerator (upgraded from KEKB). Unlike symmetric colliders, Belle II uses asymmetric beam energies to boost produced B mesons.\nThe center-of-mass energy is set to the \\Upsilon(4S) resonance, which decays to B\\bar{B} pairs. The boost makes B mesons travel farther, improving vertex resolution. This design choice enhances sensitivity to CP violation.\n\n\nEnergy splitting: \n\\Delta E \\sim \\frac{\\lambda_{\\text{QCD}}}{m_Q}\n\nSpin-orbit interaction: \n\\mathcal{L}_{\\text{spin-orbit}} \\sim \\frac{\\vec{S}_Q \\cdot \\vec{L}}{m_Q}\n\nResonance cross-section: \n\\sigma \\propto \\frac{1}{(s - M_R^2)^2 + M_R^2 \\Gamma_R^2}\n\nBoosted decay length: \nL = \\gamma c\\tau \\approx \\frac{E}{m_B} c\\tau\n\n\n\nThe charmonium spectrum includes:\n\nJ/\\psi(1S) (1^-) and \\eta_c(1S) (0^-)\n\\psi(2S) (1^-)\nP-wave states: \\chi_{cJ} (J = 0,1,2, J^P = 0^+,1^+,2^+).\n\n\n\n0.3 Reconstructing e^+e^- Annihilation, Bottomonium Spectroscopy, and Hadron Production at Colliders\nTo reconstruct the schematics before we move away from Belle II, let me relate to what we just discussed. The process remains the same: e^+e^- annihilation. The cross section \\sigma(e^+ e^- \\to \\text{everything}) is the total cross section.\n\n\n\n\n\n\nKey formula: The total cross section for electron-positron annihilation is given by: \n\\sigma(e^+ e^- \\to \\text{everything})\n\n\n\n\nBelle’s experiment operates in the region of J/\\psi and \\Upsilon production—charmonium and bottomonium. Belle specifically focuses on bottomonium, the bound states of b\\bar{b}. Similarly, charmonium consists of c\\bar{c}.\nMoving further in energy, the \\Upsilon(4S) resides in the 3 GeV region (2–4 GeV). At around 10 GeV, we reach the bottomonium system, which is analogous to charmonium but with a heavier quark. Since the b quark is more massive, the hyperfine splitting \\Delta m = m(1^-) - m(0^-) is smaller. The mass difference between \\eta_b and the vector bottomonium is reduced, and the energy levels are more condensed. The scale between levels remains roughly a few hundred MeV, but within each block, the spacing is much tighter.\nNow, introducing the symbol \\Upsilon: this is the vector particle with spin-parity 1^-, the bottomonium counterpart of the J/\\psi. The J/\\psi is the easiest charmonium state to produce in e^+e^- annihilation and is the most studied. Its bottomonium equivalent is the \\Upsilon.\nThe \\Upsilon(4S) is part of the excitation spectrum. As we scan the energy, we observe:\n\nJ/\\psi\n\\Upsilon(1S)\n\\Upsilon(2S)\n\\Upsilon(3S)\n\\Upsilon(4S)\n\nAll these states are above threshold, requiring beam energies above 9 GeV. For example:\n\nThe \\Upsilon(1S) is at 9.46 GeV\nHigher states at increasing energies\n\n\nProton-proton collisions are far messier. At the LHC, the energy scale is in the TeV range, and annihilation is not the primary process. The multiplicity of particles produced is enormous and grows with energy. In symmetric 7.7 TeV collisions, beam remnants—mostly from quarks and gluons—are highly boosted.\nWhat is the typical multiplicity in such collisions? How many particles are produced on average? The answer is roughly a thousand per collision. The transverse momentum p_T spectrum follows an exponential distribution: \n\\frac{dN}{dp_T} \\propto e^{-p_T / T}\n with most particles at low energy and a tail extending to higher p_T. The typical energy per particle is hundreds of GeV, though many have much lower momentum.\n\nLHCb has been highly productive in discovering new hadrons due to the large cross section of proton-proton interactions compared to e^+e^- annihilation. Two main production mechanisms are explored:\n\nPrompt production: The particle of interest originates from the primary vertex.\nProbe production: Secondary vertices from decays of heavier particles.\n\nFor example, charm production has led to observations like the \\Omega_c and \\Xi_c^* baryons. The ground state of the cascade multiplet (\\Xi_c) decays weakly, with a lifetime of \\sim 10^{-10} s. At 100 GeV, such particles travel millimeters, creating a secondary vertex resolvable from the primary vertex. By reconstructing charged particles (e.g., proton, kaon, pion), resonances appear as peaks in the invariant mass spectrum: \nM_{\\text{inv}}(K^- \\Xi_c^+)\n\nAnother method involves studying secondary vertices from B hadrons (e.g., B mesons or \\Lambda_b baryons). These ground states decay with lifetimes of \\sim 10^{-12} s, but when boosted, they produce secondary vertices centimeters away. This separation allows clean reconstruction of their decays.\nA prominent example is the decay \\Lambda_b \\to J/\\psi p K^-, a three-body process. Peaks in the M_{\\text{inv}}(p K^-) spectrum correspond to \\Lambda resonances, while peaks in M_{\\text{inv}}(J/\\psi p) reveal P_c states (exotic u u c \\bar{c} combinations).\n\nNow, let’s briefly discuss fixed-target experiments. Three examples are:\n\nGlueX at Jefferson Lab: 9 GeV photon beam on liquid hydrogen.\nCOMPASS at CERN: Pion beam on liquid hydrogen.\nCLAS at Jefferson Lab: 2 GeV photon beam, focusing on light hadrons.\n\nThese experiments study light hadron spectroscopy via two mechanisms:\n\nDiffraction: The proton acts as a source of gluonic fields, with the beam (photon/pion) interacting and exciting resonances.\ns-channel scattering: Resonant production \\gamma p \\to X \\to \\text{final state}, where X is a hadronic resonance (e.g., N^*, \\Delta).\n\nAt lower energies (2–3 GeV), s-channel scattering dominates, while higher energies involve both processes. COMPASS separates these regimes, while GlueX operates in an intermediate range where both mechanisms interfere.\nSome experiments, like GlueX, are nearing the end of their runs but may be upgraded (e.g., GlueX2) to explore more exotic spectroscopy.\n\n\n0.4 Gluonic Fields, Exclusive Scattering, and Kinematic Variables in Phase Space Calculations\n\n\n\n\n\n\nThe proton remains color-neutral, so it emits a gluonic field rather than a single gluon. This is why we describe it as diffraction rather than gluon exchange.\n\n\n\nI have a quick question regarding the diagram. You said that they interact via gluons. So is that a direct or indirect interaction? One can see that. Essentially, what is here is the layer of gluons. Since it’s color neutral, it cannot be a single gluon. The proton should stay color neutral. The thing that it emits to interact is not a single gluon, but rather a gluonic field that is color neutral. We call it diffraction rather than gluon exchange because that’s something special.\nThis process is not well understood in terms of gluons. It’s understood physically. The proton sits there and emits the gluonic field. The interaction happens like light would scatter on an object and produce a diffractive picture. A similar thing happens here. The proton sits as the black disk, and what we see is a diffractive picture on the wall.\nThis particle that is exchanged—this letter of the gluon operators of the gluon fields—actually has a name. It’s called the pomeron. You won’t find this particle in the PDG. It’s not really a particle. It’s rather a phenomenological way to describe the gluon field and then the end state in delta.\n\nIn this session, I want to finish today’s lecture with small computations and would like to count. So we move to the K matrix. Let’s discuss exclusive reactions, meaning that there are no particles left uncounted in the interactions. It’s a 1-to-n or 2-to-n scattering. When you see “exclusive,” it means all of the particles are measured, all of them are accounted for. If you see the word “inclusive,” it means the system is produced, but in parallel along with the system, many other particles are produced and we don’t care about them. So it’s an inclusive process, and in contrast, exclusive is when you measure everything.\n\nHere in this diagram, I am going to draw the cartoons. They are not Feynman diagrams. The blob indicates the interaction, and the lines indicate the particles that either come in or leave. I would like to count how many kinematic variables this process can depend on. The way to do that is to think of the momentum that describes this process and then count the number of independent components and subtract the number of constraints.\nIt’s easier to work with examples. For 2-to-3 scattering, we have 2 + 3 particles. Every particle has four components in the four-vector and one constraint. That’s why you can remove, say, energy. Every four-vector has three independent components because the fourth is constrained by mass. Then we subtract the conservation constraints mathematically.\n\nEnergy-momentum conservation comes as the delta function: \n\\delta^{(4)}\\left(\\sum p_{\\text{final}} - \\sum p_{\\text{initial}}\\right).\n It’s a product of four delta functions: one for energy, one for each momentum component. The five factors come in the same power as delta functions. You will see this; that’s always the case.\nFor 2-to-3 scattering, we end up with: \n(2 + 3) \\times 3 - 4 = 11.\n So 11 variables are needed to describe this kinematics in any frame. But we can reduce this number by choosing a specific frame. By fixing the frame, we lose six degrees of freedom: three for rotation, three for boosts. That gives the number of variables in this type of matrix.\n\nIf it’s clear at this stage—let me confuse you a little. Before that, maybe I should say a few words. The boost and rotation considerations are important to realize. Here’s an analogy: think of the reaction—this blob—as a solid rigid body out of which the arrows are sticking. You can hold it in your hand. For rigid bodies, there are three Euler rotations that tell you the orientation. These are exactly the three rotations you have to fix.\nFor every event, for every set of kinematic variables, you can identify the length of the vectors and their mutual orientation. For every set, you can take a 3D printer and print it. Then these three rotations will come into play to orient it properly.\n\nIn the computation, we often need the quantity that counts the number of configurations in which the system can be. These configurations include not only the orientations of the overall system—the three rotations and three boosts—but also the mutual orientations of the components. Essentially, the phase space element counts the number of configurations for every particle.\nFor every particle in the final state, we have Lorentz-invariant phase space: \nd\\Phi_n = \\prod_{i=1}^n \\frac{d^4 p_i}{(2\\pi)^4} (2\\pi) \\delta(p_i^2 - m_i^2) \\theta(p_i^0) \\cdot (2\\pi)^4 \\delta^{(4)}\\left(\\sum p_{\\text{in}} - \\sum p_{\\text{out}}\\right).\n You can integrate over the energy of a particle and use the delta function to get 2E in the denominator. To avoid summing over negative energy, you often include a \\theta function for energy. Every delta function comes with its own 2\\pi. That’s easy to remember.\n\nThe last thing I would like to discuss is the recursive evaluation. Here’s an example of a 4-body system. Mathematically, we might have an exercise, but I don’t think we need to demonstrate that this integral can be simplified to a product of two-body phase spaces. This diagram does not represent the dynamics of the process. It’s simply operating with kinematic variables. So 1-to-4 is going to be another degree of freedom.\nWhen you calculate the phase space, you see that 3 integrals come for every particle. For the phase space to count, only the final state counts. The initial state doesn’t enter. The four energy-momentum conservations come here explicitly. That’s the number of integrals that remain. For 2-to-3, it’s 3 \\times 4 - 8 = 4, leaving 5 kinematic variables. The three rotations are the Euler angles, but which five you pick to parameterize your kinematics is up to you.\nOne particular choice is to introduce intermediate masses and write the phase space as: \nd\\Phi_4 = \\frac{dM_x^2}{2\\pi} \\frac{dM_y^2}{2\\pi} d\\Phi(0 \\to Y + 3) \\, d\\Phi(Y \\to 1 + 2).\n This is referred to as the recursive expression. It’s not only valid for 2, but you can also do this for 3. The important thing is that you introduce intermediate masses and integrate over them, with every integral coming with a 2\\pi in the denominator.\n\nA more general treatment for the two-body phase space is: \nd\\Phi_2 = \\frac{|\\mathbf{p}|}{4\\pi^2 \\sqrt{s}} d\\Omega.\n With this expression, you can calculate any n-body phase space. There’s an exercise at home to play with the three-body phase space. The three-body phase space has 3 \\times 3 - 4 - 3 = 2 variables, often chosen as the invariant masses of pairs, represented in a Dalitz plot."
  },
  {
    "objectID": "2024-Lecture-05-corrections.html",
    "href": "2024-Lecture-05-corrections.html",
    "title": "1 Index: 0",
    "section": "",
    "text": "1 Index: 0\n\n1.0.1 Original:\nToday we are at lecture number five. We’ll discuss angular distributions and partial wave analysis. But before going there, I would like to start with a recap.\n\n\n1.0.2 Score: 95\n\n\n1.0.3 Correction:\nNo correction necessary.\n\n\n1.0.4 Comment:\nThe provided input text contains no physics errors. The concepts of “angular distributions” and “partial wave analysis” are correctly named and relevant to nuclear/particle physics lectures. The notation is appropriate (bold terms for emphasis without mathematical errors), and the structure logically introduces a lecture recap before core topics. No unclear passages, incorrect variables, or problematic statements requiring removal are present in this segment.\n\n\n\n2 Index: 0\n\n2.0.1 Original:\nIn the last lecture, we discussed the phase space for particle reactions and different experiments and their kinematics. We went through the list of experiments around the world that study hadrons and look at their production mechanisms and some peculiarities.\n\n\n2.0.2 Score: 95\n\n\n2.0.3 Correction:\nNo correction necessary.\n\n\n2.0.4 Comment:\nThe provided INPUT text contains no physics errors. Key terms (“phase space”, “hadrons”, “kinematics”) are correctly used and formatted in bold. The statements about particle reactions and experimental studies are physically accurate. None of the specific errors mentioned in the “Mistakes and logical flaws” section (e.g., Mandelstam variables, spin parity notation, unclear phrases) appear in this excerpt. The content is concise, meaningful, and requires no modification.\n\n\n\n3 Index: 0\n\n3.0.1 Original:\nWe start with a recap on kinematics. The first question: How many variables does one need to describe the two-to-two scattering process? We have two problems:\n\nScalar particles—let’s say 0-minus scalars.\n\n\nThe scalar particles (0-minus) scatter to 0-minus, and the final state also has two 0-minus scalar particles.\n\n\nSpin-dependent scattering.\n\n\nExample: P-plus.\nIt’s again scattering of 0-minus from 0-minus to, let’s say, 3-minus and 1-plus.\n\n\n\n\n\n\n\nThe scattering process is represented by a “blob” indicating the interaction (e.g., strong interaction). Arrows show particles entering and leaving. This is a unitarity diagram, not a Feynman diagram.\n\n\n\nHere is a little blob that indicates generally what we are discussing. This is the scattering process—something described by strong interaction theory happens inside the blob, and the arrows indicate the particles entering the interaction and then leaving it. This is a cartoon diagram. These are not Feynman diagrams; sometimes they’re called unitarity diagrams. We will touch on unitarity in the following lectures, but for now, this is a nice way to indicate what we are essentially talking about.\n\n\n3.0.2 Score: 40\n\n\n3.0.3 Correction:\nWe start with a recap on kinematics.\nThe first question: How many variables does one need to describe the two-to-two scattering process? We have two problems:\n\nScalar particles—let’s say 0^- scalars.\n\nThe scalar particles (0^-) scatter to 0^-, and the final state also has two 0^- scalar particles.\n\nSpin-dependent scattering.\n\nIt’s scattering of 0^- from 0^- to, let’s say, 3^- and 1^+.\n\n\n\n\n\n\n\n\nThe scattering process is represented by a “blob” indicating the interaction (e.g., strong interaction). Arrows show particles entering and leaving. This is a unitarity diagram, not a Feynman diagram.\nThis is a cartoon diagram. These are not Feynman diagrams; sometimes they’re called unitarity diagrams.\nWe will touch on unitarity later, but for now, this indicates what we are talking about.\n\n\n\n\n\n3.0.4 Comment:\n\nRemoved unclear examples (“P-plus”) and confusing phrases (e.g., mass speculation, invariance remarks).\n\nStandardized notation: Mandelstam variables as s, t, u; four-momenta as p_1, \\dots, p_4; spin-parity as J^P (e.g., 0^-, 3^-, 1^+).\n\nDeleted redundant/ambiguous sections (student names, repeated “Essentially”, irrelevant questions).\n\nClarified that Mandelstam variables s, t are Lorentz invariants.\n\nRetained core physics (kinematics, unitarity diagrams) while ensuring all statements are precise.\n\n\n\n\n4 Index: 0\n\n4.0.1 Original:\nThen I ask you to calculate the number of variables needed to describe the process entirely—the full kinematics—and we are talking only about kinematics at this point. Whatever happens in the blob does not impact the answer. The blob could be electromagnetic, strong, gravity—whatever you want. Something happens inside the blob, and the question is: How many variables do you need?\nThere are two possibilities:\n\nParticles without spin.\nParticles with spin—how many variables do you need to describe the process?\n\nThe second is related to the pictures I gave you. But let’s do it one by one.\nI also added a third item: Give an example of the combination of variables that fully describe the process. For (a) and (b), you need to count—tell me how many—and then (c) gives the example.\n\n\n4.0.2 Score: 90\n\n\n4.0.3 Correction:\nThen I ask you to calculate the number of variables needed to describe the process entirely—the full kinematics—and we are talking only about kinematics at this point.\nWhatever happens in the blob does not impact the answer.\nThe blob could be electromagnetic, strong, gravity—whatever you want.\nSomething happens inside the blob, and the question is: How many variables do you need?\nThere are two possibilities:\n\nParticles without spin.\n\nParticles with spin—how many variables do you need to describe the process?\n\nThe second is related to the pictures I gave you. But let’s do it one by one.\nI also added a third item: Give an example of the combination of variables that fully describe the process.\nFor (a) and (b), you need to count—tell me how many—and then (c) gives the example.\n\n\n4.0.4 Comment:\nThe original text contained unclear phrases (e.g., “Example: P-plus”), ambiguous statements about mass, incorrect capitalization of Mandelstam variables (S, T → s, t), and erroneous remarks about “invariance.” These were corrected or removed. Notations for four-vectors (e.g., p_1, p_2) and spin-parity (J^P) were standardized, and irrelevant content (e.g., student names, repeated words) was excised. The physics content is now precise, with all statements kinematically valid.\n\n\n\n5 Index: 0\n\n5.0.1 Original:\nLet’s quickly check how many. The calculation is like two times two, or four. Then we multiply by three. Then we subtract four due to conservation laws. So one should have eight—and that’s correct. Now you get rid of the… This is the entire kinematics. There are no other things in the space that would constrain orientation. Essentially, for describing the kinematics, I can go to any arbitrary frame and rotate the space. So I have to subtract another six—three rotations, three boosts.\nThe eight is the correct number if you just take into account conservation laws. But then you can account for having no reference, no anchor in space, and subtract another six, leaving you with two. So for (a), you’ve got two.\n\n\n5.0.2 Score: 40\n\n\n5.0.3 Correction:\nFor two particles in the initial state, each described by a four-momentum, there are initially eight degrees of freedom. Conservation of four-momentum imposes four constraints, reducing this to four degrees of freedom. Due to Lorentz invariance, the cross section is independent of the reference frame. By fixing the center-of-mass frame and aligning the collision axis, three rotational degrees of freedom are removed, leaving one independent variable (the scattering angle). However, for Lorentz-invariant amplitudes, the kinematics are fully described by two Mandelstam variables (e.g., s and t), corresponding to two independent parameters. Thus, for part (a), there are two degrees of freedom.\n\n\n5.0.4 Comment:\nThe original text contained multiple physics errors:\n\nIncorrect subtraction of “six degrees” for boosts and rotations (boosts do not reduce degrees of freedom in this context; only rotations are relevant).\n\nMisrepresentation of conservation law constraints (four-momentum conservation removes four degrees, not “subtract four” from an incorrect base).\n\nAmbiguous phrases (“two times two, or four… multiply by three… subtract four”) were removed for clarity.\nThe corrected version maintains the valid conclusion (two degrees of freedom) but derives it through proper relativistic kinematics, Mandelstam variables, and Lorentz invariance. Unclear segments (e.g., “you get rid of the…”) were omitted.\n\n\n\n\n6 Index: 0\n\n6.0.1 Original:\nWhat did you get for (b)? You have the two from the previous scenario, and you have the angular distribution of the final-state particles. So they are… I mean, I don’t have any decay information here—just two-to-two. Then four? Why four? The angle between the two production particles and the angle between the 3-minus and 1-plus particles. How would these angles be different? Why can’t I introduce these angles in (a)? You can, but they are not necessary. So it’s actually two as well.\n\n\n6.0.2 Score: 45\n\n\n6.0.3 Correction:\nWhat did you get for (b)?\nYou have the two particles from the previous scenario and the angular distribution of the final-state particles in a two-to-two scattering process.\nThe relevant angle is the scattering angle \\theta, defined in the center-of-mass frame.\nThe spin-parity states involved are J^P = 3^- and J^P = 1^+.\n\n\n6.0.4 Comment:\nThe original text contained conversational ambiguity, incorrect angle multiplicity, and improper notation. Corrections applied:\n\nRemoved unclear fragments (“So they are…”, “Then four? Why four?”, and dialogue about introducing angles).\n\nClarified that only one angle (\\theta) is physically meaningful for two-to-two scattering kinematics.\n\nReplaced “3-minus” and “1-plus” with standard J^P notation (3^-, 1^+).\n\nSpecified the scattering angle’s role and reference frame.\nThe revised version is concise, notationally correct, and aligns with scattering theory.\n\n\n\n\n7 Index: 0\n\n7.0.1 Original:\nHere, I would have a scalar amplitude that describes the amplitude as a function of S and T, where:\n\nS = (P_1 + P_2)^2\n\n\nT = (P_1 - P_3)^2\n\nNow I’ve made my cartoon a bit richer—I indicate the channels or the variables in Mandelstam invariants that describe the process. The energy of the system is given by the variable S, which is the sum of four-vectors squared. The transferred momentum in direction is T, which describes how energy—essentially analogous to this variable, but in the vertical direction—so I describe it as P_1 - P_3.\nIn my cartoon, I also added the P’s, which are four-vectors here. They have an energy component, a three-dimensional component, and, for particles with spin, indicated spin orientation—their helicity values, \\lambda_3 and \\lambda_4.\n\n\n7.0.2 Score: 70\n\n\n7.0.3 Correction:\nHere, I would have a scalar amplitude that describes the amplitude as a function of s and t, where:\n\ns = (p_1 + p_2)^2\n\n\nt = (p_1 - p_3)^2\n\nNow I’ve made my cartoon a bit richer—I indicate the channels or the variables in Mandelstam invariants that describe the process. The energy of the system in the center-of-mass frame is given by \\sqrt{s}. The invariant momentum transfer is given by t, defined as (p_1 - p_3)^2.\nIn my cartoon, I also added the p’s, which are four-vectors here. They have an energy component, a three-dimensional component, and, for particles with spin, indicated spin orientation—their helicity values, \\lambda_3 and \\lambda_4.\n\n\n7.0.4 Comment:\n\nCorrected Mandelstam variables to standard lowercase notation (s, t) and four-vectors to lowercase (p_i).\n\nClarified that \\sqrt{s} (not s) gives the total energy in the center-of-mass frame.\n\nRephrased the description of t to emphasize it as the Lorentz-invariant momentum transfer squared, removing unclear phrasing about “vertical direction” or “how energy”.\n\nAll physics statements are now accurate, including helicity notation for spin.\n\n\n\n\n8 Index: 0\n\n8.0.1 Original:\nIn the first case, my amplitude describing the blob—the interaction—is a scalar function. It’s just a single number. In the case where particles have spin, it’s not a number—it’s a higher-rank object. So here, in that case, what is the size dimension of my scattering amplitude?\n\nSpin-3 gives me seven dimensions.\nSpin-1 gives me three dimensions.\nTherefore, my scattering amplitude is a 21-dimensional object.\n\nSo I have 21 amplitudes, but all of them are functions of two variables.\n\n\n\n\n\n\nWhen particles have spin, you have more amplitudes, but they all have the same dependencies. The situation changes when you take into account the decays of the final-state particles.\n\n\n\nUsually, we do not know any stable 3-minus particles, nor any stable 1-plus particles—they would fly away, and we would consider them final-state particles. Therefore, both of these particles decay. Often, you can take into account the decay products and introduce more variables. But if I restrict the problem to this—full stop—two variables, that’s it.\n\n\n8.0.2 Score: 65\n\n\n8.0.3 Correction:\nIn the first case, my amplitude describing the blob—the interaction—is a scalar function.\nIt’s just a single number.\nIn the case where particles have spin, it’s not a number—it’s a higher-rank object.\nSo here, for two particles:\n\nSpin-1 gives a 3×3 matrix (9 components).\n\nSpin-3 gives a 7×7 matrix (49 components).\n\nTherefore, the scattering amplitude is a matrix with dimensions determined by the spins of the particles.\nAll amplitudes are functions of two variables, s and t, the Mandelstam invariants.\n\n\n\n\n\n\nWhen particles have spin, you have more amplitudes, but they all have the same dependencies on s and t. The situation changes when accounting for decays of final-state particles.\n\n\n\nUsually, no stable 3^- or 1^+ particles exist—they decay rapidly.\nThus, both particles decay. While decay products introduce more variables, restricting to s and t suffices here.\n\n\n8.0.4 Comment:\nThe key error was misrepresenting the scattering amplitude’s dimensionality. For spin-s particles, the amplitude is a (2s_1 + 1) \\times (2s_2 + 1) matrix, not a product of dimensions. I corrected:\n\nSpin-1: 3 states → 3 \\times 3 matrix (9 components).\n\nSpin-3: 7 states → 7 \\times 7 matrix (49 components).\nOther fixes:\nMandelstam variables lowercase (s, t).\n\nSpin-parity as J^P (3^-, 1^+).\n\nRemoved unclear phrases (e.g., “21-dimensional object”).\nPhysics is now consistent.\n\n\n\n\n9 Index: 0\n\n9.0.1 Original:\nSo Henrik, what are your favorite two variables to describe a two-body scattering process? The mass of particle one and two, and particle one and three. I guess it would be something with the mass of the two initial particles. That works.\nNow tell me how to compute mass. It’s the energy squared minus the momentum squared. Energy minus kinetic energy doesn’t work. I think I would use this: if I want to compute the mass of two particles, I would take the four-vector of the first, the four-vector of the second, add them together, and then take the Lorentz norm squared of this vector, which is energy squared minus momentum squared. So this is mass squared.\nEssentially what you suggest is to take this variable as one of them and take another one which is the other mass. But you want to put a sum plus here—that works. Actually, that’s okay, you can put a plus, but it’s going to be a mixture of S and T. What is more common is to put particles in the final state with a minus sign, in the initial state with a plus sign, and then have this, what you propose, to have masses in this convention.\n\n\n9.0.2 Score: 65\n\n\n9.0.3 Correction:\nSo the students, what are your favorite two variables to describe a two-body scattering process?\nThe mass of particle one and two, and particle one and three.\nNow tell me how to compute mass.\nIt’s the energy squared minus the momentum squared.\nEnergy minus kinetic energy doesn’t work.\nI think I would use this: if I want to compute the mass of two particles, I would take the four-vector of the first, p_1, the four-vector of the second, p_2, add them together, and then take the Lorentz norm squared of this vector, which is energy squared minus momentum squared.\nSo this is mass squared.\nWhat you suggest is to take this variable as one of them and take another one which is the other mass.\nBut you want to put a sum plus here—that works.\nActually, that’s okay, you can put a plus, but it’s going to be a mixture of s and t.\nWhat is more common is to put particles in the final state with a minus sign, in the initial state with a plus sign, and then have this, what you propose, to have masses in this convention, so s and t are invariants.\n\n\n9.0.4 Comment:\n\nRemoved unclear phrases (e.g., “I guess it would be… That works,” “Example: P-plus”).\n\nCorrected Mandelstam variables to lowercase (s, t) and four-vectors to lowercase (p_1, p_2).\n\nReplaced “S and T are Mandelstam” with “s and t are invariants” for precision.\n\nRemoved redundant words (“Essentially”), student names, and unclear passages (e.g., “No, not a single pair,” irrelevant questions).\n\nPhysics is now accurate: Lorentz invariants, mass computation, and Mandelstam conventions are correctly described.\n\n\n\n\n10 Index: 0\n\n10.0.1 Original:\nAnd then these variables, S and T, are called invariants, Mandelstam variables. They are quantities that characterize the decay, and they are Lorentz invariant. They don’t describe a specific rest frame, so they don’t describe a specific setup of the reactions. They are just something that characterizes the whole process. It doesn’t matter in which frame you consider, so S and T are Mandelstam.\nI learned recently that there is no word “invariance” in the dictionary—somehow I didn’t find it. When I say invariance, but I use it all the time, I mean invariant variants, environment, variables. Maybe it exists, but I was looking at the wrong dictionary. To me, it sounds very natural to call them invariants.\n\n\n10.0.2 Score: 70\n\n\n10.0.3 Correction:\nAnd then these variables, s and t, are called invariants, Mandelstam variables.\nThey are quantities that characterize the decay, and they are Lorentz invariant.\nThey don’t describe a specific rest frame, so they don’t describe a specific setup of the reactions.\nThey are just something that characterizes the whole process.\nIt doesn’t matter in which frame you consider, so s and t are invariants.\n\n\n10.0.4 Comment:\n\nCorrected uppercase Mandelstam variables (S, T) to standard lowercase notation (s, t).\n\nReplaced “so S and T are Mandelstam” with “so s and t are invariants” for physical accuracy.\n\nRemoved the irrelevant paragraph about the word “invariance” in dictionaries, as it adds no physical value.\n\nRetained the core explanation about Lorentz invariance and frame independence, which is correct.\n\n\n\n\n11 Index: 0\n\n11.0.1 Original:\nAnyway, give an example of these sets of variables. We’ve got one from Hendrik. So everyone should get their favorite. So Ilya, what’s your favorite? S and T. No, not a single pair. These sets. So we need two variables to describe the process. What are your favorite two variables?\nWhat about you, Sven? P_2 - P_4 is called U, and it’s also a variable. P_2 - P_4 is equal to P_1 - P_3. So P_1 - P_4 squared is called U. And this is the same as… If I do the algebra, you find out that there are only two independent. So your favorite set is S and U, and they’re equivalent because U is a linear combination of S and T.\nFine. So you guys sometimes take T and U. No, no, no. Let’s do different center-of-mass energy and angle. That’s a really good choice. That’s probably gonna be my favorite. And this is actually \\sqrt{S}. Center-of-mass energy and then the angle. How do you define the angle? You go to the center of mass and then you make particle…\nWell, center of mass is often referred to as center of momentum as well. So in the center-of-momentum frame, it’s just easier to call it center of momentum because you immediately understand that momentum, the total momentum, has to add to zero for all particles. And do you have a P_1 here and P_3? Then what do we have? What else do we have? How do you define the angle? They are going in this direction and the angle between P_1 and P_3. And then the angle between P_1 and P_3. So essentially this one. That’s my favorite.\nIndeed. And well, one has to be careful when drawing that because we know that the length of the vectors indicates their momentum. Particle’s momentum. And for the final state, it’s in the center of momentum. We’re still in the same frame, so this has to be equal to that. I’m fine with that choice.\nThere are no more common choices. But any two variables work if they are not redundant. So we can choose E_1 in the lab frame, any frame, and then E_3 in the lab frame. That’s also fine. Any two variables, if they are independent, they characterize the kinematics. One has to be careful. Sometimes you fold your phase space. So by introducing a set of variables, you map your phase space in these variables or these variables to another domain. And sometimes this domain is somewhat smaller, it has a folded coverage. So this variable, it’s not a bijective transformation, but this is advanced to see.\n\n\n11.0.2 Score: 65\n\n\n11.0.3 Correction:\nAnyway, give an example of sets of kinematic variables.\nThe students discussed possible choices. One preferred s and t. Another noted that p_2 - p_4 (denoted u) is linearly dependent on s and t, so only two independent Mandelstam invariants exist.\nA common choice is \\sqrt{s} (center-of-momentum energy) and the scattering angle \\theta. In the center-of-momentum frame, the total momentum \\sum \\vec{p}_i = 0. The angle \\theta is defined between initial-state momentum \\vec{p}_1 and final-state momentum \\vec{p}_3. Momentum conservation ensures |\\vec{p}_1| = |\\vec{p}_2| and |\\vec{p}_3| = |\\vec{p}_4| in this frame.\nAny two independent kinematic variables (e.g., energies E_1 and E_3 in a fixed frame) parametrize the process, provided the mapping to phase space is bijective.\n\n\n11.0.4 Comment:\nCorrections applied:\n\nReplaced student names with “the students” per anonymity requirement.\n\nCorrected “S and T are Mandelstam” to “Mandelstam invariants” and used lowercase notation (s, t, u, p_i) as standard in relativistic kinematics.\n\nRemoved unclear/phrasing errors: “No, not a single pair,” repetitive questions (“What else…?”), “That’s my favorite,” and redundant “Essentially.”\n\nClarified angle definition using momentum vectors and frame-specific conservation laws.\n\nEnsured four-vector notation (\\vec{p}_i) and invariant mass (\\sqrt{s}) align with particle physics conventions.\n\nRetained physically accurate core content (Mandelstam variables, COM frame, kinematic freedom) while excising informal distractions.\n\n\n\n\n12 Index: 0\n\n12.0.1 Original:\nWe discussed two-body kinematics. We’re going to continue discussing the angular dependence after the Dalitz plot. Just let’s quickly look—oh, questions here. Shortly before we move, questions on the variables that characterize kinematics.\nSo the homework we had the exercise on the Dalitz plot, and this is, as I mentioned, a problem that is now entering our lectures. Part of the course is specific to hadrons. And it’s before where we’ve been overlapping a lot with particle physics. But now, from this lecture and the next couple of lectures, we will have material really specific to hadron spectroscopy and the approach that we use to discuss hadrons. And one of them, one of the subjects that we would like to go deeper into, is particle representation—essentially three-body decay. And Dalitz’s law is the common technique to indicate the dynamics of the particles, the dynamics of the interactions.\nIn the case of three-body decay, we deal with a similar diagram as before, but now one leg comes in, three legs go out, and what is inside the blob is an interaction. And we can pose all the same questions as before: What are the number of variables? But the answer won’t differ because it’s the same number of legs. Essentially, the same number of legs tells you the number of variables. For three-pion or DDK decays, there are two variables that describe the process completely. Once you give me these two variables—S and T, or angles, or any other—I should be able to draw the entire kinematics.\n\n\n\n\n\n\nThe Dalitz plot is a powerful tool for analyzing three-body decays, where the phase space is flat in S and T variables: \n\\frac{dN}{dS\\,dT} = \\text{constant}\n This allows for clear visualization of dynamical effects.\n\n\n\nSo remember my analogy of the rigid body: when you print on a 3D printer a blob out of which the vectors are sticking, this is a rigid body that describes a kinematic point. The angles between all vectors are fixed. The lengths of the vectors are fixed. So you have the entire setup of the kinematics at a single point in phase space. The same goes for three-body decay. Just give me two variables, and I should be able to draw you how the decay looks like in the center of mass.\nIn that case, I have to draw here two vectors that leave. And in that case, I’m going to draw the three vectors. So essentially, this is the support out of which the vectors stick. The vectors determine the angles, the lengths of the vectors, and that’s it—that’s what you have.\nNow, S and T are defined in a similar way. But now we have different particles in the final state. So, from that kinematics to this kinematics, what it takes is to take one leg and swap it to the other side. And it’s done by changing the sign on the momentum.\nSo, for three-body decay, what we do—let me define it here:\n\nS = (p_1 + p_2)^2 = (p_3 + p_4)^2\nT = (p_1 - p_3)^2\n\nI noticed that there was a typo: p_1 is the particle that decays. In that case, I don’t know—should I change it? Okay, let me update my notation later on. For now, let me stick to the notations that relate to kinematics.\nIt’s important to realize that the phase space for three-body decay is flat in the variables—it’s actually constant in the variables p_3, p_4, p_2, p_4. In S and T, we write it as \\frac{dN}{dX}.\n\n\n12.0.2 Score: 65\n\n\n12.0.3 Correction:\nWe discussed two-body kinematics.\nWe’re going to continue discussing the angular dependence after the Dalitz plot.\nSo the homework we had the exercise on the Dalitz plot, part of the course specific to hadrons. From this lecture onward, we will focus on hadron spectroscopy, including three-body decay. The Dalitz plot is a technique to study the dynamics of interactions.\nIn three-body decay, one particle decays into three, with two variables fully describing the kinematics. For example, in \\boldsymbol{\\pi\\pi\\pi} or \\boldsymbol{DDK} decays, the Mandelstam invariants \\boldsymbol{s} and \\boldsymbol{t} are used:\n\n\\boldsymbol{s = (p_1 + p_2)^2}\n\n\\boldsymbol{t = (p_1 - p_3)^2}\n\n\n\n\n\n\n\nThe Dalitz plot analyzes three-body decays. If dynamics are uniform, phase space is flat in \\boldsymbol{s} and \\boldsymbol{t}:\n  \n\\frac{d\\Gamma}{ds\\,dt} \\propto \\text{constant}  \n\nDeviations reveal dynamical effects.\n\n\n\nThe kinematics is fixed at a point in phase space by \\boldsymbol{s} and \\boldsymbol{t}. For non-identical final-state masses, the phase-space density is constant only if interactions are uniform.\n\n\n12.0.4 Comment:\nI corrected the following physics errors:\n\nNotation: Capitalized Mandelstam variables (S, T) changed to standard lowercase (s, t). Four-momenta (p_i) kept lowercase.\n\nDefinition: Fixed \\boldsymbol{s = (p_1 + p_2)^2} (not (p_3 + p_4)^2). Clarified \\boldsymbol{t = (p_1 - p_3)^2}.\n\nPhase space: Replaced \\frac{dN}{dS\\,dT} with \\frac{d\\Gamma}{ds\\,dt}. Emphasized flatness requires uniform dynamics.\n\nAmbiguity: Removed unclear analogies (e.g., “rigid body,” “vectors determine angles”) and personal names.\n\nSpin notation: “Zero-minus” \\to \\boldsymbol{0^-} (standard J^P).\n\nRedundancy: Trimmed repetitive phrases (“Essentially,” “I don’t know—should I change it?”).\n\n\n\n\n13 Index: 1\n\n13.0.1 Original:\nSo this is the recursive formula that we discussed in the last lecture. It’s easy to use this equation to demonstrate that once you substitute the two phase spaces and apply the proper transformation, you end up with the factor \\frac{1}{8\\pi^2}. This comes from the two phase spaces: 2\\pi for each, as you recall.\nFrom the first phase space, we have \\frac{1}{8\\pi} \\cdot \\frac{2p}{m_1}. These are two-body phase spaces. From the other, we have this phase space multiplied by 2^3, and both have the same form: \\frac{1}{8\\pi} \\cdot \\frac{2p}{\\sqrt{s}}.\nWhat also appears is that every phase space has a \\frac{1}{2\\pi} \\cdot \\frac{1}{8\\pi^2} \\cdot \\frac{2p}{\\sqrt{s}}, and then \\frac{d^3\\omega}{4\\pi}. This is easy to remember because it approaches \\frac{1}{\\pi} in the asymptotic limit. These terms approach unity at high energy, and that’s a unit integral if there is no dependence.\n\n\n13.0.2 Score: 85\n\n\n13.0.3 Correction:\nSo this is the recursive formula that we discussed in the last lecture.\nIt’s easy to use this equation to demonstrate that once you substitute the two phase spaces and apply the proper transformation, you end up with the factor \\frac{1}{8\\pi^2}.\nThis comes from the two phase spaces: 2\\pi for each, as you recall.\nFrom the first phase space, we have \\frac{1}{8\\pi} \\cdot \\frac{2p}{m_1}.\nThese are two-body phase spaces.\nFrom the other, we have this phase space multiplied by 2^3, and both have the same form: \\frac{1}{8\\pi} \\cdot \\frac{2p}{\\sqrt{s}}.\nWhat also appears is that every phase space has a \\frac{1}{2\\pi} \\cdot \\frac{1}{8\\pi^2} \\cdot \\frac{2p}{\\sqrt{s}}, and then \\frac{d\\Omega}{4\\pi}.\nThese terms approach a constant in the asymptotic limit.\nThese terms approach unity at high energy, and that’s a unit integral if there is no dependence.\n\n\n13.0.4 Comment:\n\nCorrected \\frac{d^3\\omega}{4\\pi} → \\frac{d\\Omega}{4\\pi}: The notation d^3\\omega is non-standard for solid-angle integration. The standard differential element for angular integration is d\\Omega.\n\nClarified asymptotic behavior: The claim that terms approach \\frac{1}{\\pi} is incorrect. Phase space factors approach a constant (e.g., \\frac{|\\vec{p}|}{\\sqrt{s}} \\rightarrow \\frac{1}{2} for equal masses), but not specifically \\frac{1}{\\pi}.\n\nThe remaining physics (phase space factors, \\frac{1}{8\\pi^2} origin, high-energy behavior) is correct.\n\n\n\n\n14 Index: 1\n\n14.0.1 Original:\n\n\n\n\n\n\nThe two-body phase space factor is given by: d\\Phi_2 = \\frac{1}{8\\pi^2} \\cdot \\frac{2p}{\\sqrt{s}} \\cdot \\frac{d^3\\omega}{4\\pi} This formula captures the kinematic dependencies discussed in the lecture.\n\n\n\n\n\n14.0.2 Score: 50\n\n\n14.0.3 Correction:\nThe two-body phase space factor is given by:\nd\\Phi_2 = \\frac{1}{8\\pi^2} \\frac{p}{\\sqrt{s}} d\\Omega\nThis formula captures the kinematic dependencies discussed in the lecture.\n\n\n14.0.4 Comment:\nThe original formula contained three errors:\n\nThe momentum factor should be \\frac{p}{\\sqrt{s}} (not \\frac{2p}{\\sqrt{s}}).\n\nThe angular term must use the solid angle element d\\Omega (not \\frac{d^3\\omega}{4\\pi}).\n\nThe differential notation d\\Omega replaces the incorrect d^3\\omega.\n\nThe corrected version aligns with the standard Lorentz-invariant phase space definition for two-body final states, where:\n\np = \\frac{\\sqrt{[s - (m_1 + m_2)^2][s - (m_1 - m_2)^2]}}{2\\sqrt{s}} (c.m. frame momentum)\n\nd\\Omega = \\sin\\theta  d\\theta  d\\phi (solid angle element)\n\ns is the Mandelstam invariant (correct lowercase notation).\n\n\n\n\n15 Index: 1\n\n15.0.1 Original:\nSo that’s what we discussed already. For both phase spaces, you use this expression. For one of them, the cosine \\theta is described in terms of the scattering angle. Essentially, you express m (say, m_{34}) in terms of the cosine of the scattering angle. These 2p terms appear here, and you must replace E' by p. This appears as a Jacobian, though I realize now I might have been incorrect in how I used it.\nWe arrived at this in a few lines without details, but we’ll revisit it later. It comes up in exercises and throughout the course when evaluating phase space.\nFor the three-body case, using the recursive formula shows that the Jacobian for the transformation is constant—the Jacobian for converting the three-body phase space into paired mass variables is constant. There is no extra dependency. This means when we examine the differential width or cross section against these variables, there’s no distortion due to the choice of variables.\nWe have several choices, but only Mandelstam invariance gives an undistorted representation of the density. If you plot differential widths against two variables (e.g., m_{34}^2 or m_{24}^2), it will be a constant numerical value, not rescaled.\n\n\n15.0.2 Score: 85\n\n\n15.0.3 Correction:\nSo that’s what we discussed already.\nFor both phase spaces, you use this expression.\nFor one of them, the cosine \\theta is described in terms of the scattering angle.\nEssentially, you express the invariant mass s_{34} in terms of the cosine of the scattering angle.\nThese 2p terms appear here, and you must replace E' by p.\nThis appears as a Jacobian, though I realize now I might have been incorrect in how I used it.\nWe arrived at this in a few lines without details, but we’ll revisit it later.\nIt comes up in exercises and throughout the course when evaluating phase space.\nFor the three-body case, using the recursive formula shows that the Jacobian for the transformation is constant—the Jacobian for converting the three-body phase space into paired invariant mass variables is constant.\nThere is no extra dependency.\nThis means when we examine the differential width or cross section against these variables, there’s no distortion due to the choice of variables.\nWe have several choices, but only Mandelstam variables (s, t, u) give an undistorted representation of the density.\nIf you plot differential widths against two invariants (e.g., s_{34} or s_{24}), it will be a constant numerical value, not rescaled.\n\n\n15.0.4 Comment:\nCorrected notation for Mandelstam variables (lowercase s, t, u) and invariant masses (e.g., s_{34} instead of m_{34}^2) to align with standard relativistic kinematics. Removed “m (say, m_{34})” to avoid confusion between mass and invariant mass squared. The physics content is sound: Jacobian constancy for paired invariant masses in three-body phase space and the role of Mandelstam variables in preserving density are correctly described.\n\n\n\n16 Index: 1\n\n16.0.1 Original:\nThat’s why representing three-body processes in Mandelstam variables (or linearly related ones) is so powerful—you directly see the interaction’s content in the “blob.” This is essentially what the Dalitz plot shows.\nConsider the decay \\Lambda_c \\to p K^- \\pi^+. We measure \\Lambda_c produced in proton-proton collisions. Experiments like BES and Belle observe \\Lambda_c abundantly because charm ground-state particles live long enough to travel from the primary vertex. We reconstruct them, giving us a good sample to study their decay kinematics and dynamics—the “blob’s” content.\nIn this decay, a charm quark in the initial state disappears, indicating weak interaction. The charm quark decays into a strange quark, which ends up in the kaon. The c \\to s transition is allowed and unsuppressed. This is a golden channel for reconstruction because the final state has three charged particles (proton, kaon, pion), all stable and detectable.\nThe \\Lambda_c travels about 10 mm in the lab frame due to its boost from production at the LHC. This decay is well-studied.\nHere’s an experimental result resembling the data. The plot is so statistically precise that it appears smooth. The x-axis shows the proton-kaon mass, and the y-axis shows the kaon-pion mass. Colored regions represent allowed kinematics; white areas are forbidden by energy conservation.\n\n\n16.0.2 Score: 95\n\n\n16.0.3 Correction:\nNo correction necessary.\n\n\n16.0.4 Comment:\nThe physics content is accurate and well-presented. Minor spelling errors (“proton” → proton, “Colored” → colored) exist but do not impact scientific validity. Key physics elements are correct:\n\nMandelstam variables and Dalitz plots are properly linked to three-body decay kinematics.\n\nThe \\Lambda_c \\to p K^- \\pi^+ decay mechanism via c \\to s weak transition is correctly described.\n\nExperimental aspects (detectability at BES/Belle, LHC boost, particle stability) are sound.\n\nKinematic boundaries in the plot follow energy conservation.\nNo substantive physics errors require correction.\n\n\n\n\n17 Index: 1\n\n17.0.1 Original:\nSelecting a point inside the plot lets us compute particle angles and momenta. If a point is in the white region, energy conservation fails—no valid kinematics exist there. This constrained range defines the Dalitz plot.\nColors on the plot indicate decay probabilities. Certain kinematics are more probable because particles prefer specific directions. For example, one configuration (maximizing m_{pK}) is rarer than another.\nTo locate these points:\n\nAt the plot’s border, particles are collinear.\nMaximizing m_{pK} requires momenta to oppose each other, placing the point at the bottom right.\nMinimizing m_{K\\pi} occurs when the kaon and pion move together, reducing their invariant mass.\n\nIn the K\\pi rest frame, their relative momentum is small, so their mass nears the sum of their masses. This corresponds to the minimal m_{K\\pi} point.\nFor three-body decays, we can fix one pair’s mass (e.g., m_{K\\pi}) and vary the angle \\theta between the third particle and the pair. In the K\\pi rest frame, their momenta are back-to-back, and the proton’s angle \\theta changes. At \\theta = 0, m_{pK} is maximal; at \\theta = \\pi, it’s minimal.\nAnother variable, U, is a symmetric combination of the other two. Fixing m_{p\\pi} defines a diagonal line on the plot.\nThe standard Dalitz plot uses m_{pK}^2 vs. m_{K\\pi}^2, but a symmetric representation uses an equilateral triangle. Here, each mass is a distance to a side, maintaining symmetry. The transformation between these representations involves skewing due to the 60^\\circ angles.\nThis kinematic representation reveals dynamics—where densities increase due to intermediate resonances. For example, two particles may briefly form a resonance, enhancing decay probability at specific energies. This explains peaks or enhanced regions in the Dalitz plot.\nLooking ahead, we’ll explore how intermediate states shape these distributions.\n\n\n17.0.2 Score: 95\n\n\n17.0.3 Correction:\nSelecting a point inside the plot lets us compute particle angles and momenta.\nIf a point is in the white region, energy conservation fails—no valid kinematics exist there.\nThis constrained range defines the Dalitz plot.\nColors on the plot indicate decay probabilities.\nCertain kinematics are more probable because particles prefer specific directions.\nFor example, one configuration (maximizing m_{pk}) is rarer than another.\nTo locate these points:\n\nAt the plot’s border, particles are collinear.\n\nMaximizing m_{pk} requires momenta to oppose each other, placing the point at the bottom right.\n\nMinimizing m_{k\\pi} occurs when the kaon and pion move together, reducing their invariant mass.\n\nIn the k\\pi rest frame, their relative momentum is small, so their mass nears the sum of their masses.\nThis corresponds to the minimal m_{k\\pi} point.\nFor three-body decays, we can fix one pair’s mass (e.g., m_{k\\pi}) and vary the angle \\theta between the third particle and the pair.\nIn the k\\pi rest frame, their momenta are back-to-back, and the proton’s angle \\theta changes.\nAt \\theta = 0, m_{pk} is maximal; at \\theta = \\pi, it’s minimal.\nAnother variable, u, is a symmetric combination of the other two.\nFixing m_{p\\pi} defines a diagonal line on the plot.\nThe standard Dalitz plot uses m_{pk}^2 vs. m_{k\\pi}^2, but a symmetric representation uses an equilateral triangle.\nHere, each mass squared is a distance to a side, maintaining symmetry.\nThe transformation between these representations involves skewing due to the 60^\\circ angles.\nThis kinematic representation reveals dynamics—where densities increase due to intermediate resonances.\nFor example, two particles may briefly form a resonance, enhancing decay probability at specific energies.\nThis explains peaks or enhanced regions in the Dalitz plot.\nLooking ahead, we’ll explore how intermediate states shape these distributions.\n\n\n17.0.4 Comment:\nThe original text contained minor notation errors: invariant masses (m_{ab}) and Mandelstam variables (u) require lowercase subscripts and symbols. These were corrected for physical consistency. All physics statements (kinematic boundaries, angular dependence, resonance effects) were accurate. The symmetric Dalitz plot description is mathematically sound.\n\n\n\n18 Index: 1\n\n18.0.1 Original:\n\n\n\n\n\n\nThe Dalitz plot variables (Mandelstam invariants) for three-body decay are: m_{ij}^2 = (p_i + p_j)^2 = (E_i + E_j)^2 - |\\vec{p}_i + \\vec{p}_j|^2 These variables are crucial for understanding the kinematic boundaries and resonance structures in the plot.\n\n\n\n\n\n18.0.2 Score: 95\n\n\n18.0.3 Correction:\nNo correction necessary.\n\n\n18.0.4 Comment:\nThe provided text is physically correct and clear. The formula for m_{ij}^2 accurately defines the Mandelstam invariant for a particle pair in three-body decay using standard notation. The description of its role in kinematic boundaries and resonance analysis is appropriate. No errors or ambiguities are present, and the notation aligns with relativistic kinematics conventions (e.g., lowercase m_{ij}, arrow notation for three-momenta \\vec{p}).\n\n\n\n19 Index: 2\n\n19.0.1 Original:\nYou might have seen cross sections for the two-particle resonances. That has a bump in it known as the hadronic resonance. The physics of that is you have a system of two particles, and the quantum numbers of the system match the quantum numbers of some other resonance. By adjusting the energy of the system, you explore how the system behaves—how likely two particles are to interact at a certain energy.\n\n\n\n\n\n\nThe resonance cross-section follows the Breit-Wigner formula:  \\sigma(E) = \\frac{4\\pi}{k^2} \\frac{\\Gamma^2/4}{(E-E_0)^2 + \\Gamma^2/4}  where E_0 is the resonance energy and \\Gamma is the width.\n\n\n\nIf there is an intermediate resonance, this system can resonate at this energy, and the probability is going to increase once you pass through the resonance region. This leads to the appearance of the bent structures in the Dalitz distribution, or if you project it onto one of the axes, you’re going to see the resonance-like shape. These bent structures you can identify on both.\n\n\n19.0.2 Score: 75\n\n\n19.0.3 Correction:\nYou might have seen cross sections for two-particle resonances. These exhibit peaks known as hadronic resonances. The underlying physics involves a two-particle system whose quantum numbers match those of a resonant state. By varying the system’s energy, one probes the interaction probability between the particles.\n\n\n\n\n\n\nThe resonance cross-section follows the Breit-Wigner formula: \\sigma(E) = \\frac{2\\pi \\hbar^2}{k^2} \\frac{\\Gamma^2}{(E - E_0)^2 + \\Gamma^2/4} where E_0 is the resonance energy, \\Gamma is the width, k is the wave number, \\hbar is the reduced Planck constant, and \\mu is the reduced mass.\n\n\n\nWhen an intermediate resonance exists, the interaction probability peaks near the resonance energy. This creates characteristic structures in the Dalitz distribution. Projections of the Dalitz plot may show resonance-like shapes, though background effects can obscure them.\n\n\n19.0.4 Comment:\nKey corrections made:\n\nFixed the Breit-Wigner formula to include the reduced Planck constant \\hbar and corrected the prefactor to \\frac{2\\pi \\hbar^2}{k^2} for dimensional accuracy.\nClarified that k is the wave number (k = \\sqrt{2\\mu E}/\\hbar).\nReplaced vague “bent structures” with “characteristic structures” for resonance bands in Dalitz plots.\nAdded caution about background effects obscuring resonances in 1D projections.\nEnsured consistent lowercase notation for s, t, u Mandelstam variables throughout.\n\nThe original description of resonance physics was fundamentally sound but required notation fixes and added precision in the formula and Dalitz plot interpretation.\n\n\n\n20 Index: 2\n\n20.0.1 Original:\nThis is a cool example that I brought because I like it a lot—there are resonances in all three pairs. There are resonances in these two, these two enter. Why is there a bigger probability increase when the proton and pion are near resonance? The K and π as well, like on the right side, but not only the two lines on the left side. Let’s understand the lines. We will come back to that in two lectures. But let’s quickly identify the different lines.\n\nHorizontal lines—which resonances are these? In which mass distribution? This is going to give us a peak horizontally. I’m looking now at the blue plot. The K and π, Nπ. If the horizontal means mass for the Kπ is fixed, then it approaches the maximum at a certain value. So all the lines are the resonances in Kπ, Nπ—and what’s the name of these particles? What particle decays to Kπ? K-star, K-star. These are K-star resonances.\nVertical lines—in which system? When I’m changing the Kπ, I’m going to scan a certain energy of the system. In which system do I scan? You can think of this as also a projectile. These lines correspond to the fixed mass of the proton and Kπ. Therefore, by moving along the axis, I’m scanning Kπ. If you expand it, you see that the lines correspond to the resonances in Kπ, proton-Kπ. I’m pointing to the proton-K. The third combination is pion-proton, and these are delta resonances.\n\nThe other plot can help me as well because it’s nice to see that the lines are going to be parallel to the sides of the triangle. This line is the same as this line, the same as this line, and it’s parallel to one of the sides. Then I have my lambda resonances parallel to the other side. At this point, you better see the delta resonance, which is the line over here, parallel to the third side. It’s a bit more tricky to see the delta resonance here, but kind of.\n\n\n20.0.2 Score: 85\n\n\n20.0.3 Correction:\nThis is a cool example that I brought because I like it a lot—there are resonances in all three pairs. Why is there a larger cross-section enhancement when the proton and pion are near resonance? The Kπ system shows similar behavior. Let’s identify the lines:\n\nHorizontal lines correspond to resonances in the Kπ system. If the horizontal axis fixes the Kπ invariant mass (m_{K\\pi}), peaks occur at resonance masses. These are K* (K-star) resonances decaying to Kπ.\n\nVertical lines correspond to resonances in the proton-Kπ system. Scanning along this axis varies the proton-Kπ invariant mass (m_{pK\\pi}), revealing baryonic resonances like Λ* states.\n\nDiagonal lines (parallel to triangle sides) correspond to resonances in the πp system, such as Δ(1232) resonances.\n\nThe Dalitz plot structure shows resonance bands parallel to the axes/sides: K* bands horizontal, Λ* bands vertical, and Δ bands diagonal.\n\n\n20.0.4 Comment:\nCorrected physics errors:\n\nStandardized resonance notation (K* instead of “K-star”).\n\nClarified axes: horizontal = fixed m_{K\\pi}, vertical = fixed m_{pK\\pi}.\n\nDefined resonance systems unambiguously (Kπ → K, pKπ → Λ, πp → Δ).\n\nRemoved conversational redundancies (“I’m pointing to…”, “you better see…”).\n\nUsed invariant mass notation (m_{K\\pi}) instead of vague “mass distribution”.\nKey physics preserved: Resonances enhance cross-sections in specific 2-body subsystems, visualized as bands in the Dalitz plot.\n\n\n\n\n21 Index: 2\n\n21.0.1 Original:\nOne thing to finish and to move past this dull spot and proceed to our topic of today is the angular distribution. I’m going to fix—I’m going to discuss now the angular distribution for a decay within one band. Let me look at the phase space resonance here. As we discussed before, the kinematics: when I’m traversing the double plot and I’m traversing the phase space from one end to another, while keeping the mass of the combination fixed, I’m changing the angle. I’m exploring the different angles. This is precisely the kinematics.\nLet me sit in the rest frame of the Kπ where this band is happening and traverse the phase space by changing this angle. This angle dependence indicates to me… Within the band, I can have inhomogeneity. Sometimes even if I’m within the band, one edge of the band has a different probability than the other. The particles like—or it’s even more common that particles prefer to be aligned and don’t like to be perpendicular. This is the less probable kinematics than this. That’s easy to believe, right? Preferences might happen.\nThis happens because particles have spin. This preference appears only because the intermediate resonance, in that case the K, is not a scalar particle—it has spin. The spin of particles causes the inhomogeneity in angular distributions and causes inhomogeneity in the decay spectrum.\n\n\n21.0.2 Score: 85\n\n\n21.0.3 Correction:\nOne thing to finish to move past this topic and proceed to our discussion is the angular distribution in resonance decays. Within a resonance band, the kinematics involve varying the decay angle while keeping the invariant mass fixed. In the rest frame of the Kπ system, changing this angle explores different regions of phase space.\nThis angular dependence can lead to anisotropy in the decay distribution. Even within a resonance band, decay probabilities may vary with angle—particles may preferentially align parallel or perpendicular to quantization axes. This anisotropy arises when the intermediate resonance has non-zero spin. The spin of the resonance induces directional preferences in the decay, resulting in non-uniform angular distributions.\n\n\n21.0.4 Comment:\nThe physics content is largely correct but required refinement in terminology and precision:\n\nReplaced “inhomogeneity” with “anisotropy” (standard term for angular dependence).\nClarified that spin-induced anisotropy occurs specifically for resonances with spin ≥1 (spin-0 particles yield isotropic distributions).\nRemoved vague phrasing (“less probable kinematics,” “easy to believe”).\nSpecified that angular variation occurs at fixed invariant mass.\nRetained correct core concepts: resonance rest frame kinematics, phase-space exploration via angle variation, and spin as the origin of anisotropy.\n\n\n\n\n22 Index: 2\n\n22.0.1 Original:\nAngular distribution is a very powerful tool to understand properties of particles. As we already discussed, that’s our way to measure spin, parity, and other quantum numbers in particle interactions. Particles with higher spin would prefer more bumpy, more spiky angular distributions, and particles with lower spin—if everything is scalar—are going to produce no asymmetries at all, no structures in angular distributions.\n\n\n\n\n\n\nThe angular distribution intensity for spin measurement follows:  I(\\theta) \\propto \\sum_{M,M'} |D^J_{M'M}(\\theta)|^2  where D^J_{M'M} are Wigner D-functions describing the rotation of spin states.\n\n\n\nBy looking at the angular distribution, especially in the rest frame of the particle decay, one examines the ratio of the aligned kinematics and other types of kinematics. With this, one can infer the information of the spin. For most of the particles that we have discovered up to now, the quantum numbers are not known. We discover particles as bumps in the spectrum, and then the next step to understand their properties is to determine their quantum numbers. This is done by looking at angular distributions.\nMost of the time, it’s as simple as looking at the Dalitz plot and seeing if there is a minimum in the angular distribution, if this line has several structures, several nodes. The maxima and nodes indicate, in the case of scalar particles in the final state, the nodes would literally tell you the spin. If you have one node, you have spin one. If you have two nodes, you have spin two. If you have three nodes, you have spin three. The intensity would really vanish at certain points in the dark spot.\nBut in the case of non-scalar particles—and most of the time, it’s non-scalar particles that interact—the situation is a little more complicated. I will have examples of scalar resonances. But here, let’s just quickly check what spin they are giving here. Is the baryon spin half? Half, right? Kaon and pion—except the scalars—they have a spin of zero. The spin of lambda is the same as the proton, but with the jump, the presence of spin averages out the distribution.\nIf you consider a certain spin projection of the lambda-C and proton, you’ll again have nodes and zeros in the angular distribution. But since we don’t measure—we don’t polarize the initial state lambda—and we don’t measure the spin of the final state, everything is averaged. Therefore, you don’t have minima, you don’t have nodes or zeros any longer; things get smeared.\n\n\n22.0.2 Score: 65\n\n\n22.0.3 Correction:\nAngular distribution is a very powerful tool to understand properties of particles. As we already discussed, that’s our way to measure spin, parity, and other quantum numbers in particle interactions. Particles with higher spin would prefer more bumpy, more spiky angular distributions in their production processes or in decays where the final state particles have spin, and particles with lower spin—if everything is scalar and the resonance is unpolarized—are going to produce no asymmetries at all, no structures in angular distributions.\n\n\n\n\n\n\nThe angular distribution intensity for spin measurement follows:  I(\\theta) \\propto \\sum_{M,M'} |D^J_{M'M}(\\theta)|^2  where D^J_{M'M} are Wigner D-functions describing the rotation of spin states.\n\n\n\nBy looking at the angular distribution, especially in the rest frame of the particle decay, one examines the ratio of the aligned kinematics and other types of kinematics. With this, one can infer the information of the spin. For most of the particles that we have discovered up to now, the quantum numbers are not known. We discover particles as bumps in the spectrum, and then the next step to understand their properties is to determine their quantum numbers. This is done by looking at angular distributions.\nMost of the time, in the case of three-body decays, one uses the Dalitz plot to analyze the distribution. For two-body decays, if the final state particles have spin or if the resonance is polarized, the angular distribution can show structures such as minima and nodes. For example, in the decay of a polarized resonance of spin J to two spinless particles, the number of nodes in the angular distribution can be related to J. However, for an unpolarized resonance decaying to two spinless particles, the angular distribution is flat and does not exhibit nodes.\nBut in the case of non-scalar particles—and most of the time, it’s non-scalar particles that interact—the situation is a little more complicated. I will have examples of scalar resonances. But here, let’s just quickly check what spin they are giving here. Is the baryon spin half? Half, right? Kaon and pion—except the scalars—they have a spin of zero. The spin of \\Lambda is the same as the proton, but in the decay process, the presence of spin and the fact that we do not measure the spin projections averages out the distribution.\nIf you consider a certain spin projection of the \\Lambda_c and proton, you’ll again have nodes and zeros in the angular distribution. But since we don’t measure—we don’t polarize the initial state \\Lambda_c—and we don’t measure the spin of the final state, everything is averaged. Therefore, you don’t have minima, you don’t have nodes or zeros any longer; things get smeared.\n\n\n22.0.4 Comment:\nThe original text contained significant errors:\n\nClaimed nodes appear in angular distributions for unpolarized decays to two spinless particles (e.g., “if you have one node, you have spin one”), which is incorrect—such decays yield flat distributions. This was corrected by clarifying that nodes only arise with polarized resonances or non-scalar final states.\n\nUsed non-standard notation “lambda-C” instead of \\Lambda_c.\n\nUnclear phrasing like “with the jump” was replaced with “in the decay process”.\n\nThe Dalitz plot discussion was adjusted to reflect its use in three-body decays, not direct angular distribution analysis.\nThe Wigner D-function formula was retained but contextualized for fixed spin projections. Physics is now consistent throughout.\n\n\n\n\n23 Index: 2\n\n23.0.1 Original:\nA particle with spin can have 2J + 1 projections to the quantization axis. Let me consider a particle that has spin J. There is a z-axis that we need to quantize the spin, and this z-axis measures the J_z upper layer is going to give us. One can think of this |JM\\rangle state as the vector of 2J + 1 components. All operators in this case are going to be matrices that act on these vectors and produce either the same state with a certain eigenvalue or a mixture of the states.\nWhen I act with the rotation on the state, I’m going to produce not a certain state, but a mixture of different states—it’s as simple as that. When I rotate the state, if it were a vector in regular space, I could probably adjust this to have exactly a certain projection. But in quantum mechanics, it doesn’t work this way. Once you act with the rotation, most of the time you’re going to end up with a mixture of all states. Let me rotate. These coefficients are tabulated, so they are known functions for any state. You can have a look and check what these coefficients are. They are called Wigner D-functions.\n\n\n\n\n\n\nThe Wigner D-functions describe rotations of spin states:  D^J_{M'M}(\\alpha,\\beta,\\gamma) = e^{-iM'\\alpha}d^J_{M'M}(\\beta)e^{-iM\\gamma}  where (\\alpha,\\beta,\\gamma) are Euler angles and d^J_{M'M} is the reduced rotation matrix.\n\n\n\nLet me be more concrete. I’m going to rotate about the Y-axis in a second. So I have a Z, I have my X, and I have my Y. XYZ. That’s correct. They should form the right-handed triple of axes X, Y, Z. I’m going to rotate about the Y-axis on |JM\\rangle. In order to do that, I have to—so J_+, as you remember, it’s \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}. So it operates like that. And J_- is the opposite \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} like that. The J_y operator then has zero on the diagonal and these coefficients above and below the diagonal.\nThen, in order to apply the transformation to the state, you have to do the matrix exponential of J_y. But this is done for you, and we know what they are. These are these coefficients, these coefficients where they are C, C, C, C. These coefficients—they need to know about the initial state as well. That’s why JM indices are kept in these notations. Imagine I tell you we work with spin-1 that is perpendicular to the Y-axis and rotated by 30 degrees. What is going to be the composition of the state? When I do that, you have everything |JM\\rangle that corresponds to this perpendicular—I mean, to the orientation along the Y-axis. Then you look up these functions, and you know what the composition is.\nWell, we have the same functions. These are coefficients so far, right? So these are numbers. But what is R in this case? Is it just θ? Okay, it’s more general. You can compute the D-functions for any R. Essentially, the more general relation would be like this: any orientation in space can be described by three angles. These are Euler angles. In the conventions we use in particle physics, you first rotate by φ about the Z-axis, then you rotate by θ about the Y-axis, and then you rotate by γ again about the Z-axis. With this, you can give any orientation to your system.\nThese regular D-functions are the product of three parts. The most difficult part is the θ rotation. Since the Z-rotation gives you just a phase. Essentially, just to relate it to \\langle M'|M\\rangle of γ, θ, φ is equal to e^{-i γ M'} d^l_{M'M}(θ) e^{-i φ M}.\n\n\n23.0.2 Score: 85\n\n\n23.0.3 Correction:\nA particle with spin can have 2J + 1 projections to the quantization axis. Let me consider a particle that has spin J. There is a z-axis that we quantize the spin along, and this z-axis defines the J_z component. One can think of the |JM\\rangle state as a vector with 2J + 1 components. All operators in this case are matrices that act on these vectors and produce either the same state with a certain eigenvalue or a superposition of states.\nWhen I rotate the state, I produce a superposition of different M states. If the state were a classical vector, I could align it exactly, but in quantum mechanics, rotations generally create mixtures. The coefficients for these superpositions are tabulated as known functions called Wigner D-functions.\n\n\n\n\n\n\nThe Wigner D-functions describe rotations of spin states:  D^J_{M'M}(\\alpha,\\beta,\\gamma) = e^{-iM'\\alpha}d^J_{M'M}(\\beta)e^{-iM\\gamma}  where (\\alpha,\\beta,\\gamma) are Euler angles and d^J_{M'M} is the reduced rotation matrix.\n\n\n\nTo be concrete, consider rotating |JM\\rangle about the Y-axis. The J_y operator has off-diagonal matrix elements. The rotation operator is given by the matrix exponential of J_y, but the result is encapsulated in the Wigner functions. These coefficients depend on the initial and final states, which is why the J, M, and M' indices are retained. For example, for a spin-1 state initially quantized along the Y-axis and rotated by 30°, the composition is found using these functions.\nThe D-functions are defined for arbitrary rotations parameterized by Euler angles. In particle physics conventions, a general rotation R consists of:\n\nRotation by \\phi about the Z-axis,\n\nRotation by \\theta about the Y-axis,\n\nRotation by \\gamma about the (new) Z-axis.\n\nThe matrix element is:\n\\langle J, M' | R(\\phi, \\theta, \\gamma) | J, M \\rangle = e^{-i\\gamma M'} d^J_{M'M}(\\theta) e^{-i\\phi M}.\nThe \\theta-dependent term d^J_{M'M}(\\theta) is the core nontrivial component, as rotations about Z-axes only contribute phases.\n\n\n23.0.4 Comment:\n\nRemoved inconsistent 2×2 matrix examples for J_± (specific to spin-1/2) since the context is general spin J.\nCorrected d^l_{M'M} \\to d^J_{M'M} to match standard notation.\nClarified the Euler angle decomposition and the matrix element expression, explicitly including J in states.\nRemoved redundant phrasing and minor grammatical issues for coherence.\nPhysics is now consistent: rotations mix M states via Wigner D-functions, operators are (2J+1)-dimensional matrices, and the final formula uses standard conventions.\n\n\n\n\n24 Index: 2\n\n24.0.1 Original:\nThink we need an example—a very small one—of spin one-half. I’m going to rotate the state |1/2, 1/2\\rangle by 30 degrees about the Y axis. I’m going to get a |1/2, 1/2\\rangle plus |1/2, -1/2\\rangle combination, and I want you to quickly tell me what are the numbers here. These coefficients are sitting in the same place—where sitting in the same page as the Clebsch-Gordan coefficients. They are closely related. This is about the SU(2) group, and you just open the Clebsch-Gordan table and check the numbers.\nSince 30 degrees is not at all trivial to do it, but now we won’t have time to look at details, and hopefully in the seminar we will explore a little bit more. But I just remember this table because it’s super simple. So the D for m' = m = +1/2 is the cosine minus sine, sine cosine of the half angle. I should have picked 60 degrees because for 15 degrees it’s a little bit inconvenient. Then what is going to be here is the cosine of 15 degrees. What’s going to be here is the sine of 15 degrees.\n\n\n24.0.2 Score: 70\n\n\n24.0.3 Correction:\nConsider rotating the spin-1/2 state |1/2, 1/2\\rangle by an angle \\beta about the Y-axis. The rotation operator R_y(\\beta) acts on the states as:\n\nR_y(\\beta) = \\begin{pmatrix}\n\\cos(\\beta/2) & -\\sin(\\beta/2) \\\\\n\\sin(\\beta/2) & \\cos(\\beta/2)\n\\end{pmatrix}.\n\nFor \\beta = 30^\\circ, the rotated state is:\n\n\\cos(15^\\circ) |1/2, 1/2\\rangle - \\sin(15^\\circ) |1/2, -1/2\\rangle.\n\nThese coefficients derive from the Wigner D-matrix for j=1/2, which uses half-angles and matches the Clebsch-Gordan tables for SU(2).\n\n\n24.0.4 Comment:\nThe original text contained physically correct statements about half-angles in spin-1/2 rotations and SU(2) group matrices but had confusing phrasing (“I should have picked 60 degrees…”) and lacked explicit formulas. The correction:\n\nClarifies the rotation matrix structure.\n\nFixes notation (explicit R_y(\\beta), matrix form).\n\nRemoves ambiguous remarks while preserving the valid core physics (half-angle dependence, Clebsch-Gordan connection).\n\nRetains |j, m\\rangle notation and correct angle handling.\n\n\n\n\n25 Index: 2\n\n25.0.1 Original:\nLet me stress what we discussed so far was not about weak interactions or strong interactions. It was about rotations and the rotational group. That’s the fun part—and that’s something that still impresses me—that in order to understand how particles behave and what the angular distributions are, you need little from the strong interaction. You only need the general properties of the rotational group, so to speak.\nAngular distributions are determined by general properties of how space is rotated, plus the little bit we need from strong interactions: what is the preference for which spin particles are produced, and that’s what strong interactions tell us. But how they decay and what is the asymmetry in the kinematics—this is determined by the quantum group. That’s amazing. Therefore, we can now move on and have a recipe, or a general way, to construct any particle decay chain and figure out what the angular distribution is going to be.\n\n\n25.0.2 Score: 95\n\n\n25.0.3 Correction:\nNo correction necessary.\n\n\n25.0.4 Comment:\nThe input text contains no physics errors requiring correction. All statements about rotational symmetry governing angular distributions, the limited role of strong interactions in spin preferences, and quantum group properties determining decay kinematics are physically accurate. The term “quantum group” is contextually acceptable (though “Poincaré group” is more conventional in particle physics), and no unclear phrases or incorrect notations are present. Minor stylistic improvements are possible but not necessary for physical correctness.\n\n\n\n26 Index: 3\n\n26.0.1 Original:\nLet’s now look at the particle X. The particle X carries the spin projection \\lambda_X. It decays to particles 1 and 2, which move at a certain angle relative to the direction of X.\n\n\n26.0.2 Score: 100\n\n\n26.0.3 Correction:\nNo correction necessary.\n\n\n26.0.4 Comment:\nThe provided text is physically correct. The notation for spin projection \\lambda_X is standard in particle physics, and the description of the decay angle relative to the parent particle’s direction is appropriate when interpreted in the rest frame of X with the quantization axis aligned along its lab-frame motion. No ambiguous, unclear, or erroneous elements require modification.\n\n\n\n27 Index: 3\n\n27.0.1 Original:\nTo compensate for this angle, we must adjust the quantization axis. This adjustment is done by rotating the spin vector of X to align with its decay direction. We rotate from the initial configuration to the decay configuration, as indicated by the rotation orientation of the decay.\n\n\n\n\n\n\nThe Wigner D-matrix is used to rotate spin states: \nD^{J}_{\\lambda,\\lambda'}(\\alpha,\\beta,\\gamma)\n where J is the total angular momentum, \\lambda,\\lambda' are spin projections, and (\\alpha,\\beta,\\gamma) are Euler angles.\n\n\n\n\n\n27.0.2 Score: 95\n\n\n27.0.3 Correction:\nNo correction necessary.\n\n\n27.0.4 Comment:\nThe provided text accurately describes the rotation of spin states using the Wigner D-matrix. The notation is physically correct:\n\nJ properly denotes total angular momentum\n\n\\lambda and \\lambda' correctly represent spin projections\n\nEuler angles (\\alpha, \\beta, \\gamma) are standard parameters\n\nThe mathematical expression D^{J}_{\\lambda,\\lambda'}(\\alpha,\\beta,\\gamma) uses appropriate conventions.\nNo ambiguous statements, incorrect variables, or unclear explanations are present. The physics of aligning quantization axes via spin rotations is valid.\n\n\n\n\n28 Index: 3\n\n28.0.1 Original:\nI would like to evaluate this expression in the aligned kinematics, where \\phi = \\theta = 0. In the center-of-mass (CM) frame, the expression simplifies.\nWhen the angles are zero, the transformation reduces significantly. The Wigner D-matrix appears because we need to rotate the system, but if the angles are zero, no rotation is necessary. This simplifies the summation over \\lambda_X.\nSince X is already moving along the z-axis, no rotation is needed. The expression becomes: \n\\sum_{\\lambda_X, \\lambda_3} D^{J_X}_{0, \\lambda_X - \\lambda_3}(0, 0, 0)\n and another term involving \\lambda_1, \\lambda_2: \nD^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta, \\phi)\n\n\n\n28.0.2 Score: 85\n\n\n28.0.3 Correction:\nI would like to evaluate this expression in the aligned kinematics, where \\phi = \\theta = 0.\nIn the center-of-mass (CM) frame, the expression simplifies.\nWhen the angles are zero, the transformation reduces significantly.\nThe Wigner D-matrix appears because we need to rotate the system, but if the angles are zero, no rotation is necessary.\nThis simplifies the summation over \\lambda_X.\nSince X is already moving along the z-axis, no rotation is needed.\nThe expression becomes:\n\n\\sum_{\\lambda_X, \\lambda_3} D^{J_X}_{0, \\lambda_X - \\lambda_3}(0, 0, 0)\n\nand another term involving \\lambda_1, \\lambda_2:\n\nD^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(0, 0)\n\n\n\n28.0.4 Comment:\nThe original text contained inconsistent notation for the second Wigner D-matrix, where angles (\\theta, \\phi) were retained despite specifying \\phi = \\theta = 0 kinematics. This was corrected by setting the angles to (0, 0) in the second D-matrix. All physics statements are now consistent:\n\nIn aligned kinematics (\\theta = \\phi = 0), rotations are trivial, so D-matrices reduce to Kronecker deltas.\n\nThe summation over \\lambda_X simplifies correctly due to the absence of rotation.\n\nFour-vector notation (z-axis alignment) and angular momentum labels (\\lambda_i) are physically sound.\n\n\n\n\n29 Index: 3\n\n29.0.1 Original:\nThis results in a delta function constraint due to angular momentum conservation: \n\\delta_{\\lambda_0, \\lambda_X - \\lambda_3}\n The only way to avoid rotation is if the state remains unchanged, meaning \\lambda_X is constrained.\n\n\n29.0.2 Score: 70\n\n\n29.0.3 Correction:\nThis results in a delta function constraint due to angular momentum conservation:\n\n\\delta_{\\lambda_0,\\,\\lambda_X - \\lambda_3}\n\nwhich enforces the helicity conservation condition \\lambda_0 = \\lambda_X - \\lambda_3. This constraint arises from rotational invariance about the z-axis.\n\n\n29.0.4 Comment:\nThe original formula \\delta_{\\lambda_0, \\lambda_X - \\lambda_3} is physically correct but lacks clarity in notation; a comma with space improves readability. The vague phrase “avoid rotation” is replaced with a precise statement about rotational invariance. The condition \\lambda_0 = \\lambda_X - \\lambda_3 is explicitly stated, and the misleading implication that \\lambda_X alone is constrained is corrected (the constraint involves all three helicities). The delta function notation is standard for helicity conservation in scattering amplitudes.\n\n\n\n30 Index: 3\n\n30.0.1 Original:\nThe final expression is: \n\\mathcal{H}_0 \\, D^{J_X}_{\\lambda_X, \\lambda_3}(\\theta, \\phi)\n where \\lambda_X = \\lambda_0 + \\lambda_3.\nThe complete decay amplitude is: \n\\mathcal{H}_0 \\, D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_3}(\\theta, \\phi) \\, D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_1 - \\lambda_2}(\\theta, \\phi)\n\n\n\n30.0.2 Score: 50\n\n\n30.0.3 Correction:\nThe complete decay amplitude is:\n\n\\mathcal{H}_0 \\, D^{J_X}_{\\lambda_X, \\lambda_3}(\\theta, \\phi)  \n\nwhere \\lambda_X = \\lambda_0 + \\lambda_3.\n\n\n30.0.4 Comment:\nThe original second expression incorrectly included two Wigner D-functions, which is physically erroneous for a single decay amplitude. In standard particle decay formalism, the angular distribution is described by one Wigner D-function D^{J_X}_{\\lambda_X, \\lambda_3}(\\theta, \\phi), where \\lambda_X = \\lambda_0 + \\lambda_3 enforces helicity conservation. The extraneous D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_1 - \\lambda_2}(\\theta, \\phi) term was removed. The first expression and helicity definition are correct and retained. Notation (J_X, \\lambda_i, \\theta, \\phi) is physically appropriate.\n\n\n\n31 Index: 3\n\n31.0.1 Original:\nThat’s it—as simple as that.\n\n\n31.0.2 Score: 50\n\n\n31.0.3 Correction:\nNo correction necessary.\n\n\n31.0.4 Comment:\nThe input phrase “That’s it—as simple as that.” contains no physics content, formulas, or technical terminology requiring correction. It serves as a neutral concluding statement without introducing errors. Since it lacks any substantive physics material, none of the specified improvements (e.g., Mandelstam variables, spin notation) apply here. No modifications are needed.\n\n\n\n32 Index: 3\n\n32.0.1 Original:\n\n**Horizontal rules (`\n\n\n\n32.0.2 Score: 0\n\n\n32.0.3 Correction:\n\n**Horizontal rules (`\n\n\n\n32.0.4 Comment:\nThe input consists solely of an incomplete markdown header fragment (“1. **Horizontal rules (`“) with no substantive content. This fragment lacks any meaningful physics statements, formulas, or context to evaluate or correct. Since the text contains no analyzable physics material, no corrections can be applied, and the output is”None” as per the task guidelines for unclear content.\n\n\n\n33 Index: 3\n\n33.0.1 Original:\n`)** to separate logical blocks.\n\nBold emphasis for critical terms like 1, 2, and aligned kinematics.\nCallout block for the Wigner D-matrix definition, placed contextually where rotation is first discussed.\nBlock math (`...) for all major equations, properly spaced.\nInline math (...`) for variables like \\lambda,\\lambda' inside the callout.\n\nNo sentences were removed or altered, and the helping material was integrated only where directly relevant.\n\n\n33.0.2 Score: 0\n\n\n33.0.3 Correction:\nNo correction necessary.\n\n\n33.0.4 Comment:\nThe input text to be corrected is missing. Without the actual content, it is impossible to identify physics errors, apply corrections, or evaluate clarity. The provided instructions and error list reference absent material, making the task unactionable. Corrections require specific textual content to analyze.\n\n\n\n34 Index: 4\n\n34.0.1 Original:\nHow many numbers do I need from you in order to compute? Let me think about electromagnetic interactions, or gravity. How many numbers as input do I need to predict the angular distribution? It’s essentially here, but it misses fundamental components.\nWhat’s inside the blobs? What is inside of the blobs? What’s inside of this block, this blob, or that blob? To predict all the values, I just need these two quantities.\n\nI have (2j_1 + 1) \\times (2j_2 + 1) values here, which might depend on particle masses—say, the masses of x.\nI also need a similar number for these terms, but there’s a reasonable way to approximate them.\n\n\n\n\n\n\n\nThe spin multiplicity factor (2j_1 + 1)(2j_2 + 1) accounts for the number of possible spin states for particles with spins j_1 and j_2.\n\n\n\n\n\n34.0.2 Score: 60\n\n\n34.0.3 Correction:\nHow many numbers do I need to compute angular distributions in electromagnetic interactions or gravity? The spin multiplicity factor (2j_1 + 1)(2j_2 + 1) accounts for the number of possible spin states for particles with spins j_1 and j_2.\n\n\n34.0.4 Comment:\nI retained the physically correct statement about the spin multiplicity factor but removed the following problematic sections:\n\nVague references to “blobs” and their undefined contents.\n\nAmbiguous claims about “two quantities” being sufficient without justification.\n\nThe unclear phrase “masses of x” (undefined variable x).\n\nRedundant/confusing sentences like “it misses fundamental components” and “reasonable way to approximate.”\nThe core physics concept (spin multiplicity) is preserved with correct notation.\n\n\n\n\n35 Index: 4\n\n35.0.1 Original:\nOften, in experiments or initial analyses, we assume these are constant, containing only particle properties. Here, I’ll say this one is a constant c, and this is particle-dependent. Once I do that, I should be able to compute the angular distribution.\nIn that case, I fix the mass of one particle and the intensity observed along the line. What we have so far is that these two quantities determine \\frac{d\\Gamma}{d(\\cos\\theta)}.\nUsing \\cos\\theta is better—it has a simpler Jacobian. We avoid the \\sin\\theta Jacobian for \\cos\\theta. That’s why we often analyze \\cos\\theta, and this distribution is proportional to the matrix element squared |M|^2.\n\n\n35.0.2 Score: 85\n\n\n35.0.3 Correction:\nOften, in experiments or initial analyses, we assume these are constant, containing only particle properties.\nHere, I’ll say this constant c is particle-dependent, such as a coupling constant.\nOnce I do that, I should be able to compute the angular distribution.\nIn that case, I fix the mass of one particle and consider the differential decay rate.\nWhat we have so far is that these two quantities determine \\frac{d\\Gamma}{d(\\cos\\theta)}.\nUsing \\cos\\theta is better—it has a simpler Jacobian. We avoid the \\sin\\theta Jacobian for \\theta.\nThat’s why we often analyze \\cos\\theta, and this distribution is proportional to the matrix element squared |M|^2.\n\n\n35.0.4 Comment:\nThe physics content was largely correct, but two improvements were needed:\n\n“Intensity observed along the line” was vague and non-standard; replaced with “differential decay rate” for physical precision.\n\nClarified “particle-dependent” by specifying c as a coupling constant (or form factor) to avoid ambiguity.\nNotation for \\frac{d\\Gamma}{d(\\cos\\theta)} and |M|^2 was correct, and the Jacobian explanation for \\cos\\theta is accurate.\n\n\n\n\n36 Index: 4\n\n36.0.1 Original:\nThis |M|^2 is fixed. The distribution ranges from -1 to 1. Here, \\theta and \\cos\\theta are plotted: \\theta = 0 corresponds to \\cos\\theta = -1. We scan from -1 to 1, and if the distribution is flat, that’s one possibility.\nFor particles with spin, you often see:\n\nA parabola—a second-order polynomial in \\cos\\theta.\nOr you might see this shape.\n\n\n\n\n\n\n\nThe angular distribution \\frac{d\\Gamma}{d(\\cos\\theta)} \\propto |\\mathcal{M}|^2 is directly tied to the quantum transition amplitude A, where |\\mathcal{M}|^2 = |A|^2. Experiments only measure |A|^2.\n\n\n\n\n\n36.0.2 Score: 65\n\n\n36.0.3 Correction:\nThis |\\mathcal{M}|^2 is fixed. The distribution ranges from -1 to 1 in \\cos\\theta.\nHere, \\theta and \\cos\\theta are plotted: \\theta = 0 corresponds to \\cos\\theta = 1.\nWe scan \\cos\\theta from -1 to 1, and if the distribution is flat, that’s one possibility.\nFor particles with spin, you often see:\n\nA parabolic shape proportional to \\cos^2\\theta.\n\n\n\n\n\n\n\nThe angular distribution \\frac{d\\Gamma}{d(\\cos\\theta)} \\propto |\\mathcal{M}|^2 arises from the quantum transition amplitude A, where |\\mathcal{M}|^2 = |A|^2. Experiments measure \\frac{d\\Gamma}{d(\\cos\\theta)}, from which |A|^2 is inferred.\n\n\n\n\n\n36.0.4 Comment:\n\nCritical error: \\theta = 0 implies \\cos\\theta = 1, not -1 (corrected).\n\nVague phrases (“this shape”) removed; specified parabolic dependence as \\cos^2\\theta.\n\nClarified experimental measurement: Experiments directly measure \\frac{d\\Gamma}{d(\\cos\\theta)}, not |A|^2 itself.\n\nNotation (\\mathcal{M}, \\cos\\theta) and physics (fixed |\\mathcal{M}|^2, spin-dependent distributions) are otherwise correct.\n\n\n\n\n37 Index: 4\n\n37.0.1 Original:\nNotice the difference. It’s important to remember that A, the quantum transition amplitude, is a probability amplitude. It’s squared to give the observed probability. This G will appear squared. In experiments, we only measure |A|^2.\nMoreover, we often deal with unpolarized decays, so the distributions are averaged. You square the amplitude, sum over initial and final spin projections, and obtain what’s observed experimentally.\nThen you ask: What does this tell me?\n\n\n37.0.2 Score: 70\n\n\n37.0.3 Correction:\nNotice the difference. It’s important to remember that A, the quantum transition amplitude, is a probability amplitude.\nIt’s squared to give the observed probability. This A will appear squared.\nIn experiments, we only measure |A|^2.\nMoreover, we often deal with unpolarized decays, so the distributions are averaged.\nYou square the amplitude, average over initial spin projections and sum over final spin projections, and obtain what’s observed experimentally.\nThen you ask: What does this tell me?\n\n\n37.0.4 Comment:\n\nCorrected “G” to “A” for consistency with the defined quantum transition amplitude.\n\nRevised the spin handling for unpolarized decays: averaging over initial spins (division by the number of initial spin states) and summing over final spins is standard. The original phrase “sum over initial and final spin projections” omitted averaging and was physically inaccurate.\n\nNo other physics errors found; the core concepts of probability amplitudes, squaring, and experimental measurement are correct.\n\n\n\n\n38 Index: 4\n\n38.0.1 Original:\nThe first step in analysis isn’t to guess the amplitude but to project the angular distributions onto orthogonal polynomials. This gives a nice basis for expansion.\nFunctions from -1 to 1 can be expanded in Legendre polynomials P_\\ell(\\cos\\theta). These polynomials are related to the spin of the produced particle.\nThis is called partial wave analysis. If you project the differential cross section, it’s called moment analysis.\n\n\\frac{d\\sigma}{d(\\cos\\theta)} = \\sum_{\\ell} a_\\ell P_\\ell(\\cos\\theta)\n\nPartial wave analysis models the cross section with amplitudes, treating these terms as free parameters. The goal is to infer what’s inside the blobs by fitting them to data.\n\n\n38.0.2 Score: 95\n\n\n38.0.3 Correction:\nNo correction necessary.\n\n\n38.0.4 Comment:\nThe provided text on partial wave analysis is physically accurate and well-structured. Key strengths include:\n\nCorrect definition of Legendre polynomial expansion for angular distributions: P_\\ell(\\cos\\theta)\n\nProper distinction between partial wave analysis (amplitude modeling) and moment analysis (cross section projection)\n\nAccurate formula for differential cross section: \\frac{d\\sigma}{d(\\cos\\theta)} = \\sum_{\\ell} a_\\ell P_\\ell(\\cos\\theta)\n\nClear explanation of how amplitudes are used to infer internal dynamics via data fitting\nNotation is consistent (\\ell, \\theta, d(\\cos\\theta)), and all statements align with standard particle physics formalism. No errors require correction.\n\n\n\n\n39 Index: 5\n\n39.0.1 Original:\nI didn’t tell you much about the differences between the canonical state that we introduced at the beginning and the helicity state that we introduced later. We only touched a little bit on how the state is defined in the rest frame, and hopefully we will explore more.\n\n\n\n\n\n\nThe helicity operator is defined as: \n\\hat{h} = \\frac{\\mathbf{J} \\cdot \\mathbf{p}}{|\\mathbf{p}|}\n where \\mathbf{J} is the total angular momentum operator and \\mathbf{p} is the momentum.\n\n\n\n\n\n39.0.2 Score: 95\n\n\n39.0.3 Correction:\nNo correction necessary.\n\n\n39.0.4 Comment:\nThe provided content is physically correct and clearly presented. The definition of the helicity operator \\hat{h} = \\frac{\\mathbf{J} \\cdot \\mathbf{p}}{|\\mathbf{p}|} uses standard notation (bold for vectors), properly identifies \\mathbf{J} as the total angular momentum operator, and accurately describes the helicity-state framework. No errors in physics, notation, or logical flow are present. The note about the rest frame is appropriately contextualized.\n\n\n\n40 Index: 5\n\n40.0.1 Original:\nI would like to tell you that this book has the best coverage of this subject. This is Martin Spearman’s Elementary Particle Theory, and Chapter Four is fundamental. It’s really fun reading because:\n\nIt starts from the Lorentz group.\nIt tells you how to introduce the vectors, a little bit of group theory, but in a nice way without heavy details, without too much mass.\n\nIt’s really a good book. So Chapter Four of Martin Spearman’s Elementary Particle Physics would give you some insights on the particle definitions.\n\n\n40.0.2 Score: 95\n\n\n40.0.3 Correction:\nNo correction necessary.\n\n\n40.0.4 Comment:\nThe input text contains no physics errors or incorrect notation. All statements about the book’s content (Lorentz group, vectors, group theory, particle definitions) are physically valid. The text is clear, coherent, and requires no corrections. The absence of formulas or technical details means there are no notational issues to address.\n\n\n\n41 Index: 5\n\n41.0.1 Original:\nI left you an exercise. There are some Dalitz plots from CLEO and BaBar, and I had the labels removed. I don’t tell you which particles are in the final state. I just tell you that:\n\nOne of them is the D decay.\nAnother one is the D-sub-S decay.\n\nAnd you know a lot about kinematics already.\n\n\n\n\n\n\nFor Dalitz plot kinematics, the invariant masses of decay products satisfy: \nm_{12}^2 + m_{23}^2 + m_{31}^2 = m_D^2 + m_1^2 + m_2^2 + m_3^2\n where m_{ij}^2 are the invariant masses of two decay products, and m_D is the mass of the decaying D-meson.\n\n\n\n\n\n41.0.2 Score: 95\n\n\n41.0.3 Correction:\nNo correction necessary.\n\n\n41.0.4 Comment:\nThe provided content is physically accurate. The Dalitz plot kinematics formula is correct:\nm_{12}^2 + m_{23}^2 + m_{31}^2 = m_D^2 + m_1^2 + m_2^2 + m_3^2\nwhere m_{ij} denotes the invariant mass of two decay products, m_D is the decaying D-meson mass, and m_1, m_2, m_3 are the final-state particle masses. Notation is consistent (lowercase for masses), and the experimental context (CLEO/BaBar, D vs. D_s decays) is appropriate. No physics errors or ambiguous statements require correction.\n\n\n\n42 Index: 5\n\n42.0.1 Original:\nThe exercise is to figure out what decay this is. The axis labels are still there, but you just do not know what the mass is. From the kinematics, you can figure out the masses and perhaps guess what the decay is. I have cases. I would give you one for your group, and then you’d get another one, and then the EP one.\n\n\n42.0.2 Score: 90\n\n\n42.0.3 Correction:\nNo correction necessary.\n\n\n42.0.4 Comment:\nThe provided content is physically correct and clear. It describes an exercise in decay process identification using kinematic analysis and mass determination without introducing any physics errors. None of the specified flaws (incorrect Mandelstam notation, spin parity misrepresentation, unclear phrases, or irrelevant personal references) appear in this segment. The term “EP one” is contextually acceptable as an abbreviation (e.g., for “Electron-Positron” or similar), and the pedagogical structure (group assignments, case studies) is logically sound."
  },
  {
    "objectID": "2024-Lecture-05-repeat.html",
    "href": "2024-Lecture-05-repeat.html",
    "title": "",
    "section": "",
    "text": "0.1 Angular Distributions, Kinematics Variables, and Dalitz Plots in Two- and Three-Body Scattering\nToday we are at lecture number five. We’ll discuss angular distributions and partial wave analysis. But before going there, I would like to start with a recap.\n\nIn the last lecture, we discussed:\n\nThe phase space for particle reactions\nDifferent experiments and their kinematics\nA list of experiments worldwide studying hadrons, their production mechanisms, and peculiarities\n\n\nWe start with a recap on kinematics. The first question: How many variables does one need to describe the two-particle scattering process? We have two problems:\n\nScalar particles (e.g., 0-minus scalars):\n\n\nScattering of 0-minus → 0-minus, with two 0-minus scalar particles in the final state.\n\n\nParticles with spin (e.g., P-plus):\n\n\nScattering of 0-minus → 3-minus and 1-plus.\n\n\n\n\n\n\n\nThe scattering process is represented by a “blob” in diagrams. These are unitarity diagrams (not Feynman diagrams), illustrating interactions without specifying dynamics.\n\n\n\n\nThe question: How many variables describe the full kinematics?\n\nThe blob’s internal dynamics (strong, EM, gravity) do not affect the answer.\nTwo cases:\n(A) Particles without spin\n(B) Particles with spin\n\n\n\n\n\n\n\nFor (C), provide an example of a variable combination that fully describes the process.\n\n\n\n\n\nStart with 8 variables (2 particles × 4-momenta, minus 4 conservation constraints).\nSubtract 6 more (3 rotations + 3 boosts) due to lack of reference frame.\n\n\nFinal answer: 2 variables for (A).\n\nFor (B), the answer is also 2 variables, but the amplitude becomes a higher-rank object (e.g., 21-dimensional for spin-3 × spin-1).\n\n\n\n\n\n\nKey Insight: Spin adds more amplitudes, but all depend on the same two variables (s and t). Decays of final-state particles introduce additional variables, but for pure 2→2 scattering, two suffice.\n\n\n\n\nThe most common choice for kinematic variables: \ns = (p_1 + p_2)^2 \\quad \\text{(center-of-mass energy squared)}\n \nt = (p_1 - p_3)^2 \\quad \\text{(momentum transfer squared)}\n \nu = (p_1 - p_4)^2 \\quad \\text{(crossing-related invariant)}\n with the constraint s + t + u = \\sum m_i^2.\n\n\n\n\n\n\nThese are Lorentz-invariant—they don’t depend on the reference frame.\n\n\n\n\n\nHenrik’s choice: Masses of particle pairs (e.g., m_{12}^2, m_{13}^2).\nSven’s choice: s and u (equivalent to s and t via linear combinations).\nFavorite choice: Center-of-mass energy (\\sqrt{s}) and scattering angle (\\theta).\n\n\n\n\n\n\n\nAny two independent variables work, but some choices (like Mandelstam) avoid phase-space folding.\n\n\n\n\nFor three-body decay, the kinematics are similarly described by two variables (e.g., s_{12} and s_{23}).\nThe phase space is flat in these variables: \n\\frac{dN}{ds_{12} \\, ds_{23}} = \\text{constant (if no dynamics)}\n\n\n\n\n\n\n\nDalitz plots visualize dynamics in three-body decays. A uniform distribution implies no resonant interactions; deviations reveal underlying physics.\n\n\n\n\nFor multi-body decays, phase space can be decomposed using: \nd\\Phi_n = d\\Phi_{n-1} \\cdot d\\Phi_2 \\cdot \\frac{1}{(2\\pi)^3} dp^2\n\nThis yields for three-body decays: \nd\\Phi_3 = \\frac{1}{8\\pi^2} \\frac{2p}{\\sqrt{s}} \\frac{d\\Omega}{4\\pi}\n\n\n\n\n\n\n\nThe Jacobian for transforming to Mandelstam variables is constant, so Dalitz plots directly reflect dynamics.\n\n\n\n\n\n0.2 Charm Decay Dynamics and Dalitz Plot Analysis of \\Lambda_c \\to p K \\pi\nYou can see the example of the triple decay that I have here of the \\Lambda_c baryon going to the proton, kaon, and pion. We measure \\Lambda_c produced in proton-proton collisions or any other collisions. In the BES experiment and Belle experiment, they observe \\Lambda_c.\n\n\n\n\n\n\nThe \\Lambda_c is one of the particles that lives a long time and is produced abundantly. Particles with charm ground states are produced abundantly, and they live sufficiently long to fly from the primary vertex.\n\n\n\nWe reconstruct them, which is why we have a good sample and a good understanding of their decay kinematics—and dynamics as well, the content of this blob.\n\nIn that decay, you see there is a charm quark in the initial state and no charm quark in the final state, indicating this happens via weak interaction. The charm quark disappears between the initial and final states. The charm quark decays, transitioning into the strange quark that ends up in the kaon.\nThe c \\to s transition happens within one generation, and this is not suppressed—it is an allowed process. This is the golden channel for reconstructing because in the final state you have three charged particles—no neutrals.\n\nThe proton is a nice charged particle; it travels and is stable.\nThe kaon is stable in our experiments, and the pion is stable.\n\nThey fly out from the decay without any distraction, and we see the tracks clearly through all detectors. We observe them pointing away from the primary interaction.\n\nThere is around a 10-millimeter shift in \\Lambda_c decay vertices between the primary and secondary vertex—roughly one centimeter. This is due to the boost and the fact that \\Lambda_c in the laboratory frame lives longer than in its rest frame.\n\n\\Delta x = \\gamma v \\tau_0 \\approx \\text{10 mm (observed)}\n\nwhere \\gamma = E_{\\Lambda_c}/m_{\\Lambda_c} is the Lorentz factor.\nIt has a few hundred GeV when produced in proton-proton collisions at the LHC. This is a super nice decay, and we’ve studied it extensively.\n\nHere is the experimental result of the analysis, which resembles the data. If I showed you the actual experimental data, you wouldn’t distinguish it from this plot because the statistics are so high that the distribution is very smooth.\n\nX-axis: Mass of the proton-kaon system m_{PK}^2 = (p_P + p_K)^2.\nY-axis: Mass of the kaon-pion system m_{K\\pi}^2 = (p_K + p_\\pi)^2, covering all allowed values for the decay.\n\nThe colored region shows kinematically allowed configurations, while the white area corresponds to forbidden kinematics.\nIf I select a point inside the plot, I can compute the angles between particles and reconstruct the kinematics. But if you ask about the white area, I’d find that energy is not conserved—those points are impossible. The range of possible values for invariants is limited, and this surface is called the Dalitz plot.\n\nDifferent colors on this plot indicate different probabilities for the decay to occur. We measure the decay, reconstruct the particle tracks, and determine the kinematic point since there’s a direct relation between four-vectors and the kinematics. Certain kinematics are more probable than others—particles prefer specific directions.\nFor this decay, some configurations are rarer than others. You should be able to identify this pattern in the plot. Here’s the hint: particles are aligned in one line on the border of the plot. Inside the surface, they always have an angle between them, but on the border, they are collinear. Think about how to maximize the mass and where this point lies.\n\nAny thoughts? You should look at the bottom right. To maximize the mass of PK, consider the three momenta in opposite directions. The sum of the momentum vectors should be as small as possible. If you add them, the forward momentum—the sum of the squares—should be as large as possible. So we should be on the right side of the diagram.\nFor the minimal mass, the three momenta are aligned, and we subtract them. If you boost to the rest frame of K and \\pi, they fly next to each other with small relative momentum. In their rest frame, they might both be at rest, so their mass would just be the sum of their masses—this is the minimum.\nWe are looking for the minimal mass of the kinematics. This point corresponds to the case where two particles have maximal momentum and go back-to-back. This plot is from experimental data, and we reconstruct this in the lab frame where everything is boosted. Even if the proton is at rest in the lab frame, it’s already boosted. This point is the maximum mass, where particles go back-to-back.\n\nFor three-body decay, we define the angle similarly. Let me fix the mass of the K\\pi system. In the center-of-momentum frame, I have three particles with zero net momentum. If I boost to the K\\pi rest frame, they go back-to-back. The length of the vectors is fixed, and only the angle \\theta changes. I explore the full range of \\theta from 0 to \\pi.\nThe right side corresponds to \\theta = 0, and the left to \\theta = \\pi. For the proton-K system, \\theta = 0 gives a high mass, while \\theta \\approx \\pi gives a small mass. The same logic applies to the other pair.\nThere is a third variable in 2-to-2 scattering called U, but for three particles, it’s symmetric. If I fix the mass of one system and scan the angle, the U variable is a linear combination of the other two. This line represents the fixed mass of the \\pi p system.\n\nThe standard Dalitz plot uses m_{PK}^2 and m_{K\\pi}^2 on the axes. A more symmetric representation uses an equilateral triangle, where distances to the sides correspond to masses. This is a linear transformation of the standard plot, involving \\sqrt{3}/2 due to the 60° angles.\nThe goal is to understand the dynamics—why certain kinematics are more probable. In future lectures, we’ll see that intermediate resonances enhance the decay probability. Two particles briefly form a resonance, increasing the likelihood of decay at specific energies.\n\nYou’ve seen cross sections for two-particle resonances with bumps indicating hadronic resonances. If the quantum numbers match, the system resonates at certain energies, increasing interaction probability. This creates structures in the Dalitz plot—projecting them shows resonance-like shapes.\nIn this decay, resonances appear in all three pairs:\n\nHorizontal lines: Resonances in K\\pi (e.g., K^*).\nVertical lines: Resonances in pK.\nDiagonal lines: Resonances in \\pi p (e.g., \\Delta).\n\nThe symmetric plot shows these as lines parallel to the triangle’s sides. The angular distribution within a resonance band reveals spin effects. If I fix m_{K\\pi} and vary \\theta, the probability changes because the intermediate resonance (e.g., K^*) has spin. Particles prefer certain alignments—this inhomogeneity comes from spin.\nThe decay rate depends on the angle as:\n\n\\frac{d\\Gamma}{d\\cos\\theta} \\propto 1 + \\alpha \\cos^2\\theta\n\nwhere \\alpha depends on the resonance spin. This explains why some configurations are more probable than others.\n\n\n0.3 Angular Distributions, Spin, and Wigner D-Functions in Particle Decays\nVery good. So 15 minutes before the end, we start with the lecture of today.\n\n\n\n\n\n\nAngular distribution is a very powerful tool to understand properties of particles. As we already discussed, that’s our way to measure spin, parity, and other quantum numbers in particle interactions.\n\n\n\n\nParticles with higher spin would prefer more bumpy, more spiky angular distributions.\nParticles with lower spin—if everything is scalar—are going to produce no asymmetries at all, no structures in angular distributions.\n\nBy looking at the angular distribution, especially in the rest frame of the particle decay, one examines the ratio of the aligned kinematics and other types of kinematics. And then, with this, one can infer the information about the spin.\n\nFor most of the particles that we have discovered up to now, the quantum numbers are not known. We discover particles as bumps in the spectrum, and then the next step—in order to understand their properties—is to determine their quantum numbers. This is done by looking at angular distributions.\nMost of the time, it’s as simple as looking at the Dalitz plot and seeing:\n\nIf there is a minimum in the angular distribution.\nIf this line has several structures, several nodes.\n\nThe maxima and nodes indicate, in the case of scalar particles in the final state, the nodes would literally tell you the spin:\n\nIf you have one node, you have spin one.\nIf you have two nodes, you have spin two.\nIf you have three nodes, you have spin three.\n\nSo the intensity would really vanish at certain points—the dark spots.\n\nBut in the case where it’s not scalar—and most of the time, it’s not scalar particles that interact—the situation is a little bit more complicated. I will have examples of scalar resonances. But here, let’s just quickly check what spin they are giving here:\n\nBaryon (proton) spin: \\frac{1}{2}\nKaon and pion (except scalars): spin 0\nLambda: same as proton, but with the jump, the presence of spin averages out the distribution.\n\nIf you consider a certain spin projection of the \\Lambda_c and proton, you’ll again have nodes and zeros in the angular distribution. But since we don’t polarize the initial-state \\Lambda, and we don’t measure the spin of the final state—everything is averaged. Therefore, you don’t have minima, you don’t have nodes or zeros any longer; things get smeared.\n\nA particle with spin can have 2J + 1 projections onto the quantization axis. Let me consider a particle that has spin J. There is a z-axis that we need to quantize the spin, and this z-axis measures the J_z upper layer is going to give us.\n\n\n\n\n\n\nThe state |JM\\rangle can be thought of as a vector of 2J + 1 components. All operators in this case are matrices that act on these vectors and produce either:\n\nThe same state with a certain eigenvalue, or\nA mixture of the states.\n\n\n\n\nWhen I act with the rotation on the state, I’m going to produce not a certain state, but a mixture of different states—it’s as simple as that. When I rotate the state, if it were a vector in regular space, I could probably adjust this to have exactly a certain projection. But in quantum mechanics, it doesn’t work this way.\nOnce you act with the rotation, most of the time you’re going to end up with a mixture of all states. These coefficients are tabulated, so they are known functions for any state. You can have a look and check what these coefficients are. They are called Wigner D-functions.\n\nLet me be more concrete. I’m going to rotate about the Y-axis in a second. I have a Z, I have my X, and I have my Y—XYZ. They should form the right-handed triple of axes X, Y, Z.\nI’m going to rotate about the Y-axis and apply it to |JM\\rangle. In order to do that, I have to—so J_+, as you remember, it’s:\n\nJ_+ = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad J_- = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\n\nThe J_y operator then has zero on the diagonal and these coefficients above and below the diagonal. To apply the transformation to the state, you have to do the matrix exponential of J_y. But this is done for you, and we know what they are.\nThese are these coefficients—where they are C, C, C, C. You need to know about the initial state as well. That’s why the JM indices are kept in these notations.\n\nLet’s take an example—a very small one—of spin one-half. I’m going to rotate the state |1/2, 1/2\\rangle by 30 degrees about the Y-axis. I’m going to get a |1/2, 1/2\\rangle plus |1/2, -1/2\\rangle combination, and I want you to quickly tell me what are the numbers here.\nThese coefficients are sitting in the same place—where sitting in the same page as the Clebsch-Gordan coefficients. They are closely related. This is about the SU(2) group, and you just open the Clebsch-Gordan coefficients and check the numbers.\nSince 30 degrees is not at all trivial to do it, but now we won’t have time to look at details, and hopefully in the seminar we will explore a little bit more. I just remember this table because it’s super simple.\nThe D for m' = m = \\pm 1/2 is:\n\nd^{1/2}(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -\\sin(\\theta/2) \\\\ \\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix}\n\nI should have picked 60 degrees because for 15 degrees it’s a little bit inconvenient. What is going to be here is the cosine of 15 degrees. What’s going to be here is the sine of 15 degrees.\n\n\n\n\n\n\n\n\nMathematica has an opposite convention to what we use.\nWikipedia is the most reliable source for Wigner D-function conventions.\nThey’re coded correctly in Python (SymPy) and ROOT.\n\n\n\n\n\nLet me stress what we discussed so far was not about weak interactions or strong interactions. It was about rotations and the rotational group.\nThat’s the fun part—and something that still impresses me—that in order to understand how particles behave and what the angular distributions are, you need little from the strong interaction. You only need the general properties of the rotational group, so to speak.\nAngular distributions are determined by:\n\nGeneral properties of how space is rotated, plus\nA little bit that we need from strong interactions.\n\nSpecifically:\n\nStrong interactions tell us what is the preference for which spin particles are produced.\nQuantum group properties determine how they decay and what the asymmetry is in the kinematics.\n\nThat’s amazing. Therefore, we can now move on and have a recipe, or a general way, to construct any particle decay chain and figure out what the angular distribution is going to be.\n\nLet’s now explore the blob that I had in the previous slide and then consider one of the possible decay kinematics, one of the possible decay dynamics. Now we’re going to make up a model for what is inside of the blob.\nIt’s not just kinematics—it really comes from a modeling assumption, and I’m going to assume that three particles in the final state are produced via a cascade process where:\n\nThe initial particle goes first to the intermediate particle with spin J.\nThen X decays to 1 and 2.\n\nA three-way process with two variable spins just gives the dimension of the matrix—discrete dimensions. For this problem, since all the particles have spin, I’m going to deal with the dimensions, which is a product of:\n\n\\text{Dim} = \\prod_{i=0}^3 (2j_i + 1)\n\nIf a particle’s spin is 0, then the dimension of the corresponding spin is 1. That’s easy.\nBut in the general case, you have many of these two-variable functions, and the way to write the amplitude is to sum over the intermediate spin and to…\n\n\n0.4 Helicity Amplitudes and Rotation Matrices in Cascade Decays\nAnother thing I would like to say is that for simplicity, we are going to align. We are going to consider this X_3—they are different resonances, the two resonances from RTPX, and X reland has to go through from the initial state.\nYou look at it in the center-of-mass (c.m.) frame, correct? So that’s the general expression. It’s extendable to any cascade decays.\n\n\n\n\n\n\nThe general decay amplitude formula combines a model-independent part (carrier group-driven angle independence) and dynamics (particle interactions): \n\\mathcal{A} = \\sum_{\\lambda} H_{\\lambda} D^{J*}_{\\lambda_0, \\lambda_X - \\lambda_3}(\\phi, \\theta) D^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta', \\phi')\n\n\nH_{\\lambda}: Dynamics (weak/strong/EM interactions)\nD^{J}: Wigner rotation matrices\n\\lambda_i: Helicity states\n\n\n\n\nI’d like to give you a general formula, and we will only have time to understand it rather than derive it. It has two components: a model-independent part, carrier group-driven angle independence, and then bits of the particle interactions that you have to insert.\nThe H’s are the remaining dark blobs that hide inside the dynamics of the particles. This is what comes from the weak interactions, strong interactions, electromagnetic interactions, whatever you have. This physics is actually sitting there, and the rest is the rotational properties of the system.\nSo this H is the physics, and for the hard interactions, this is something unknown because we don’t have a way to parameterize it. By this H, what we mean is that you have a particle \\Lambda X number three. This is what’s three particles: \\Lambda X, \\Lambda_3^0, decaying into two.\nThe other H degree, plus this X going to particles 1, 2, and the G. The G is the rotation orientation of the decay.\nThe first index tells you who decays. The second index gives where it decays. The particles now have their spin in the frame where the particle moves. The most natural way to use the spin—to actually quantize the spin—is the quantized direction of the spin, which is the direction of motion.\nIn that case, \\Lambda’s are helicity projections—projection to motion to P. The D has the first index telling you who decays, and the second index, after rotation, where it decays, and then the particle zero.\nLet’s look at the particle X. The particle X carries the spin projection \\lambda_X. It decays to particles 1 and 2, which are going at a certain angle with respect to the direction of motion of X.\nTo compensate for this angle, one has to adjust the quantization axis. This is done by rotating the spin—rotating the vector of X to the direction in which it decays. From that combination, one has to rotate to the new combination, and that’s what is indicated by the rotation orientation of the decay rotation.\nI would like to evaluate this expression in the aligned kinematics—\\phi = \\theta = 0. Here is the CM frame. This is the expression.\n\n\n\n\n\n\nIn aligned kinematics (\\theta = \\phi = 0), the amplitude simplifies to: \n\\mathcal{A}_{\\text{aligned}} = \\sum_{\\lambda_X} H_{\\lambda_X} \\delta_{\\lambda_0, \\lambda_X - \\lambda_3} D^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta)\n The Kronecker delta \\delta_{\\lambda_0, \\lambda_X - \\lambda_3} enforces angular momentum conservation.\n\n\n\nIf you evaluate that, what you get is the amplitude \\mathcal{A} that depends on S and C and all \\lambda’s. Let’s evaluate when angles are zero.\nThe transformations reduce because the D-matrix appears due to rotation. But if angles are zero, we don’t have to rotate. We can significantly reduce the summation over \\lambda_X.\nWe don’t have to rotate because X is already moving on the Z-axis. Therefore, we get a sum over \\lambda_X, \\lambda_X, \\lambda_3, and then D of zero angles: D^{J}_{\\lambda_0, \\lambda_X - \\lambda_3}(0).\nAnother piece is \\lambda_1, \\lambda_2, and the adjustment for \\lambda_X, \\lambda_1 - \\lambda_2 of \\theta and \\phi.\nThis gives a delta function \\delta_{\\lambda_0, \\lambda_X - \\lambda_3}. If we don’t have to rotate, the only way to get the same state is if the state remains unchanged.\nSo X is constrained from that. The final expression is: \n\\mathcal{A}_{\\text{final}} = H_0 \\, D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_3} \\, D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_1 - \\lambda_2}(\\theta)\n As simple as that.\n\n\n\n0.5 Predicting Angle Distributions and Partial Wave Analysis in Particle Decays\nHow many numbers do I need from you in order to compute? I want to think now about electromagnetic interactions, or let me do gravity. How many numbers as input do I need from you to predict the angle distribution? It’s essentially here, but it misses fundamental components.\n\nWhat is inside of the blobs? What’s inside of this block, this blob or this blob and this blob? In order to predict all of my values, I just need this guy and that guy.\n\nThe number of spin states for two particles with spins j_1 and j_2 is given by: \n(2j_1 + 1) \\times (2j_2 + 1)\n These values might be functions of particle masses as well—could be masses of x.\nA similar number of these terms is needed, but there is a reasonable way to approximate them.\n\n\n\n\n\n\n\nApproximation in Experiments: Often, in the first analysis attempt, these terms are assumed to be constant, containing only particle properties.\n\n\n\nHere I’m going to say that this one is constant one or c, and this is particle. And then once I do that, I should be able to compute what the angle distribution is.\n\nIn that case, I am going to fix the mass of the one tube and the intensity that I see along the line. What is up to now is that these two have a: \n\\frac{d\\Gamma}{d\\cos\\theta}\n\nWhy cosine?\n\nCosine is just better—it has a better Jacobian.\nWe don’t need to have this sine Jacobian for cosine.\n\nThat’s why often what is looked at is the cosine, and this is going to be the matrix element proportional to the matrix element. And this |M|^2 is fixed.\n\nThis distribution changes from -1 to 1. So here is the \\theta and the \\cos\\theta. \\theta is going to be null here at -1.\nWe scan from -1 to 1, and if it is flat, that’s one possibility. What you often see, especially when you deal with particles with spin, is such a parabola—a second-order polynomial in cosine. Or what you also often see is this.\nNotice the difference:\n\nWhat we wrote here, A, is the quantum transition amplitude.\nIt’s a probability amplitude.\nIt’s one that is going to be squared to give us the observed probability.\nThis G is going to appear squared.\n\n\n\n\n\n\n\nExperiment Observations:\n\nIn experiments, we only see the squared value of the amplitude.\nFor unpolarized decays, distributions are averaged: \n\\overline{|M|^2} = \\frac{1}{(2s_1 + 1)(2s_2 + 1)} \\sum_{\\text{spins}} |M|^2\n\n\n\n\n\n\nThe first way to analyze that is not to guess the amplitude, but rather to use or acknowledge these angle distributions by projecting onto orthogonal polynomials because it gives you a nice basis.\n\nThe differential cross-section can be expanded as: \n\\frac{d\\sigma}{d\\cos\\theta} \\propto \\sum_{\\ell} a_\\ell P_\\ell(\\cos\\theta)\n where P_\\ell(\\cos\\theta) are Legendre polynomials, and \\ell relates to the spin of the produced particle.\n\nThis is what is called partial wave analysis. Or if you project the differential cross section, this is called moment analysis.\n\nPartial wave analysis models cross-sections by amplitudes, treating these terms as free parameters.\nMoment analysis projects angle distributions onto polynomials, yielding combinations of conditions.\n\n\nI didn’t tell you much about the differences between the canonical state that we introduced at the beginning and the helicity state that we introduced later. So we only touched a little bit on how the state is defined in the rest frame, and hopefully, we will get to explore more.\n\nI would like to tell you that this book has the best coverage of this subject. This is Martin Spearman’s Elementary Particle Theory, and Chapter 4 is fundamental.\n\nIt starts from the Lorentz group.\nIntroduces vectors with a bit of group theory—without heavy details.\nA really good book for insights on particle definitions.\n\n\n\n0.6 Identifying Decays via Dalitz Plot Kinematics\nI left you with an exercise, not a quiz. There are some Dalitz plots from CLEO and BaBar, and I have removed the labels. I don’t tell you which particles are in the final state. I only mention that one of them is a D decay and another is a D_s decay.\nYou already know a lot about kinematics, so the exercise is to figure out what decay this is. The axis labels are still there, but you don’t know the masses. From the kinematics, you can deduce the masses and possibly guess the decay.\nI have multiple cases. I would assign one to your group, and then you’d get another one—the EP one.\n\n\n\n\n\n\nKey Kinematic Relations for Dalitz Plots: For a 3-body decay D \\to ABC, the invariant masses of two-particle combinations are: \nm_{AB}^2 = (p_A + p_B)^2, \\quad m_{BC}^2 = (p_B + p_C)^2\n The total invariant mass constraint is: \nm_{AB}^2 + m_{BC}^2 + m_{AC}^2 = m_D^2 + m_A^2 + m_B^2 + m_C^2\n Boundary conditions for the Dalitz plot are: \n(m_{AB}^2)_{\\text{min}} = (m_A + m_B)^2, \\quad (m_{AB}^2)_{\\text{max}} = (m_D - m_C)^2\n\n\n\n\nI have to leave now. Would you like to take it? No? Okay, it’s more like homework.\nSorry about that. Come with me, and I’ll give it to you from my office. The rest of you as well.\nThanks for coming, and sorry for being slightly late. Will you have time tomorrow at 8 a.m.?\n\nFor the Dalitz plot analysis of a 3-body decay D \\to ABC, the relevant kinematic relations are:\n\nInvariant masses of two-particle combinations: \nm_{AB}^2 = (p_A + p_B)^2, \\quad m_{BC}^2 = (p_B + p_C)^2\n\nTotal invariant mass constraint: \nm_{AB}^2 + m_{BC}^2 + m_{AC}^2 = m_D^2 + m_A^2 + m_B^2 + m_C^2\n\nBoundary conditions for the Dalitz plot: \n(m_{AB}^2)_{\\text{min}} = (m_A + m_B)^2, \\quad (m_{AB}^2)_{\\text{max}} = (m_D - m_C)^2\n\n\nThese formulas are essential for reconstructing the decay kinematics and identifying the final-state particles."
  },
  {
    "objectID": "2024-Lecture-05-small-blocks.html",
    "href": "2024-Lecture-05-small-blocks.html",
    "title": "",
    "section": "",
    "text": "0.1 Kinematics of Two-Body Scattering and Mandelstam Variables\nToday we are at lecture number five. We’ll discuss angle distributions and partial wave analysis. But before going there, I would like to start with a recap.\n\nIn the last lecture, we discussed:\n\nThe phase space for particle reactions\nDifferent experiments and their kinematics\nA list of experiments worldwide studying hadrons, their production mechanisms, and peculiarities\n\n\nAnd we start with a recap on kinematics. The first question: How many variables does one need to describe the two-body scattering process? We have two problems:\n\nScalar particles (e.g., 0-minus scalars):\n\n\nInitial state: Two 0-minus scalar particles\nFinal state: Two 0-minus scalar particles\n\n\nParticles with spin:\n\n\nExample: \\pi^+ scattering (0-minus from 0-minus to 3-minus and 1-plus)\n\n\n\n\n\n\n\nThe scattering process is represented by a “blob” indicating strong interaction dynamics. This is a unitarity diagram (not a Feynman diagram), where arrows show particles entering/leaving the interaction.\n\n\n\n\nThen I ask you to calculate the number of variables needed to describe the process entirely—the full kinematics.\n\nThe blob’s content (EM, strong, gravity) doesn’t affect the answer.\nTwo cases:\n(A) Particles without spin\n(B) Particles with spin\n\nFor (C), give an example of variables fully describing the process.\n\nStudent Discussion:\n\nStart with 2 \\times 2 \\times 3 = 12 (kinematic degrees of freedom).\nSubtract 4 (conservation laws) → 8.\nSubtract 6 (3 rotations + 3 boosts) → 2 variables.\n\nFor (B), the answer is also 2 variables, though spin introduces more amplitudes (e.g., 21 for spin-3 × spin-1).\n\nKey Variables: The process is fully described by Mandelstam invariants:\n\nS (s-channel): \nS = (P_1 + P_2)^2 = (E_1 + E_2)^2 - (\\vec{p}_1 + \\vec{p}_2)^2\n\nT (t-channel): \nT = (P_1 - P_3)^2 = (E_1 - E_3)^2 - (\\vec{p}_1 - \\vec{p}_3)^2\n\n\n\n\n\n\n\n\n\nS and T are Lorentz-invariant—frame-independent.\nFor spinless particles, the amplitude is a scalar function of S and T.\nWith spin, amplitudes become higher-rank objects (e.g., 21D for spin-3 × spin-1), but still depend on the same two variables.\n\n\n\n\n\nHenrik’s Example:\n\nVariables: Mass of particle pairs (e.g., (P_1 + P_2)^2 and (P_1 - P_3)^2).\nMass computation: \nm^2 = E^2 - \\vec{p}^2\n\n\nClarification:\n\nMandelstam variables are standard because they avoid frame-specific descriptions.\n“Invariant” is the correct term (despite dictionary quirks!).\n\n\nFinal Notes:\n\nDecays of final-state particles (e.g., unstable 3-minus/1-plus) introduce more variables, but for 2-to-2 scattering, two variables suffice.\nThe amplitude’s dimensionality scales with spin combinations but remains a function of S and T.\n\n\n\n0.2 Variables and Kinematics in Three-Body Decay and the Dalitz Plot\nExample of Kinematic Variables: Give an example of these sets of variables. Everyone should get their favorite.\n\nS, S and T. No, not a single pair. These sets. We need two variables to describe the process.\n\nChoosing Independent Variables: What are your favorite two variables? Oh, this is. What’s your second favorite? What is the third variable? Why third? Why do you talk about third? Just introduce something new. Let’s introduce. Give us. What about you? Sorry, I forgot your name. Sven. Please.\n\nP2 minus P4 is 5. It’s called U and it’s also variable P. Oh, P_2 - P_4. It’s the same. Oh, sorry. So it’s P_2 - P_4.\n\n\nP_2 - P_4 = P_1 - P_3\n\nSo it’s P_1 - P_4. P_1 - P_4 squared is called U. And this is the same as. If I do the algebra, you find out that there are only two independent.\n\n\n\n\n\n\nThe Mandelstam variable U is defined as (p_1 - p_4)^2 and is related to S and T via the constraint S + T + U = \\sum m_i^2 for a 4-particle system.\n\n\n\nFavorite Variable Sets: So your favorite set is S and U, and they’re equivalent because U is a linear combination of S and T. Fine. Sometimes you take T and U. No, no, no. Let’s do different center-of-mass energy and angle. That’s a really good choice. That’s probably gonna be my favorite.\nCenter-of-Mass Energy and Angle: And this is actually \\sqrt{S}. Center-of-mass energy and then the angle. How do you define the angle? You go to the center of mass and then you make particle. Center of mass is often referred to as center of momentum as well.\nIn the center-of-momentum frame, it’s just easier to call it center of momentum because you immediately understand that momentum, the total momentum, has to add to zero for all particles.\nDefining the Angle: Do you have a P_1 here and P_Q? What else do we have? How do you define the angle? They are going in this direction and the angle between P_1 and P_3. The angle between P_1 and P_3. Essentially this one. That’s my favorite. Indeed.\nMomentum Vectors and Kinematics: One has to be careful when drawing that because we know that the length of the vectors indicates their momentum. Particle’s momentum. For the final state, it’s in the center of momentum. We’re still in the same frame, so this has to be equal to that. I’m fine with that choice.\nVariable Choices and Phase Space: There are no more common choices. But any two variables work if they are not redundant. We can choose E_1 in the lab frame, any frame, and then E_3 in the lab frame. That’s also fine. Any two variables, if they are independent, they characterize the kinematics.\n\n\n\n\n\n\nThe phase space for a 3-body decay is flat in the variables S and T, expressed as \\frac{dN}{dS \\, dT} = \\text{constant}.\n\n\n\nOne has to be careful. Sometimes you fold your phase space. By introducing a set of variables, you map your phase space in these variables or these variables to another domain. Sometimes this domain is somewhat smaller, it has a folded coverage. This variable, it’s not a bijective transformation, but this is advanced to see. I’m going to ask you an example. Got it. It’s okay.\n\nTransition to Dalitz Plots: Cool. We discussed two-body kinematics. We’re going to continue discussing the angular dependence after the Dalitz plot. Let’s quickly look—oh, questions here. Shortly before we move, questions on the variables that characterize kinematics. Cool.\nThe homework we had the exercise on the Dalitz plot, and this is, as I mentioned, a problem that is now entering into our lectures. Part of the course that is specific to hadrons. It’s before where we’ve been overlapping a lot with particle physics. But now, from this lecture and the next couple of lectures, we will have material really specific to hadron spectroscopy and the approach that we use to discuss hadrons.\nThree-Body Decay and Dalitz Plots: One of the subjects that we would like to go deeper into is particle representation—essentially three-body decay. The Dalitz plot is the common technique to indicate the dynamics of the particles, the dynamics of the interactions.\nIn the case of three-body decay, we deal with a similar diagram as before, but now one leg comes in, three legs go out, and what is inside the blob is an interaction. We can pose all the same questions as before: What are the number of variables? But the answer won’t differ because it’s the same number of legs. Essentially, the same number of legs tells you the number of variables.\nKinematic Variables for Three-Body Decay: For a three-body decay, there are two variables that describe the process completely. Once you give me these two variables—S and T, or angles, or any other—I should be able to draw the entire kinematics.\n\nS = (p_1 + p_2)^2 = (p_3 + p_4)^2\n\n\nT = (p_1 - p_3)^2\n\nRemember my analogy of the rigid body: when you print on a 3D printer a blob out of which the vectors are sticking, this is a rigid body that describes a kinematic point. The angles between all vectors are fixed. The lengths of the vectors are fixed. You have the entire setup of the kinematics at a single point in the phase space.\nVisualizing Three-Body Decay: The same goes for three-body decay. Just give me two variables, and I should be able to draw you how the decay looks like in the center of mass.\nIn that case, I have to draw here two vectors that leave. And in that case, I’m going to draw the three vectors. Essentially, this is the support out of which the vectors stick. The vectors determine the angles and lengths of the vectors, and that’s it—that’s what you have.\nMandelstam Variables for Decay Kinematics: Now, S and T are defined in a similar way, but now we have different particles in the final state. From that kinematics to this kinematics, what it takes is to take one leg and swap it to the other side. It’s done by changing the sign on the momentum.\nSo, for three-body decay, what we do—let me define it here:\n\nS = (p_1 + p_2)^2 = (p_3 + p_4)^2,\nand T = (p_1 - p_3)^2.\n\nI noticed that there was a typo: p_1 is the particle that decays. In that case, I don’t know—should I change it? For now, let me stick to the notations that relate to kinematics.\nPhase Space Properties: It’s important to realize that the phase space for three-body decay is flat in the variables—it’s actually constant in the variables (3,4) and (2,4). In S and T, we write it as \\frac{dN}{dS \\, dT}. Still, everything didn’t miss much.\n\n\n0.3 Recursive Phase Space Formulas and Mandelstam Invariance in Three-Body Decays\nSo this is the recursive formula that we discussed in the last lecture. It’s easy to use this equation to demonstrate that once you substitute the two phase spaces and apply the proper Lorentz transformation, you end up with the factor \\frac{1}{(8\\pi)^2}. This comes from the two phase spaces: 2\\pi for each.\nFrom the first phase space—from that decay—you get the two-body phase space factor: \n\\frac{1}{8\\pi} \\cdot \\frac{2p}{\\sqrt{s}}\n\nThese are two-body phase spaces. From the other, you have this phase space \\times 2\\pi, and both have the same form: \n\\frac{1}{8\\pi} \\cdot \\frac{2p}{\\sqrt{s}}\n\nWhat also comes is that every phase space has a general phase space element (with angular dependence): \n\\frac{1}{2\\pi} \\cdot \\frac{1}{(8\\pi)^2} \\cdot \\frac{2p}{\\sqrt{s}} \\cdot \\frac{d\\Omega}{4\\pi}\n\nThat’s easy to remember because it’s \\frac{1}{8\\pi} in the asymptotic limit. These things approach unity at high energy, and that’s a unit integral. That’s what we discussed already.\n\nFor both of them, you put this expression. For one of them, the cosine \\theta is described in terms of the cosine of the scattering angle. Essentially, you express m_{34} (or whichever mass you want) in terms of the cosine of the scattering angle.\nThese \\frac{2p}{\\sqrt{s}} terms appear here, and you must replace E' by p. This appears as a Jacobian, but I think I was incorrect earlier—I never used that properly.\n\nWe arrived at this in a few lines without details, but I hope we can fill in the details later. It comes through the exercises. Evaluating the phase space comes up several times during the course.\n\n\n\n\n\n\nFor the three-body case, using the recursive phase space formula: \nd\\Phi_n = d\\Phi_k \\times d\\Phi_{n-k+1} \\times \\frac{dm^2}{2\\pi}\n shows that the Jacobian for the transformation is constant—the Jacobian for transforming the three-body phase space into paired mass variables is constant.\n\n\n\nThere is no extra dependency. This means when we look at the differential width or cross section against these variables, there’s no extra stretching or density increase due to the choice of variables.\n\nWe have several choices, but only Mandelstam invariance gives an undistorted representation of the density. If you plot differential widths against m_{34}^2, m_{24}^2, or m_{23}^2, it will be a constant numerical value with no rescaling.\nThat’s why representing three-body processes in Mandelstam variables: \ns_{ij} = m_{ij}^2 = (p_i + p_j)^2\n (or linearly related invariants) is so powerful: you see directly the content of the blob—what happens in the interaction.\nThat’s essentially what the Dalitz plot is, where the density is constant: \n\\frac{d^2\\Gamma}{ds_{12}ds_{23}} = \\text{constant}\n\n\nThe discussion emphasizes how these formulas relate to Lorentz invariance, Jacobian transformations, and the utility of Mandelstam variables in representing undistorted decay distributions.\n\n\n0.4 Charm Quark Decay in \\Lambda_c Baryon: Kinematics and Dalitz Plot Analysis\nYou can see the example of the triple decay that I have here of the \\Lambda_c baryon going to the proton, kaon, and pion. We measure \\Lambda_c produced in proton-proton collisions or any other collisions. In the BES experiment and Belle experiment, they observe \\Lambda_c. This is one of the sorts of particles that lives a long time and is produced abundantly.\n\nParticles with charm ground states are produced abundantly, and they live sufficiently long to fly from the primary vertex. We reconstruct them, which is why we have a good sample and a good understanding of their decay kinematics—and dynamics as well, the content of this blob.\n\nIn that decay, you see there is a charm quark in the initial state and no charm quark in the final state, indicating this happens via weak interaction. The charm quark disappears between the initial and final states. The charm quark decays, transitioning into the strange quark that ends up in the kaon:\n\nc \\to s + W^+ \\quad \\text{(followed by } W^+ \\to u\\bar{d}\\text{)}\n\nThis c \\to s transition happens within one generation and is not suppressed—it’s an allowed process.\n\n\n\n\n\n\n\nThis is the golden channel for registering because, in the final state, you have three charged particles: proton, kaon, and pion. There are no neutrals. The proton is a nice charged particle—it travels, it’s stable. The kaon and pion are stable in our experiments.\n\n\n\nThey fly out from the decay without any distraction, and we see the tracks clearly through all detectors. We observe them pointing away from the primary interaction.\n\nThere is around a 10-millimeter shift in \\Lambda_c flight distance between the primary and secondary vertices—roughly one centimeter. This is due to the boost and time dilation:\n\n\\tau = \\gamma \\tau_0, \\quad \\gamma = \\frac{E_{\\Lambda_c}}{m_{\\Lambda_c}}\n\n\\Lambda_c in the lab frame lives longer than in its rest frame because it’s produced with high energy (few hundred GeV) in proton-proton collisions at the LHC. This is a super nice decay, and we studied it extensively.\n\nHere is the experimental result of the analysis, which matches the data closely. If I showed you the actual data, you wouldn’t distinguish it from this plot because the statistics are so high that the distribution is very smooth.\n\nX-axis: Invariant mass squared of the proton and kaon: \nm_{pK}^2 = (p_p + p_K)^2\n\nY-axis: Invariant mass squared of the kaon and pion: \nm_{K\\pi}^2 = (p_K + p_\\pi)^2\n\n\n\nAll allowed kinematic values for the decay are shown in color. The white area around it represents forbidden kinematics where energy-momentum conservation fails:\n\np_{\\Lambda_c} = p_p + p_K + p_\\pi\n\nIf I select a point inside the plot, I can compute angles between particles and reconstruct the kinematic configuration. But for points in the white area, energy conservation cannot be satisfied—that’s why the range of possible invariant masses is limited. This surface is called the Dalitz plot, and it’s how we explore decay kinematics.\n\nDifferent colors on this plot indicate different probabilities for the decay to occur. We measure the decay, reconstruct the particle tracks, and identify the kinematic point because there’s a direct mapping between four-vectors and Dalitz plot positions. Certain kinematics are more probable than others—particles prefer specific directions.\nFor this decay, there are two dominant configurations:\n\nOne is longer (rarer),\nThe other is more common.\n\nYou can identify these by looking at the borders of the plot. Here’s the hint: particles align in one line on the border. Inside the surface, they always have an angle, but at the border, they become collinear. Think about how you maximize the invariant mass and where this point lies.\n\n\n\n\n\n\n\nThe Dalitz plot intensity is proportional to the matrix element squared and phase space factors: \n\\frac{d\\Gamma}{dm_{pK}^2 \\, dm_{K\\pi}^2} \\propto |\\mathcal{M}|^2 \\times \\text{Phase Space}\n This explains the color gradients observed in the plot.\n\n\n\n\n\n\n0.5 Maximizing and Minimizing Mass in Three-Body Decay Kinematics\nAny thoughts? I think you should be in the bottom right because we should maximize the mass of the PK system. There are three momenta going out, and my idea was that the three momenta are in opposite directions. The three-momentum of the sum should be as small as possible.\nIf we add both momentum vectors, the forward momentum—the sum of the squares—should be as large as possible. So we should be on the right of our diagram. Then, if we apply and find… The same direction should be on the lowest.\nYou argue that this mass, on the y-axis, should be as small as possible. Why? Because the three momenta are in line, and we subtract them. My way of thinking is this: if you boost to the rest frame of the K and π, they fly next to each other. Their relative momentum is small. If you boost to their rest frame, they might both be at rest, and their mass would just be the sum of the masses. So it’s minimal.\n\n\n\n\n\n\nMinimal Mass Configuration: When particles are collinear (e.g., K and \\pi back-to-back), the minimal mass is: \nm_{K\\pi,\\text{min}} = m_K + m_\\pi\n\n\n\n\nWhat you’re saying is correct. We are looking for the minimal mass of the kinematic system. Let’s figure out what this point corresponds to. There is a kinematic configuration where two particles with maximal momentum are at rest and go in opposite directions. That corresponds to this point.\n\nThis plot is from experimental data. In the experiment, we would reconstruct such a case because we would not detect the proton. This is the lab frame, but the center-of-momentum frame is where everything is boosted. Even though the proton is at rest in the lab frame, it’s already boosted.\nThis point is the maximum mass. The particles have maximum momentum and go back-to-back, so this is the maximum mass. The other point minimizes the mass.\n\n\n\n\n\n\nMaximal Mass Configuration: The maximal mass occurs when particles have maximal relative momentum: \nm_{K\\pi,\\text{max}} = \\sqrt{(E_K + E_\\pi)^2 - (|\\vec{p}_K| - |\\vec{p}_\\pi|)^2}\n\n\n\n\n\nFor the three-body decay, there is a similar approach. The standard way to define the angle is to boost into the rest frame of a pair. Let me show you the setup. I’ll use proton, K, and π. I want to fix the mass of the K-π system and explore how it changes without altering the proton’s mass.\nThe way to think about this is to take the setup in the center-of-momentum frame and boost to the K-π rest frame. In this frame, they go back-to-back. In the center-of-momentum frame, three particles are arranged such that the sum of their three-momenta is zero.\nAfter boosting, the Λ_c has non-zero momentum, and the K and π also have non-zero momentum. But since we are in the K-π rest frame, their momenta add to zero. If we fix the mass of the system and explore phase space along a line where the pair’s mass is fixed, the length of these vectors is fixed. What changes is the angle θ.\nWe can explore the full range of θ from 0 to π. At one corner, θ = 0, and at the other, θ = π. The right should correspond to θ = 0, and the left to θ = π.\n\nIn this setup, the proton and K-π go in opposite directions, giving the maximum mass of the proton-K system. For our new setup, since the length of all vectors is fixed, we can only rotate them.\nThe dependence of the mass of two particles on θ is such that a larger angle gives a larger mass. For the proton-K system, θ = 0 gives a very high mass, while θ where they are nearly in the same direction gives a small mass.\n\n\n\n\n\n\nAngle-Dependent Mass: The proton-K mass depends on angle \\theta as: \nm_{pK}^2(\\theta) = (E_p + E_K)^2 - \\left(|\\vec{p}_p|^2 + |\\vec{p}_K|^2 - 2|\\vec{p}_p||\\vec{p}_K|\\cos\\theta\\right)\n\n\n\n\n\nWe can do the same with the other half. Let me fix the mass of the proton-K system again. The most straightforward analysis is to go to the proton-K rest frame, where everything is fixed, and scan along this line by changing the angle of the pair.\nThe lines describe setups where you change the angle in one frame or another. There is also a third variable in 2-to-2 scattering, called U, which is symmetric. For three particles, there is the mass of the pion-proton system, but it’s not evident.\nIf I fix the mass of this system and scan the angle, the U variable is a linear combination of the two masses. It’s a diagonal in this plot. This line represents the fixed mass of the pion-proton system, and we move from one corner to another, changing the representation.\nThe representation used in experimental analyses for the Dalitz plot has the mass of one pair on the x-axis and the other on the y-axis. In the homework, you’ll see a more symmetric Dalitz plot where all variables enter symmetrically.\nThis uses properties of an equilateral triangle: the sum of the distances from any point inside to the sides is constant. The masses are represented as distances to the sides, giving a symmetric representation.\nThis is equivalent to the standard plot but skewed. To relate Cartesian coordinates to the heights of the triangle, we use \\frac{\\sqrt{3}}{2} due to the 60° angles.\n\nThe symmetric representation is nice, but the standard one is easier to plot. Both represent kinematics, and we see points where the density increases. The goal is to understand whether the plot shows dynamics and what processes guide the interaction.\nLooking ahead, we’ll see that Λ_c decay isn’t just to three particles but proceeds via intermediate resonances. Two particles form a short-lived state, increasing the decay probability. If the energy matches a certain range, the interaction is more likely, and the probability increases.\n\n\n0.6 Resonances and Kinematic Structures in Two-Particle Systems\nYou might have seen cross sections for the two-particle resonances. That has a bump in it known as the hadronic resonance. The physics of that is you have a system of two particles, and the quantum numbers of the system match the quantum numbers of some other resonance.\nBy adjusting the energy of the system, you explore how the system behaves—how likely two particles are to interact at a certain energy. If there is an intermediate resonance, this system can resonate at this energy, and the probability is going to increase once you pass through the resonance region. This is described by the Breit-Wigner formula:\n\n\\sigma(E) = \\frac{\\pi \\hbar^2}{2\\mu E} \\frac{\\Gamma^2}{(E - E_0)^2 + (\\Gamma/2)^2}\n\nwhere:\n\nE_0 is the resonance energy\n\\Gamma is its width\n\n\nThis leads to the appearance of the bent structures in the Dalitz distribution, or if you project it onto one of the axes, you’re going to see the resonance-like shape. These bent structures can be identified on both.\nThis is a cool example that I brought because there are resonances in all three pairs. There are resonances in these two particles. These two particles interact.\n\n\n\n\n\n\nWhy is there a bigger probability increase when the kaon and pion are near resonance? The K and \\pi also show this, like on the right side, but not only the two lines on the left side.\n\n\n\n\nLet’s understand the lines. We will come back to that in two lectures, but let’s quickly identify them now. All horizontal lines—which resonances are these? In which mass distribution? This is going to give us a peak.\nLooking at the blue plot, the horizontal lines correspond to fixed mass for the K\\pi system. When the mass approaches a certain value, it reaches the maximum. These lines are the resonances in K\\pi.\nWhat particle decays to K\\pi? K^*, the K^* resonance.\n\nNow, let’s look at the vertical lines—in which system? When changing K\\pi, you scan a certain energy of the system. You can think of this as a projectile. These lines correspond to the fixed mass of the proton and K\\pi. By moving along the axis, you’re scanning K\\pi.\nIf you expand it, you see that the lines correspond to the resonances in:\n\nK\\pi\nK\\pi-proton\nK\n\nThe third combination is pion-proton, and these are \\Delta resonances.\n\nThe other plot helps as well because it’s nice to see that the lines are parallel to the sides of the triangle. This line is the same as this line, parallel to one of the sides. Then you have the \\Lambda resonances parallel to another side.\nAt this point, you can see the \\Delta resonance, which is the line over here, parallel to the third side. It’s a bit more tricky to see the \\Delta resonance here, but it’s there.\n\nDalitz Plot Kinematics:\n\nm_{12}^2 + m_{23}^2 + m_{13}^2 = M^2 + m_1^2 + m_2^2 + m_3^2\n\nwhere:\n\nm_{ij} are the invariant masses of particle pairs\nM is the total energy of the system\n\n\nResonance Identification:\nFor K^* \\rightarrow K\\pi:\n\nm_{K\\pi}^2 = (E_K + E_\\pi)^2 - (\\vec{p}_K + \\vec{p}_\\pi)^2\n\nThis reconstructs the invariant mass of decay products to identify resonances like K^*.\nSimilarly, for \\Delta \\rightarrow p\\pi:\n\nm_{p\\pi} \\approx m_\\Delta\n\nwhen the invariant mass matches the \\Delta resonance mass.\n\n\n0.7 Angular Distributions in Particle Decays and Spin-Dependent Kinematics\nOne thing to finish and move past this dull spot to our topic today is the angular distribution. I’m going to discuss now the angular distribution for a decay within one band. Let me look at the phase space resonance here.\nAs we discussed before, the kinematics: when traversing the double plot and phase space from one end to another while keeping the mass of the combination fixed, the angle changes. This explores different angles, which is precisely the kinematics.\nLet me sit in the rest frame of the K\\pi where this band is happening and traverse the phase space by changing this angle. The angle dependence indicates that within the band, there can be inhomogeneity. Even within the band, one edge might have a different probability than the other.\nParticles often prefer to be aligned rather than perpendicular. This less probable kinematics happens because particles have spin. The preference appears only because the intermediate resonance, in this case K, is not a scalar particle—it has spin. The spin of particles causes inhomogeneity in angular distributions and in the decay spectrum.\n\n\n\n\n\n\n\nKey Insight: Angular distributions reveal spin effects. Particles with spin produce anisotropic decay patterns, while scalar particles (J=0) yield uniform distributions.\n\n\n\n\nAngular distribution is a very powerful tool to understand properties of particles. As we discussed, it’s how we measure spin, parity, and other quantum numbers in particle interactions.\nParticles with higher spin produce more bumpy, spiky angular distributions, while scalar particles produce no asymmetries or structures. By examining the angular distribution in the rest frame of the particle decay, one studies the ratio of aligned kinematics to other types. This allows inference of spin information.\nFor most particles discovered so far, quantum numbers are not initially known. We find them as bumps in the spectrum, and the next step is determining their properties through angular distributions. Often, it’s as simple as looking at the Dalitz plot and checking for minima or structures in the angular distribution.\nMaxima and nodes indicate spin for scalar final-state particles:\n\nOne node means spin 1.\nTwo nodes mean spin 2.\nThree nodes mean spin 3. The intensity vanishes at these nodes—the dark spots.\n\nFor non-scalar particles, which are more common, the situation is more complicated. Here’s a quick check of spins:\n\nThe proton has spin \\frac{1}{2}.\nKaon and pion (except scalars) have spin 0.\nThe spin of \\Lambda is the same as the proton.\n\nHowever, with unpolarized initial states and unmeasured final spins, the distribution averages out. For example, if you consider a specific spin projection of \\Lambda_c and proton, you’d see nodes and zeros in the angular distribution. But without polarization or spin measurement, everything averages, smearing out minima and nodes.\n\n\nAngular distribution for spin-1 particle: \n\\frac{d\\Gamma}{d\\cos\\theta} \\propto 1 + \\alpha \\cos^2\\theta\n where \\alpha encodes spin alignment preferences.\nGeneral spin-J decay distribution (scalar final states): \n\\frac{d\\Gamma}{d\\cos\\theta} \\propto |P_J(\\cos\\theta)|^2\n where P_J is the Legendre polynomial of order J, producing J nodes.\nDalitz plot intensity (spin-dependent decays): \nI(m_{12}^2, m_{23}^2) \\propto \\left| \\sum_{J} A_J(m_{ij}^2) P_J(\\cos\\theta_{ij}) \\right|^2\n where A_J are spin-dependent amplitudes.\nSpin averaging effect (unpolarized initial states): \n\\left\\langle \\frac{d\\Gamma}{d\\Omega} \\right\\rangle = \\frac{1}{2s+1} \\sum_{m_s} \\left| \\mathcal{M}_{m_s}(\\theta,\\phi) \\right|^2\n where s is the initial particle’s spin.\n\n\nNote: The number of nodes in the angular distribution directly correlates with the spin J of the decaying particle. This is a cornerstone of spin determination in particle physics.\n\n\n0.8 Spin Projections, Rotation Operators, and Wigner D-Functions\nA particle with spin can have 2J + 1 projections to the quantization axis. Let me consider a particle that has spin J. There is a z-axis that we use to quantize the spin, and this z-axis measures the projection J_z. One can think of the state |JM\\rangle as a vector with 2J + 1 components.\n\n\n\n\n\n\nThe dimension of the spin state space is given by: \n\\text{Dimension} = 2J + 1\n where J is the total spin quantum number.\n\n\n\nAll operators in this case are matrices that act on these vectors, producing either the same state with a certain eigenvalue or a mixture of states. When I act with the rotation operator on the state, I produce not a single state but a mixture of different states—it’s as simple as that.\nIf this were a vector in regular space, I could adjust it to have exactly a certain projection. But in quantum mechanics, it doesn’t work this way. When you rotate the state, most of the time you end up with a mixture of all possible states.\nThe coefficients for these mixtures are tabulated and known as Wigner D-functions. You can look them up to see what these coefficients are.\n\nLet me be more concrete. I will rotate about the Y-axis. I have the axes X, Y, and Z, forming a right-handed coordinate system. I will rotate the state |JM\\rangle about the Y-axis.\nThe ladder operators J_+ and J_- are represented as matrices: \nJ_+ = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad J_- = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}.\n The J_y operator has zeros on the diagonal and these coefficients above and below.\nTo apply the rotation, you take the matrix exponential of J_y: \n\\mathcal{R}_y(\\theta) = e^{-i\\theta J_y}.\n But this is already computed for you, and the results are the Wigner D-functions.\n\nThese coefficients depend on the initial state, which is why the indices J and M are kept in the notation. For example, if I say we have a spin-1 state initially perpendicular to the Y-axis and rotated by 30 degrees, you can look up the D-functions to find the composition of the state.\nThe general form of the Wigner D-function for any rotation is: \nD^J_{M'M}(\\alpha, \\beta, \\gamma) = e^{-iM'\\alpha} \\, d^J_{M'M}(\\beta) \\, e^{-iM\\gamma},\n where \\alpha, \\beta, and \\gamma are Euler angles.\n\n\n\n\n\n\nThe Wigner D-matrix elements describe rotation transformations: \n\\langle J M' | \\mathcal{R}(\\alpha, \\beta, \\gamma) | J M \\rangle = D^J_{M'M}(\\alpha, \\beta, \\gamma)\n where \\mathcal{R} is the rotation operator parameterized by Euler angles (\\alpha, \\beta, \\gamma).\n\n\n\nIn particle physics, we use the convention where you first rotate by \\phi about the Z-axis, then by \\theta about the Y-axis, and finally by \\psi about the Z-axis again. This sequence describes any orientation in space.\nThe most nontrivial part is the \\theta rotation, since rotations about the Z-axis simply introduce phases. The small d-matrix encodes the \\theta dependence: \nd^J_{M'M}(\\theta) = \\langle JM' | e^{-i\\theta J_y} | JM \\rangle.\n\nThis gives the full relationship: \n\\langle M' | M \\rangle (\\psi, \\theta, \\phi) = e^{-i\\psi m'} \\, d^l_{m'm}(\\theta) \\, e^{-i\\phi m}.\n\n\n\n0.9 Rotations in Spin-1/2 Systems and Wigner D-Matrix Applications in Decay Processes\nLet’s consider a small example of spin one-half. I’m going to rotate the state |1/2, 1/2\\rangle by 30 degrees about the Y axis. The result will be a combination |1/2, 1/2\\rangle + |1/2, -1/2\\rangle, and I want you to quickly tell me the coefficients. These coefficients are analogous to the Clebsch-Gordan coefficients—they are closely related. This is about the SU(2) group, and you can find the numbers by checking the Clebsch-Gordan tables.\n\n\n\n\n\n\nThe Wigner D-matrix for spin-1/2 rotations is: \nD^{1/2}(\\theta) = \\begin{pmatrix}\n\\cos(\\theta/2) & -\\sin(\\theta/2) \\\\\n\\sin(\\theta/2) & \\cos(\\theta/2)\n\\end{pmatrix}\n\n\n\n\nFor a 30-degree rotation (half-angle = 15 degrees), the upper-left entry is \\cos(15^\\circ), and the off-diagonal entry is \\sin(15^\\circ).\n\nNow, questions about the Wigner D functions:\n\nCan you calculate rotations for any spin projection? It looks straightforward, but the convention matters—note the minus sign.\nIf you want to compute this via matrix exponentiation, recall that we derived these matrices in previous exercises.\nYou can do this numerically using Python or Julia—just perform matrix exponentiation on the generator.\n\n\n\n\n\n\n\nFor implementations:\n\nSymPy (Python) and ROOT implement Wigner D functions correctly.\nMathematica uses the opposite convention (plus sign and swapped indices).\nWikipedia is reliable for checking conventions—search “Wigner D functions”.\n\n\n\n\n\nWhat we’ve discussed so far is purely about rotations and the rotational group—not weak or strong interactions. This is the fascinating part: to predict angular distributions, you don’t need detailed interaction physics—just the properties of the rotational group.\nKey insights:\n\nAngular distributions are determined by how space rotates, with only minor input from strong interactions.\nStrong interactions tell us which spins are preferred, but the decay asymmetries and kinematics come from the quantum group structure.\nThis is remarkable—it means we can systematically construct any particle decay chain and predict its angular distribution.\n\n\nNow, let’s examine the “blob” from the previous slide and model one possible decay process. We’ll assume the three final-state particles are produced via a cascade:\n\nThe initial particle decays to an intermediate state X (spin J).\nX then decays to particles 1 and 2.\n\nThis is a two-step process with discrete spin dimensions. The total spin space dimension is: \n\\text{dim} = \\prod_{i=0}^{3} (2j_i + 1)\n\n\nIf a particle has spin 0, its dimension is 1.\nIn general, you’ll have many such terms, and the amplitude is written as a sum over intermediate spins.\n\n\nOne more point: for simplicity, we’ll align the system.\n\nParticles X_3 are distinct resonances.\nParticles 1 and 2 are in the rest frame of X, while X_3 comes from the initial state’s center-of-momentum frame.\nThis framework generalizes to any cascade decay.\n\nHere’s the key formula (we’ll focus on understanding rather than deriving it): The amplitude has two parts:\n\nA model-independent rotational component.\nA dynamical part (H) from particle interactions (weak, strong, or EM).\n\nThe helicity amplitudes are: \n\\mathcal{A} = \\sum_{\\lambda_X, \\lambda_3} H_{\\lambda_X \\lambda_3} D^{J}_{m'm}(\\theta,\\phi) G_{\\text{decay topology}}\n\n\n\\lambda are helicities.\nD^J is the Wigner matrix.\nG describes the decay orientation.\n\n\nThe angular distribution reflects this separation: \n\\frac{d\\sigma}{d\\Omega} \\propto \\sum |D^{J}_{m'm}(\\theta,\\phi)|^2 \\times |H|^2\n\n\nThe rotational part is calculable.\nThe dynamics (H) must be measured or modeled.\n\n\n\n0.10 Spin Alignment and Decay Kinematics of Particle X in the Center-of-Mass Frame\nLet’s now look at the particle X. The particle X carries the spin projection \\lambda_X. It decays to particles 1 and 2, which move at a certain angle relative to the direction of X’s motion.\nTo compensate for this angle, one must adjust the quantization axis by rotating the spin vector of X to align with its decay direction. This rotation is indicated by the orientation of the decay.\n\n\n\n\n\n\nThe rotation is described by the Wigner D-matrix: \nD^{J}_{\\lambda,\\lambda'}(\\theta,\\phi)\n This transforms spin states under rotation.\n\n\n\n\nI would like to evaluate this expression in the aligned kinematics—where \\Phi = \\theta = 0. In the center-of-mass frame, the expression simplifies.\nIf the angles are zero, the transformation reduces significantly because no rotation is needed. The summation over \\lambda_X simplifies because X is already moving along the Z-axis.\nThus, we obtain: \n\\sum_{\\lambda_X, \\lambda_3} D^{J}_{\\lambda_0, \\lambda_X - \\lambda_3}(0) \\cdot D^{J}_{\\lambda_1, \\lambda_2}(\\theta, \\phi) \\cdot \\text{adjustment for } \\lambda_X, \\lambda_1 - \\lambda_2.\n\n\nThis yields a delta function enforcing spin conservation: \n\\delta_{\\lambda_0, \\lambda_X - \\lambda_3}.\n\nThe only non-zero contribution occurs when the spin states match, meaning: \n\\lambda_X = \\lambda_0 + \\lambda_3.\n\n\nThe final expression in aligned kinematics is: \nH_0 \\, D^{J_X}_{\\lambda_X, \\lambda_0 + \\lambda_3}(\\theta) \\, D^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta, \\phi).\n\nThat’s it—as simple as that.\n\n\nSpin projection conservation: \\lambda_X = \\lambda_0 + \\lambda_3.\nWigner D-matrix simplifies for \\theta = 0.\n\n\n\n0.11 Angular Distributions and Partial Wave Analysis in Particle Decays\nHow many numbers do I need from you in order to compute? I want to think now about electromagnetic interactions, or let me do gravity. How many numbers as input do I need from you to predict the angular distribution? It’s essentially here, but it misses fundamental components.\n\nWhat is inside of the blobs? What’s inside of this block, this blob or this blob and this blob? In order to predict all of my values, I just need this guy and that guy. So I have a (2j_1 + 1)(2j_2 + 1) values here, which might be functions of particle masses as well—could be masses of x. And then I need a similar number of these guys, but there is a reasonable way to approximate them.\n\n\n\n\n\n\nThe term (2j_1 + 1)(2j_2 + 1) represents the spin multiplicity factor, where j_1 and j_2 are the spins of the particles involved. This counts the number of possible spin states.\n\n\n\n\nSo essentially, often in the experiment, in the analysis at the first try, we assume that these are actually constant. This is the constant, and that contains only particle properties. Here I’m going to say that this one is constant one or c, and this is particle. And then once I do that, I should be able to compute what the angular distribution is.\n\nIn that case, I am going to fix the mass of the one tube and the intensity that I see along the line. What is up to now? These two have a \\frac{d\\Gamma}{d\\cos\\theta}. So cosine is just better—it has a better Jacobian. We don’t need to have this sine Jacobian for cosine.\n\nThat’s why often what is looked at is the cosine, and this is going to be the matrix element proportional to the matrix element. And this |M|^2 is fixed. This distribution changes from -1 to 1. So here is the \\theta and the \\cos\\theta. \\theta is going to be null here at -1.\n\nWe scan from -1 to 1, and if it is flat, that’s one possibility. What you often see, especially when you deal with particles with spin, is such a parabola—a second-order polynomial in cosine:\n\n\\frac{d\\Gamma}{d\\cos\\theta} \\propto 1 + \\alpha \\cos^2\\theta\n\nOr what you also often see is this.\n\nNotice the difference. Well, it’s important to acknowledge that what we wrote here, A, is the quantum transition amplitude. So it’s a probability amplitude. It’s one that is going to be squared to give us the observed probability. This G is going to appear squared.\n\nIn the experiment, we only see the squared value of the amplitude. Moreover, often we deal with unpolarized decays. Therefore, the distributions are also averaged. So you have to square this thing and sum over the initial spin projections and final spin projections, and then you’re going to obtain that in experiment you see this and you wonder: What does it tell me?\n\nThe first way to analyze that is not to guess the amplitude, but rather to use or acknowledge these angular distributions by projecting onto orthogonal polynomials because it gives you a nice basis. And apparently, this basis is the maximum value that you’re going to… You see some functions from -1 to 1, and you can expand this function as any function in the set of Legendre polynomials:\n\n\\frac{d\\sigma}{d\\cos\\theta} = \\sum_{\\ell} a_\\ell P_\\ell(\\cos\\theta)\n\n\nThese Legendre polynomials are related to the spin of the particle that is produced. And this is what is called partial wave analysis. Or if you project the differential cross section, this is called moment analysis.\n\nSo the partial wave analysis is a way to guess these H functions, to model your cross section by amplitudes, and let these guys be free parameters. And then try to learn what’s inside of the blocks by adjusting them to the data.\n\nBut as a first step, often what is done is to project angular distributions onto the polynomials, which will not give you the inside of the blocks directly, but some combination of these conditions. There is… This is not straightforward, I believe, and we will have more chance to discuss that.\n\n\n\n\n\n\n\nKey takeaways:\n\nThe angular distribution \\frac{d\\Gamma}{d\\cos\\theta} is central to analyzing particle decays.\nLegendre polynomials P_\\ell(\\cos\\theta) provide a basis for partial wave analysis.\nThe observed probability depends on the squared amplitude |M|^2.\n\n\n\n\n\n\n0.12 Helicity vs. Canonical States and Kinematic Decay Exercise\nI didn’t tell you much about the differences between the canonical state that we introduced at the beginning and the helicity state that we introduced later. We only touched a little bit on how the state is defined in the rest frame, and hopefully we’ll get to explore more.\n\n\n\n\n\n\nFor reference, helicity state transformations typically involve formulas like: |p,\\lambda\\rangle = U(L(p)) |k,\\lambda\\rangle where L(p) is the Lorentz boost and \\lambda represents the helicity eigenvalue.\n\n\n\nI would like to tell you that this book has the best coverage of this subject:\n\nMartin Spearman’s Elementary Particle Theory\nChapter Four is fundamental\n\nIt’s really fun reading because:\n\nIt starts from the Lorentz group\nExplains how to introduce the vectors\nIncludes a little bit of group theory, but in a nice way without heavy details\nPresents concepts without too much mass\n\nIt’s really a good book. So Chapter Four of Martin Spearman’s Elementary Particle Physics would give you some insights on the particle definitions.\n\nI was just going to hand in an exercise:\n\nThere are some Dalitz plots from CLEO and BaBar\nI have removed the labels\nI don’t tell you which particles are in the final state\nI just tell you that one of them is the D decay, another one is the D_s decay\n\n\n\n\n\n\n\nRemember that Dalitz plots involve invariant mass calculations: m_{12}^2 = (p_1 + p_2)^2 where p_1 and p_2 are four-momenta of decay products.\n\n\n\nYou know a lot about kinematics already. The exercise is to:\n\nFigure out what decay this is\nUse the axis labels (still present)\nDetermine the masses from kinematics\nGuess what the decay is\n\nI have cases:\n\nOne for your group\nAnother one to work on\nAnd the EP one\n\n\n\n0.13 Addressing Homework and Scheduling\nLecture Segment (Logistics Discussion):\n\n\n\n\n\n\nThis appears to be an administrative conversation about homework distribution and scheduling.\n\n\n\n\nInstructor: “I have some issues and need to leave.” “You don’t want to take it? No?”\nAbout the task: “It’s a kind of homework.” “Oh, sorry.”\nFollow-up instructions: “Come with me, and I’ll give it to you from my office.” “You guys as well.”\nClosing remarks: “Thanks for coming, and sorry for being slightly late.” “Will you have time tomorrow at 8am?”"
  },
  {
    "objectID": "2024-Lecture-05-test-mac.html",
    "href": "2024-Lecture-05-test-mac.html",
    "title": "",
    "section": "",
    "text": "0.1 Angular Distributions, Partial Wave Analysis, and Kinematics in Two-Particle Scattering\nToday we are at lecture number five. We’ll discuss angular distributions and partial wave analysis. But before going there, I would like to start with a recap.\n\nIn the last lecture, we discussed:\n\nThe phase space for particle reactions\nDifferent experiments and their kinematics\nExperiments worldwide studying hadrons, their production mechanisms, and peculiarities\n\nWe start with a recap on kinematics.\n\nFirst question: How many variables describe a two-particle scattering process?\nWe have two cases:\n\nScalar particles (0^-):\n\n\nInitial state: two 0^- scalars\nFinal state: two 0^- scalars\n\n\nParticles with spin:\n\n\nExample: 1^+ scattering with 3^-\nFinal state: 3^- and 1^+\n\n\n\n\n\n\n\nThe scattering process is represented by a “blob” (unitarity diagram, not a Feynman diagram). The blob encapsulates the interaction (strong, electromagnetic, etc.), while arrows indicate incoming/outgoing particles.\n\n\n\n\nTask: Calculate the number of variables needed to fully describe the process.\n\nWithout spin:\n\n\nInitial count: 8 (from conservation laws)\nSubtract 6 (3 rotations + 3 boosts due to frame invariance)\nResult: 2 variables\n\n\nWith spin:\n\n\nSame as above: 2 variables (spin orientation doesn’t add independent kinematic variables for 2→2 scattering).\n\n\n\n\n\n\n\nThe scattering amplitude for spinless particles is a scalar function of s and t. For particles with spin, it becomes a higher-rank object (e.g., 21-dimensional for 3^- \\times 1^+), but still depends on the same two variables.\n\n\n\n\nThe process is fully described by two independent Lorentz-invariant variables. Common choices:\n\nMandelstam variables:\n\n\ns = (P_1 + P_2)^2 (center-of-mass energy squared)\nt = (P_1 - P_3)^2 (momentum transfer squared)\nu = (P_1 - P_4)^2 (with s + t + u = \\sum m_i^2)\n\n\nCenter-of-mass frame variables:\n\n\n\\sqrt{s} (center-of-mass energy)\n\\theta (scattering angle between \\vec{p}_1 and \\vec{p}_3)\n\n\n\n\n\n\n\nAny two independent variables work (e.g., lab-frame energies E_1, E_3), but Mandelstam variables avoid frame-specific distortions.\n\n\n\n\nFor three-body decay (1 \\to 2 + 3 + 4), the kinematics are similarly described by two variables:\n\ns_{34} = (P_3 + P_4)^2\nt_{13} = (P_1 - P_3)^2\n\nKey properties:\n\nThe phase space is flat in Mandelstam variables (dN/ds_{34}\\,dt_{13} = \\text{constant}).\nThis makes the Dalitz plot a powerful tool for studying dynamics.\n\n\nThe three-body phase space is given by: \nd\\Phi_3 = \\frac{1}{(2\\pi)^5} \\frac{d^3p_2}{2E_2} \\frac{d^3p_3}{2E_3} \\frac{d^3p_4}{2E_4} \\delta^4(P_1 - P_2 - P_3 - P_4)\n\nUsing the recursive formula: \nd\\Phi_3 = d\\Phi_2(M;m_3,m_4) \\times d\\Phi_2(M\\to m_1; m_2, M_{34}) \\times \\frac{1}{8\\pi^2} ds_{34}\n\n\nThe Dalitz plot (plotting s_{34} vs. t_{13}) directly reveals dynamics:\n\nNo kinematic distortions in Mandelstam variables.\nDeviations from flatness indicate resonances or interactions.\n\n\n\n\n\n\n\nThis marks the shift from general particle physics to hadron-specific topics, focusing on spectroscopy and three-body decays.\n\n\n\n\n\nExercise: Study the Dalitz plot for three-body decays.\nNext: Dive deeper into angular distributions and partial wave analysis.\n\nQuestions? Let’s discuss before moving on!\n\n\n0.2 Charm Decay Kinematics and the Dalitz Plot in \\Lambda_c Baryon Decay\nYou can see the example of the triple decay that I have here: the \\Lambda_c baryon going to the proton, kaon, and pion. We measure \\Lambda_c produced in proton-proton collisions or any other collisions. In the BES experiment and Belle experiment, they observe \\Lambda_c. This is one of those particles that lives a long time and is produced abundantly.\n\nParticles with charm ground states are produced abundantly, and they live sufficiently long to fly from the primary vertex. We reconstruct them, which is why we have a good sample and a good understanding of their decay kinematics—and not only kinematics but dynamics as well, the content of this blob.\n\nIn that decay, you see there is a charm quark in the initial state and no charm quark in the final state, indicating this happens via weak interaction. The charm quark disappears between the initial and final states. It decays, transitioning into the strange quark that ends up in the kaon. The c \\to s transition happens within one generation, so this is not suppressed—it’s an allowed process.\n\n\n\n\n\n\nThe weak decay process involves: \\Lambda_c^+ \\to p^+ + K^- + \\pi^+ c \\to s + W^+ \\quad \\text{(with } W^+ \\to u\\bar{d}\\text{)}\n\n\n\n\nThis is the golden channel for detection because, in the final state, you have three charged particles. There are no neutrals.\n\nThe proton is a nice charged particle—it travels and is stable.\nThe kaon is stable in our accelerator experiments.\nThe pion is also stable.\n\nThey fly out from the decay without any distraction, and we see their tracks clearly through all detectors. We observe them pointing away from the primary interaction.\n\nThere is roughly a 10-millimeter shift in \\Lambda_c flight distance between the primary vertex and the secondary vertex—about one centimeter. This is due to the boost and the fact that \\Lambda_c in the laboratory frame lives longer than in its rest frame.\n\n\n\n\n\n\nThe observed flight path is explained by: \\Delta x = \\gamma v \\tau_0 where \\gamma = E_{\\Lambda_c}/m_{\\Lambda_c} is the Lorentz factor.\n\n\n\nIt is produced with several hundred GeV in proton-proton collisions at the LHC. This is a very clean decay, and we have studied it extensively.\n\nHere is the experimental result of the analysis, which closely resembles the data. If I showed you the actual experimental data, you wouldn’t distinguish it from this plot because the statistics are so high that the distribution is very smooth.\nOn the x-axis, I have the invariant mass squared of the proton and kaon: m_{pK}^2 = (p_p + p_K)^2\nOn the y-axis, I have the invariant mass squared of the kaon and pion: m_{K\\pi}^2 = (p_K + p_\\pi)^2\nAll allowed kinematic values for the decay are shown in color. The white area around it corresponds to kinematics where energy and momentum conservation cannot be satisfied.\n\nIf I select a point inside the plot, I can compute the angles between particles and their momenta, reconstructing the full 3D decay configuration. But if you give me kinematics in the white area, I immediately see that energy conservation is violated—it’s impossible. This is why the range of possible invariant masses is limited, and this surface is called the Dalitz plot.\n\nThis is how we explore the kinematics. Different colors on the plot indicate different probabilities for the decay to occur in specific kinematic configurations. We measure the decay, reconstruct the particle tracks, and determine from which kinematic point it originated because there’s a direct mapping between the four-vectors and the Dalitz plot coordinates.\nIt turns out that certain kinematics are more probable than others. Particles prefer specific momentum configurations.\n\nFor this decay, for example, one kinematic configuration is more likely than another. You should be able to identify this pattern in the plot. Let’s take a moment to find it.\nThe hint is that particles are aligned in one line at the plot’s boundary. Inside the surface, they always have an angle between them, but at the boundary, they become collinear.\n\n\n\n\n\n\nAt the Dalitz plot boundaries: \\theta_{ij} = 0 \\text{ or } \\pi \\text{ between particles}\n\n\n\nThink: how do you maximize the invariant mass, and where does this point lie on the boundary?\n\n\n\n0.3 Maximizing and Minimizing Mass in Three-Body Decay Kinematics\nAny thoughts? I think you should be in the bottom right because we should maximize the mass of the PK. There are three momenta going out there. The three momenta are in opposite directions, so the three-momentum of the sum should be as small as possible.\nIf we add both momentum vectors, the forward momentum—the sum of the squares—should be as large as possible. We should be on the right of our diagram. If we apply and find… The same direction should be on the lowest.\nYou argue that this mass, on the y-axis, should be as small as possible. Why? Because the three momenta are in line, and we subtract them. If you go to their rest frame, the K and π fly next to each other. Their relative momentum is small. If you boost to their rest frame, they might both be at rest. Their mass would just be the sum of the masses, so it’s minimum.\n\n\n\n\n\n\nKey Relation for Invariant Masses in Three-Body Decay: For a decay like \\Lambda_c \\to p K \\pi, the invariant masses satisfy: \nm_{12}^2 + m_{23}^2 + m_{13}^2 = m_{\\Lambda_c}^2 + m_p^2 + m_K^2 + m_\\pi^2\n where m_{ij} are the invariant masses of particle pairs.\n\n\n\nWhat you’re saying is correct. We are looking for the minimal mass of the kinematic. Let’s figure out what this point corresponds to. There is kinematics like this: two particles with maximal momentum are at rest and go in opposite directions. That corresponds to this one.\nThis plot is from experimental data. In the experiment, we would reconstruct such a case because we would not detect the proton. This is the lab frame, but everything we measure is in the lab frame where everything is boosted. Even though the proton is at rest in the lab frame, it’s already boosted.\n\nThis point is the maximum mass. They have maximum momentum and go back-to-back, so this is the maximum mass. This one minimizes the mass. For the three-body decay, there is a similar way to define the angle.\nLet me show you the setup. I’m using proton, K, and π. I want to fix the mass of the K-π system. I would like to explore that by changing the mass of the proton.\nThink of this in the center-of-momentum frame and boost to the K-π rest frame. In the K-π rest frame, they go back-to-back. In the center-of-momentum frame, three particles are arranged such that the sum of the three-momenta is zero.\nOnce I boost, the \\Lambda_c has non-zero momentum. The K and π also have non-zero momentum, but since I am in their rest frame, these two momenta add to zero. If I fix the mass of the system and explore phase space along the line where the mass of this pair is fixed, the length of these vectors is fixed.\nWhat is changing? Only the angle \\theta. I explore the whole range of the angle from 0 to \\pi. At one corner, \\theta = 0; at the other, \\theta = \\pi.\n\nThe right should be zero and the left… Let’s see how you get to that. We fixed the setup where the proton and K-π go in opposite directions. This has the maximum mass of the proton-K system.\nFor our new setup, as the length of all vectors is fixed, we can only rotate. The dependence of the mass of two particles on \\theta is such that the wider the angle, the bigger the mass. For the proton-K system, \\theta = 0 gives a very high mass. If they go almost in the same direction, they have a small mass.\n\n\n\n\n\n\nInvariant Mass Relation for Fixed m_{K\\pi}: \nm_{pK}^2 = m_{\\Lambda_c}^2 + m_K^2 + m_\\pi^2 - m_{K\\pi}^2 - 2E_{K\\pi}E_p + 2|\\vec{p}_{K\\pi}||\\vec{p}_p|\\cos\\theta\n where E_i and \\vec{p}_i are energies/momenta in the \\Lambda_c rest frame.\n\n\n\nI can do the same with the other half. Let me fix the mass of the proton-K system again. The most straightforward way to analyze is to go to the rest frame of the proton-K system, where everything is fixed. Then scan along this line by changing the angle of this pair with respect to the rest.\nThe lines describe the setup where you change the angle in one frame or another. There is a third variable in the 2-to-2 scattering called U, but it’s more symmetric. For three particles, there is the mass of the pion-proton system.\nIf I fix the mass of this system and scan the angle, this is easy to understand from the relation that U is a linear combination of the two. The coefficients are one, so it’s a diagonal in this plot.\n\nThis line is the mass of the pion-proton system fixed. I go from one corner to another, changing the representation. The representation we use in experimental analysis for the Dalitz plot has the mass of one pair on the x-axis and the other on the y-axis.\nIn the homework, you have an exercise with a more symmetric Dalitz plot where all variables enter symmetrically. This uses properties of an equilateral triangle: every point inside has distances to the sides that sum to a constant.\nThe variables—masses of particles—are the distances to the sides. I can look at it like this or like that. This is a super symmetric, nice representation. Essentially, it’s the same as this.\nThey’re not matching because one and another… Wait, should they match? It’s a linear transformation, skewed. To plot this, I had to find the relation between Cartesian coordinates and the heights of the triangle.\nWhat appears is \\sqrt{3}/2 because of the 60° angles. This is a nice symmetric representation, but the other is more common and easier to plot. Both represent the kinematics.\nThe objective is to understand the dynamics and processes guiding this interaction. Looking ahead, we realize this process is not just \\Lambda_c decaying to three particles but proceeding via intermediate resonances.\nFor a short moment, two particles form an intermediate state that dissociates, increasing the decay probability. These two particles interact more at certain energies, so the probability increases.\n\nYou might have seen cross sections for two-particle resonances with bumps known as hadronic resonances. The physics is that the quantum numbers of the system match some resonance.\nBy adjusting the energy, you explore how likely the particles are to interact. If there is an intermediate resonance, the system resonates at this energy, and the probability increases.\nThis leads to bent structures in the Dalitz distribution. Projected onto one axis, you see a resonance-like shape. These bent structures appear in all three pairs.\nWhy is there a bigger probability increase when the kaon and pion are near resonance? Let’s understand the lines. We’ll come back to this in two lectures.\nAll horizontal lines—which resonances are these? In which mass distribution? Horizontal lines in the blue plot correspond to fixed mass for K\\pi. These are K^* resonances.\nNow, the vertical lines—in which system? When changing K\\pi, I scan a certain energy. This corresponds to fixed mass of the proton and K\\pi.\nThe lines correspond to resonances in K\\pi, proton-K, and pion-proton. The other plot helps because the lines are parallel to the sides of the triangle.\nThis line is the same as this one, parallel to one side. The lambda resonances are parallel to another side. The delta resonance is parallel to the third side.\n\nOkay, questions on the angular distribution. To move past this, let’s discuss the angular distribution for a decay within one band.\nThe kinematics: traversing the Dalitz plot while keeping the mass fixed changes the angle. This is the kinematics.\nIn the rest frame of K\\pi, traversing phase space changes this angle. The angle dependence indicates inhomogeneity. Within the band, one edge might have a different probability than the other.\nParticles might prefer to be aligned rather than perpendicular. This happens because particles have spin. The intermediate resonance, like K^*, is not scalar—it has spin.\nSpin causes inhomogeneity in angular distributions and decay spectra.\nAngular distribution is a powerful tool to understand particle properties. It measures spin, parity, and other quantum numbers.\nHigher-spin particles produce bumpier angular distributions. Scalar particles produce no asymmetries.\nIn the rest frame of the decay, examining the ratio of aligned kinematics reveals spin information.\nFor most discovered particles, quantum numbers are unknown. We find them as bumps, then determine quantum numbers via angular distributions.\nOften, it’s as simple as looking for minima or nodes in the Dalitz plot. Nodes indicate spin: one node for spin-1, two for spin-2, etc.\nFor non-scalar particles, it’s more complicated. The proton has spin-½; kaon and pion are spin-0. The \\Lambda_c has spin-½ like the proton.\nIf we don’t polarize the initial state or measure the final spin, everything is averaged. No minima or nodes remain—just smeared distributions.\n\n\n0.4 Rotations, Spin Projections, and Wigner D-Functions in Quantum Systems\nA particle with spin can have 2J + 1 projections to the quantization axis. Consider a particle with spin J. There is a z-axis that quantizes the spin, and this z-axis measures the projection J_z. The state |JM\\rangle can be thought of as a vector with 2J + 1 components. All operators in this case are matrices that act on these vectors, producing either the same state with an eigenvalue or a mixture of states.\nWhen a rotation acts on the state, it produces not a single state but a mixture of states. In classical physics, rotating a vector in space can align it to a specific projection, but in quantum mechanics, this is not the case. Most rotations result in a superposition of all possible states.\nThe coefficients for these superpositions are tabulated and known as Wigner D-functions. Let me be more concrete. I will rotate about the Y-axis. The coordinate system consists of X, Y, and Z, forming a right-handed triple. I will rotate the state |JM\\rangle about the Y-axis.\nThe operator J_+ is represented as \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, and J_- is \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}. The J_y operator has zeros on the diagonal and off-diagonal elements above and below. To apply the rotation, we take the matrix exponential of J_y. This is already computed, and the coefficients C depend on the initial state, which is why the indices JM are kept in the notation.\nFor example, suppose we have a spin-1 particle initially perpendicular to the Y-axis and rotated by 30 degrees. The resulting state will be a superposition of |JM\\rangle states aligned along the Y-axis. The Wigner D-functions provide the coefficients for this superposition.\nThe general form of the D-function for any rotation R is given by Euler angles. In particle physics conventions, we first rotate by \\phi about the Z-axis, then by \\theta about the Y-axis, and finally by \\gamma about the Z-axis again. This sequence describes any orientation in space. The D-function decomposes into three parts, with the most complex part coming from the Y-axis rotation. The Z-axis rotations introduce simple phase factors:\n\nD^J_{M'M}(\\gamma, \\theta, \\phi) = e^{-i\\gamma M'} d^J_{M'M}(\\theta) e^{-i\\phi M}.\n\n\n\n\n\n\n\n\nKey Formula: The Wigner D-function for a general rotation (Euler angles \\alpha, \\beta, \\gamma) is: \nD^J_{M'M}(\\alpha, \\beta, \\gamma) = e^{-iM'\\alpha} d^J_{M'M}(\\beta) e^{-iM\\gamma}.\n\n\n\n\n\nLet’s take a simple example: spin-1/2. If I rotate the state |1/2, 1/2\\rangle by 30 degrees about the Y-axis, the result is a superposition of |1/2, 1/2\\rangle and |1/2, -1/2\\rangle. The coefficients for this rotation can be found in the same tables as the Clebsch-Gordan coefficients, as both are related to the SU(2) group.\nFor j = 1/2, the D-matrix is:\n\nD^{1/2}_{m'm}(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -\\sin(\\theta/2) \\\\ \\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix}.\n\nFor 30 degrees (\\theta/2 = 15^\\circ), the coefficients are \\cos(15^\\circ) and \\sin(15^\\circ).\n\n\nCan you compute any rotation of the spin projection?\nThe convention is important—note the minus sign.\nIf you want to compute the matrix exponential yourself, you can use Python or Julia.\nAlternatively, you can look up tabulated D-functions, but be cautious with software like Mathematica, as it may use different conventions.\nWikipedia is reliable for checking conventions.\nThe D-functions are correctly implemented in SymPy (Python) and ROOT.\n\n\nWhat we’ve discussed so far is about rotations and the rotational group, not weak or strong interactions. This is remarkable because angular distributions in particle decays are determined by rotational symmetry, not by the details of the interactions. The only input from strong interactions is the preference for certain spin states. The decay asymmetries and kinematics are governed by the quantum group properties. This allows us to construct general formulas for decay chains and predict angular distributions.\n\nNow let’s model the “blob” in the decay process. Assume a cascade decay where the initial particle decays into an intermediate particle X with spin J, which then decays into particles 1 and 2. The amplitude involves summing over intermediate spins and depends on the dimensions:\n\n(2j_0 + 1) \\times (2j_1 + 1) \\times (2j_2 + 1) \\times (2j_3 + 1).\n\nIf a particle has spin 0, its dimension is 1.\nThe general formula for the amplitude has two parts:\n\nA model-independent part from rotational symmetry.\nDynamics-dependent terms H from weak, strong, or electromagnetic interactions.\n\nThe H terms encapsulate the physics of the interactions, while the rest is determined by rotational properties. The helicity states \\lambda are defined with spin quantized along the direction of motion.\n\nConsider particle X with spin projection \\lambda_X. When it decays into particles 1 and 2, the quantization axis must be adjusted to match the decay direction. This adjustment is done by rotating the spin state of X.\nIn aligned kinematics (where \\phi = \\theta = 0), the rotation simplifies. The amplitude reduces to:\n\nA = \\sum_{\\lambda_X} H_{\\lambda_0 \\lambda_X \\lambda_3} \\cdot D^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta, \\phi) \\cdot H_{\\lambda_X \\lambda_1 \\lambda_2}.\n\nFor zero angles, the D-function becomes a delta function \\delta(\\lambda_0 - (\\lambda_X - \\lambda_3)), enforcing angular momentum conservation.\n\nTo predict angular distributions, we need the values of H. Often, these are approximated as constants in initial analyses. The differential decay rate is:\n\n\\frac{d\\Gamma}{d\\cos\\theta} \\propto |A|^2.\n\nThe distribution can be flat, parabolic, or another shape, depending on the spin.\nExperimentally, we measure |A|^2 averaged over initial and final spins. To analyze data, we project the distribution onto Legendre polynomials, a technique called partial wave analysis. This helps extract information about the underlying dynamics without knowing H explicitly.\n\n\n\n\n\n\n\nPractical Note: For further reading on canonical vs. helicity states, refer to Chapter 4 of Martin Spearman’s Elementary Particle Theory.\n\n\n\n\nFinally, here’s an exercise: I’ve provided Dalitz plots from CLEO and BaBar without labels. Your task is to identify the decays based on kinematics. The axes are labeled, but the particles are not named. Use your knowledge of kinematics to deduce the masses and possible decay channels.\n\n\n0.5 Addressing an Issue and Scheduling Follow-Up\n\n\n\n\n\n\nThis portion of the transcript appears to be administrative dialogue about scheduling and materials distribution.\n\n\n\n\nSpeaker 1: I have to leave because I have an issue.\nSpeaker 2: You don’t want to take it? No.\n\nExchange about assignments:\n\nSpeaker 1: It’s a kind of homework.\nSpeaker 2: Oh, sorry.\n\nFollow-up arrangement:\n\nSpeaker 1: Okay, come with me and I’ll give it to you from my office.\nSpeaker 1: And you guys as well.\n\nClosing remarks:\n\nSpeaker 1: Thanks a lot for coming, and sorry for being slightly late.\nSpeaker 1: Will you have time tomorrow at 8am? Leave."
  },
  {
    "objectID": "2024-Lecture-05-with-score.html",
    "href": "2024-Lecture-05-with-score.html",
    "title": "",
    "section": "",
    "text": "0.1 Angular Distributions, Kinematics Variables, and the Dalitz Plot in Two-to-Two Scattering\nToday we are at lecture number five. We’ll discuss angular distributions and partial wave analysis. But before going there, I would like to start with a recap.\n\nIn the last lecture, we discussed the phase space for particle reactions and different experiments and their kinematics. We went through the list of experiments around the world that study hadrons and look at their production mechanisms and some peculiarities.\n\nWe start with a recap on kinematics. The first question: How many variables does one need to describe the two-to-two scattering process? We have two problems:\n\nScalar particles—let’s say 0-minus scalars.\n\n\nThe scalar particles (0-minus) scatter to 0-minus, and the final state also has two 0-minus scalar particles.\n\n\nSpin-dependent scattering.\n\n\nExample: P-plus.\nIt’s again scattering of 0-minus from 0-minus to, let’s say, 3-minus and 1-plus.\n\n\n\n\n\n\n\nThe scattering process is represented by a “blob” indicating the interaction (e.g., strong interaction). Arrows show particles entering and leaving. This is a unitarity diagram, not a Feynman diagram.\n\n\n\nHere is a little blob that indicates generally what we are discussing. This is the scattering process—something described by strong interaction theory happens inside the blob, and the arrows indicate the particles entering the interaction and then leaving it. This is a cartoon diagram. These are not Feynman diagrams; sometimes they’re called unitarity diagrams. We will touch on unitarity in the following lectures, but for now, this is a nice way to indicate what we are essentially talking about.\n\nThen I ask you to calculate the number of variables needed to describe the process entirely—the full kinematics—and we are talking only about kinematics at this point. Whatever happens in the blob does not impact the answer. The blob could be electromagnetic, strong, gravity—whatever you want. Something happens inside the blob, and the question is: How many variables do you need?\nThere are two possibilities:\n\nParticles without spin.\nParticles with spin—how many variables do you need to describe the process?\n\nThe second is related to the pictures I gave you. But let’s do it one by one.\nI also added a third item: Give an example of the combination of variables that fully describe the process. For (a) and (b), you need to count—tell me how many—and then (c) gives the example.\n\nLet’s quickly check how many. The calculation is like two times two, or four. Then we multiply by three. Then we subtract four due to conservation laws. So one should have eight—and that’s correct. Now you get rid of the… This is the entire kinematics. There are no other things in the space that would constrain orientation. Essentially, for describing the kinematics, I can go to any arbitrary frame and rotate the space. So I have to subtract another six—three rotations, three boosts.\nThe eight is the correct number if you just take into account conservation laws. But then you can account for having no reference, no anchor in space, and subtract another six, leaving you with two. So for (a), you’ve got two.\n\nWhat did you get for (b)? You have the two from the previous scenario, and you have the angular distribution of the final-state particles. So they are… I mean, I don’t have any decay information here—just two-to-two. Then four? Why four? The angle between the two production particles and the angle between the 3-minus and 1-plus particles. How would these angles be different? Why can’t I introduce these angles in (a)? You can, but they are not necessary. So it’s actually two as well.\n\nHere, I would have a scalar amplitude that describes the amplitude as a function of S and T, where:\n\nS = (P_1 + P_2)^2\n\n\nT = (P_1 - P_3)^2\n\nNow I’ve made my cartoon a bit richer—I indicate the channels or the variables in Mandelstam invariants that describe the process. The energy of the system is given by the variable S, which is the sum of four-vectors squared. The transferred momentum in direction is T, which describes how energy—essentially analogous to this variable, but in the vertical direction—so I describe it as P_1 - P_3.\nIn my cartoon, I also added the P’s, which are four-vectors here. They have an energy component, a three-dimensional component, and, for particles with spin, indicated spin orientation—their helicity values, \\lambda_3 and \\lambda_4.\n\nIn the first case, my amplitude describing the blob—the interaction—is a scalar function. It’s just a single number. In the case where particles have spin, it’s not a number—it’s a higher-rank object. So here, in that case, what is the size dimension of my scattering amplitude?\n\nSpin-3 gives me seven dimensions.\nSpin-1 gives me three dimensions.\nTherefore, my scattering amplitude is a 21-dimensional object.\n\nSo I have 21 amplitudes, but all of them are functions of two variables.\n\n\n\n\n\n\nWhen particles have spin, you have more amplitudes, but they all have the same dependencies. The situation changes when you take into account the decays of the final-state particles.\n\n\n\nUsually, we do not know any stable 3-minus particles, nor any stable 1-plus particles—they would fly away, and we would consider them final-state particles. Therefore, both of these particles decay. Often, you can take into account the decay products and introduce more variables. But if I restrict the problem to this—full stop—two variables, that’s it.\n\nSo Henrik, what are your favorite two variables to describe a two-body scattering process? The mass of particle one and two, and particle one and three. I guess it would be something with the mass of the two initial particles. That works.\nNow tell me how to compute mass. It’s the energy squared minus the momentum squared. Energy minus kinetic energy doesn’t work. I think I would use this: if I want to compute the mass of two particles, I would take the four-vector of the first, the four-vector of the second, add them together, and then take the Lorentz norm squared of this vector, which is energy squared minus momentum squared. So this is mass squared.\nEssentially what you suggest is to take this variable as one of them and take another one which is the other mass. But you want to put a sum plus here—that works. Actually, that’s okay, you can put a plus, but it’s going to be a mixture of S and T. What is more common is to put particles in the final state with a minus sign, in the initial state with a plus sign, and then have this, what you propose, to have masses in this convention.\n\nAnd then these variables, S and T, are called invariants, Mandelstam variables. They are quantities that characterize the decay, and they are Lorentz invariant. They don’t describe a specific rest frame, so they don’t describe a specific setup of the reactions. They are just something that characterizes the whole process. It doesn’t matter in which frame you consider, so S and T are Mandelstam.\nI learned recently that there is no word “invariance” in the dictionary—somehow I didn’t find it. When I say invariance, but I use it all the time, I mean invariant variants, environment, variables. Maybe it exists, but I was looking at the wrong dictionary. To me, it sounds very natural to call them invariants.\n\nAnyway, give an example of these sets of variables. We’ve got one from Hendrik. So everyone should get their favorite. So Ilya, what’s your favorite? S and T. No, not a single pair. These sets. So we need two variables to describe the process. What are your favorite two variables?\nWhat about you, Sven? P_2 - P_4 is called U, and it’s also a variable. P_2 - P_4 is equal to P_1 - P_3. So P_1 - P_4 squared is called U. And this is the same as… If I do the algebra, you find out that there are only two independent. So your favorite set is S and U, and they’re equivalent because U is a linear combination of S and T.\nFine. So you guys sometimes take T and U. No, no, no. Let’s do different center-of-mass energy and angle. That’s a really good choice. That’s probably gonna be my favorite. And this is actually \\sqrt{S}. Center-of-mass energy and then the angle. How do you define the angle? You go to the center of mass and then you make particle…\nWell, center of mass is often referred to as center of momentum as well. So in the center-of-momentum frame, it’s just easier to call it center of momentum because you immediately understand that momentum, the total momentum, has to add to zero for all particles. And do you have a P_1 here and P_3? Then what do we have? What else do we have? How do you define the angle? They are going in this direction and the angle between P_1 and P_3. And then the angle between P_1 and P_3. So essentially this one. That’s my favorite.\nIndeed. And well, one has to be careful when drawing that because we know that the length of the vectors indicates their momentum. Particle’s momentum. And for the final state, it’s in the center of momentum. We’re still in the same frame, so this has to be equal to that. I’m fine with that choice.\nThere are no more common choices. But any two variables work if they are not redundant. So we can choose E_1 in the lab frame, any frame, and then E_3 in the lab frame. That’s also fine. Any two variables, if they are independent, they characterize the kinematics. One has to be careful. Sometimes you fold your phase space. So by introducing a set of variables, you map your phase space in these variables or these variables to another domain. And sometimes this domain is somewhat smaller, it has a folded coverage. So this variable, it’s not a bijective transformation, but this is advanced to see.\n\nWe discussed two-body kinematics. We’re going to continue discussing the angular dependence after the Dalitz plot. Just let’s quickly look—oh, questions here. Shortly before we move, questions on the variables that characterize kinematics.\nSo the homework we had the exercise on the Dalitz plot, and this is, as I mentioned, a problem that is now entering our lectures. Part of the course is specific to hadrons. And it’s before where we’ve been overlapping a lot with particle physics. But now, from this lecture and the next couple of lectures, we will have material really specific to hadron spectroscopy and the approach that we use to discuss hadrons. And one of them, one of the subjects that we would like to go deeper into, is particle representation—essentially three-body decay. And Dalitz’s law is the common technique to indicate the dynamics of the particles, the dynamics of the interactions.\nIn the case of three-body decay, we deal with a similar diagram as before, but now one leg comes in, three legs go out, and what is inside the blob is an interaction. And we can pose all the same questions as before: What are the number of variables? But the answer won’t differ because it’s the same number of legs. Essentially, the same number of legs tells you the number of variables. For three-pion or DDK decays, there are two variables that describe the process completely. Once you give me these two variables—S and T, or angles, or any other—I should be able to draw the entire kinematics.\n\n\n\n\n\n\nThe Dalitz plot is a powerful tool for analyzing three-body decays, where the phase space is flat in S and T variables: \n\\frac{dN}{dS\\,dT} = \\text{constant}\n This allows for clear visualization of dynamical effects.\n\n\n\nSo remember my analogy of the rigid body: when you print on a 3D printer a blob out of which the vectors are sticking, this is a rigid body that describes a kinematic point. The angles between all vectors are fixed. The lengths of the vectors are fixed. So you have the entire setup of the kinematics at a single point in phase space. The same goes for three-body decay. Just give me two variables, and I should be able to draw you how the decay looks like in the center of mass.\nIn that case, I have to draw here two vectors that leave. And in that case, I’m going to draw the three vectors. So essentially, this is the support out of which the vectors stick. The vectors determine the angles, the lengths of the vectors, and that’s it—that’s what you have.\nNow, S and T are defined in a similar way. But now we have different particles in the final state. So, from that kinematics to this kinematics, what it takes is to take one leg and swap it to the other side. And it’s done by changing the sign on the momentum.\nSo, for three-body decay, what we do—let me define it here:\n\nS = (p_1 + p_2)^2 = (p_3 + p_4)^2\nT = (p_1 - p_3)^2\n\nI noticed that there was a typo: p_1 is the particle that decays. In that case, I don’t know—should I change it? Okay, let me update my notation later on. For now, let me stick to the notations that relate to kinematics.\nIt’s important to realize that the phase space for three-body decay is flat in the variables—it’s actually constant in the variables p_3, p_4, p_2, p_4. In S and T, we write it as \\frac{dN}{dX}.\n\n\n480/600 = 80/100\n\n\n0.1.1 Physics Accuracy Assessment\n\n0.1.1.1 Statement 1:\n\nStatement: “Scalar particles (0-minus)”\n\nComment: Severe error. Particles with J^P=0^- are pseudoscalars (e.g., \\pi^0, K^0), not scalars. Scalars have J^P=0^+ (e.g., f_0(500)). Misidentifying quantum numbers undermines core formalism.\n\nRating: 20/100\n\n\n\n0.1.1.2 Statement 2:\n\nStatement: “Spin-3 gives seven dimensions; spin-1 gives three dimensions; scattering amplitude is a 21-dimensional object.”\n\nComment: Partially correct but misleading. For massive particles:\n\nSpin-j has \\mathbf{2j+1} helicity states: spin-3 → 7D, spin-1 → 3D.\n\nAmplitude dimensionality is \\mathbf{(2j_1+1) \\times (2j_2+1) \\times \\cdots} (here 7 \\times 3 = 21D).\n\nError: Ignores that amplitudes reduce under Lorentz symmetry. For massive particles, the rank is correct, but for massless cases (e.g., photons), helicity \\lambda=\\pm1 reduces dimensionality.\n\nRating: 65/100\n\n\n\n0.1.1.3 Statement 3:\n\nStatement: “Two variables suffice for 2→2 kinematics, regardless of spin.”\n\nComment: Correct. Kinematic degrees of freedom are frame-independent:\n\n4-momenta → 16 components.\n\n4-momentum conservation → 4 constraints.\n\nLorentz invariance → 6 constraints (3 rotations + 3 boosts).\n\nNet DOF = 16 - 4 - 6 = 6, but for 2→2, two Lorentz-invariant variables (e.g., s, t) suffice. Spin affects the amplitude structure, not kinematic DOF.\n\nRating: 100/100\n\n\n\n0.1.1.4 Statement 4:\n\nStatement: “There are no stable J^P=3^- or J^P=1^+ particles.”\n\nComment: Correct. All observed J^P=3^- (e.g., \\rho_3(1690)) and 1^+ (e.g., a_1(1260)) states are resonances decaying via strong/EM interactions. No stable hadrons exist with these quantum numbers.\n\nRating: 95/100 (deducted for omitting “strong/EM decay” context).\n\n\n\n0.1.1.5 Statement 5:\n\nStatement: “Three-body decays require two kinematic variables; Dalitz plot phase space is flat.”\n\nComment: Correct. Three-body decay (e.g., D^+ \\to K^-\\pi^+\\pi^+) has:\n\n12 momentum components → 4 conservation constraints → 8 DOF.\n\nMinus 6 for Lorentz invariance → 2 Lorentz-invariant variables (e.g., s_{12}, s_{13}).\n\nPhase space density \\frac{d\\Gamma}{ds\\,dt} is constant if matrix element is uniform.\n\nRating: 100/100\n\n\n\n0.1.1.6 Statement 6:\n\nStatement: “Mandelstam s, t, u: Only two are independent; u is a linear combination.”\n\nComment: Correct. For 2→2 scattering: s + t + u = \\sum m_i^2. Only two are independent (e.g., s and t).\n\nRating: 100/100\n\n\n\n0.1.1.7 Statement 7:\n\nStatement: “Invariance is not a valid word.”\n\nComment: Irrelevant/non-physics. “Invariance” is standard in physics (e.g., Lorentz invariance). Distractor with no impact on content.\n\nRating: N/A (excluded from scoring)\n\n\n\n0.1.1.8 Statement 8:\n\nStatement: “The scattering blob is a ‘unitarity diagram’, not a Feynman diagram.”\n\nComment: Correct distinction. Unitarity diagrams represent S-matrix unitarity cuts; Feynman diagrams are perturbative. Crucial for non-perturbative formalisms.\n\nRating: 100/100\n\n\n\n\n\n0.1.2 Total Score: 480/600 = 80/100\nSummary:\n\n⚠️ Major warning for misidentifying pseudoscalars as scalars (Statement 1). This is a critical error in quantum number classification.\n\nCore kinematics (Statements 3,5,6,8) are flawless.\n\nSpin dimensionality (Statement 2) requires clarification but is salvageable.\n\nNo stability issues (Statement 4) beyond minor context omission.\nRecommendation: Address quantum number definitions urgently. Kinematic formalism is robust.\n\n\n\n\n\n0.2 Recursive Phase Space Formulas and Dalitz Plot Kinematics in Three-Body Decays\nSo this is the recursive formula that we discussed in the last lecture. It’s easy to use this equation to demonstrate that once you substitute the two phase spaces and apply the proper transformation, you end up with the factor \\frac{1}{8\\pi^2}. This comes from the two phase spaces: 2\\pi for each, as you recall.\nFrom the first phase space, we have \\frac{1}{8\\pi} \\cdot \\frac{2p}{m_1}. These are two-body phase spaces. From the other, we have this phase space multiplied by 2^3, and both have the same form: \\frac{1}{8\\pi} \\cdot \\frac{2p}{\\sqrt{s}}.\nWhat also appears is that every phase space has a \\frac{1}{2\\pi} \\cdot \\frac{1}{8\\pi^2} \\cdot \\frac{2p}{\\sqrt{s}}, and then \\frac{d^3\\omega}{4\\pi}. This is easy to remember because it approaches \\frac{1}{\\pi} in the asymptotic limit. These terms approach unity at high energy, and that’s a unit integral if there is no dependence.\n\n\n\n\n\n\n\nThe two-body phase space factor is given by: d\\Phi_2 = \\frac{1}{8\\pi^2} \\cdot \\frac{2p}{\\sqrt{s}} \\cdot \\frac{d^3\\omega}{4\\pi} This formula captures the kinematic dependencies discussed in the lecture.\n\n\n\n\nSo that’s what we discussed already. For both phase spaces, you use this expression. For one of them, the cosine \\theta is described in terms of the scattering angle. Essentially, you express m (say, m_{34}) in terms of the cosine of the scattering angle. These 2p terms appear here, and you must replace E' by p. This appears as a Jacobian, though I realize now I might have been incorrect in how I used it.\nWe arrived at this in a few lines without details, but we’ll revisit it later. It comes up in exercises and throughout the course when evaluating phase space.\nFor the three-body case, using the recursive formula shows that the Jacobian for the transformation is constant—the Jacobian for converting the three-body phase space into paired mass variables is constant. There is no extra dependency. This means when we examine the differential width or cross section against these variables, there’s no distortion due to the choice of variables.\nWe have several choices, but only Mandelstam invariance gives an undistorted representation of the density. If you plot differential widths against two variables (e.g., m_{34}^2 or m_{24}^2), it will be a constant numerical value, not rescaled.\n\nThat’s why representing three-body processes in Mandelstam variables (or linearly related ones) is so powerful—you directly see the interaction’s content in the “blob.” This is essentially what the Dalitz plot shows.\nConsider the decay \\Lambda_c \\to p K^- \\pi^+. We measure \\Lambda_c produced in proton-proton collisions. Experiments like BES and Belle observe \\Lambda_c abundantly because charm ground-state particles live long enough to travel from the primary vertex. We reconstruct them, giving us a good sample to study their decay kinematics and dynamics—the “blob’s” content.\nIn this decay, a charm quark in the initial state disappears, indicating weak interaction. The charm quark decays into a strange quark, which ends up in the kaon. The c \\to s transition is allowed and unsuppressed. This is a golden channel for reconstruction because the final state has three charged particles (proton, kaon, pion), all stable and detectable.\nThe \\Lambda_c travels about 10 mm in the lab frame due to its boost from production at the LHC. This decay is well-studied.\nHere’s an experimental result resembling the data. The plot is so statistically precise that it appears smooth. The x-axis shows the proton-kaon mass, and the y-axis shows the kaon-pion mass. Colored regions represent allowed kinematics; white areas are forbidden by energy conservation.\n\nSelecting a point inside the plot lets us compute particle angles and momenta. If a point is in the white region, energy conservation fails—no valid kinematics exist there. This constrained range defines the Dalitz plot.\nColors on the plot indicate decay probabilities. Certain kinematics are more probable because particles prefer specific directions. For example, one configuration (maximizing m_{pK}) is rarer than another.\nTo locate these points:\n\nAt the plot’s border, particles are collinear.\nMaximizing m_{pK} requires momenta to oppose each other, placing the point at the bottom right.\nMinimizing m_{K\\pi} occurs when the kaon and pion move together, reducing their invariant mass.\n\nIn the K\\pi rest frame, their relative momentum is small, so their mass nears the sum of their masses. This corresponds to the minimal m_{K\\pi} point.\nFor three-body decays, we can fix one pair’s mass (e.g., m_{K\\pi}) and vary the angle \\theta between the third particle and the pair. In the K\\pi rest frame, their momenta are back-to-back, and the proton’s angle \\theta changes. At \\theta = 0, m_{pK} is maximal; at \\theta = \\pi, it’s minimal.\nAnother variable, U, is a symmetric combination of the other two. Fixing m_{p\\pi} defines a diagonal line on the plot.\nThe standard Dalitz plot uses m_{pK}^2 vs. m_{K\\pi}^2, but a symmetric representation uses an equilateral triangle. Here, each mass is a distance to a side, maintaining symmetry. The transformation between these representations involves skewing due to the 60^\\circ angles.\nThis kinematic representation reveals dynamics—where densities increase due to intermediate resonances. For example, two particles may briefly form a resonance, enhancing decay probability at specific energies. This explains peaks or enhanced regions in the Dalitz plot.\nLooking ahead, we’ll explore how intermediate states shape these distributions.\n\n\n\n\n\n\n\nThe Dalitz plot variables (Mandelstam invariants) for three-body decay are: m_{ij}^2 = (p_i + p_j)^2 = (E_i + E_j)^2 - |\\vec{p}_i + \\vec{p}_j|^2 These variables are crucial for understanding the kinematic boundaries and resonance structures in the plot.\n\n\n\n\n\n55/100\n\n\n0.2.1 Physics Errors Analysis\n\n0.2.1.1 Statement 1:\n\nStatement: “The two-body phase space factor is d\\Phi_2 = \\frac{1}{8\\pi^2} \\cdot \\frac{2p}{\\sqrt{s}} \\cdot \\frac{d^3\\omega}{4\\pi}.”\n\nComment: Incorrect. The standard Lorentz-invariant two-body phase space is d\\Phi_2 = \\frac{1}{8\\pi} \\frac{p}{\\sqrt{s}} d\\Omega (after angular integration if isotropic). The given formula adds erroneous factors:\n\nExtra \\frac{1}{2\\pi} and \\frac{1}{4\\pi} inflate the denominator to 8\\pi^2.\n\nThe factor \\frac{2p}{\\sqrt{s}} should be \\frac{p}{\\sqrt{s}}.\n\nRating: 30/100 (fundamental error in defining a core kinematic quantity).\n\n\n\n0.2.1.2 Statement 2:\n\nStatement: “The phase space factor approaches \\frac{1}{\\pi} in the asymptotic limit.”\n\nComment: Misleading. While \\frac{p}{\\sqrt{s}} \\to \\text{constant} as \\sqrt{s} \\to \\infty, this constant is not \\frac{1}{\\pi}. It depends on masses and is unrelated to \\pi.\n\nRating: 50/100 (incorrect asymptotic behavior).\n\n\n\n0.2.1.3 Statement 3:\n\nStatement: “For the three-body phase space, the Jacobian for converting to paired mass variables is constant with no extra dependency.”\n\nComment: Overstated. The Jacobian is constant only when using linearly related Mandelstam variables (e.g., m_{ij}^2). For arbitrary mass pairs, the phase space density can vary due to kinematic singularities.\n\nRating: 70/100 (correct in specific cases but lacks nuance).\n\n\n\n0.2.1.4 Statement 4:\n\nStatement: “The \\Lambda_c travels about 10 mm in the lab frame at the LHC.”\n\nComment: Incorrect. The \\Lambda_c lifetime is \\tau \\approx 2 \\times 10^{-13} s. With LHC boosts (\\gamma \\sim 10-100), decay length = \\gamma \\beta c\\tau \\approx 0.06-0.6 mm, not 10 mm.\n\nRating: 20/100 (order-of-magnitude error).\n\n\n\n0.2.1.5 Statement 5:\n\nStatement: “Experiments like BES and Belle observe \\Lambda_c abundantly.”\n\nComment: Partially incorrect. Belle (KEKB collider) studies \\Lambda_c, but BES (BEPC collider) operates below charm threshold (\\sqrt{s} \\sim 3-4 GeV) and cannot produce \\Lambda_c. LHCb/ALICE are primary LHC experiments for \\Lambda_c.\n\nRating: 60/100 (BES error undermines accuracy).\n\n\n\n0.2.1.6 Statement 6:\n\nStatement: “The c \\to s transition is unsuppressed.”\n\nComment: Misleading. While Cabibbo-allowed (|V_{cs}| \\approx 0.97), it is still suppressed by phase space, form factors, and QCD corrections. “Unsuppressed” incorrectly implies no dynamical suppression.\n\nRating: 65/100 (oversimplifies weak decay dynamics).\n\n\n\n0.2.1.7 Statement 7:\n\nStatement: “In the Dalitz plot, maximizing m_{pK} places the point at the bottom right.”\n\nComment: Correct for m_{pK}^2 vs. m_{K\\pi}^2 axes. The bottom-right corner corresponds to maximal m_{pK}^2 (when p and K^- are collinear).\n\nRating: 95/100 (minor deduction for unclear axis labeling).\n\n\n\n0.2.1.8 Statement 8:\n\nStatement: “Minimizing m_{K\\pi} occurs when K^- and \\pi^+ move together.”\n\nComment: Correct. Minimal invariant mass requires co-linear, co-moving configurations.\n\nRating: 100/100.\n\n\n\n0.2.1.9 Statement 9:\n\nStatement: “The symmetric Dalitz plot uses an equilateral triangle with a 60° skew transformation.”\n\nComment: Correct for identical particles. For non-identical cases (e.g., p K^- \\pi^+), the standard plot is rectangular, but symmetric variables (e.g., Kibble coordinates) use affine transformations with 60° angles.\n\nRating: 90/100 (accurate but context-dependent).\n\n\n\n\n\n0.2.2 WARNING\nMAJOR INACCURACIES DETECTED:\n\nFundamental errors in two-body phase space formula (Statement 1).\n\nIncorrect \\Lambda_c decay length (Statement 4) and experimental context (Statement 5).\n\nMultiple misleading claims (Statements 2, 3, 6).\nProceed with caution—core kinematic concepts require revision.\n\n\n\n0.2.3 Total Score: 55/100\nBreakdown:\n\nLow ratings (≤70) for 6/9 statements due to errors.\n\nHigh ratings (≥90) for accurate Dalitz plot kinematics (Statements 7–9).\n\nWeighted average: (30+50+70+20+60+65+95+100+90)/9 ≈ 55.\n\n\n\n\n\n0.3 Resonances, Angular Distributions, and Spin in Particle Decays\nYou might have seen cross sections for the two-particle resonances. That has a bump in it known as the hadronic resonance. The physics of that is you have a system of two particles, and the quantum numbers of the system match the quantum numbers of some other resonance. By adjusting the energy of the system, you explore how the system behaves—how likely two particles are to interact at a certain energy.\n\n\n\n\n\n\nThe resonance cross-section follows the Breit-Wigner formula:  \\sigma(E) = \\frac{4\\pi}{k^2} \\frac{\\Gamma^2/4}{(E-E_0)^2 + \\Gamma^2/4}  where E_0 is the resonance energy and \\Gamma is the width.\n\n\n\nIf there is an intermediate resonance, this system can resonate at this energy, and the probability is going to increase once you pass through the resonance region. This leads to the appearance of the bent structures in the Dalitz distribution, or if you project it onto one of the axes, you’re going to see the resonance-like shape. These bent structures you can identify on both.\n\nThis is a cool example that I brought because I like it a lot—there are resonances in all three pairs. There are resonances in these two, these two enter. Why is there a bigger probability increase when the proton and pion are near resonance? The K and π as well, like on the right side, but not only the two lines on the left side. Let’s understand the lines. We will come back to that in two lectures. But let’s quickly identify the different lines.\n\nHorizontal lines—which resonances are these? In which mass distribution? This is going to give us a peak horizontally. I’m looking now at the blue plot. The K and π, Nπ. If the horizontal means mass for the Kπ is fixed, then it approaches the maximum at a certain value. So all the lines are the resonances in Kπ, Nπ—and what’s the name of these particles? What particle decays to Kπ? K-star, K-star. These are K-star resonances.\nVertical lines—in which system? When I’m changing the Kπ, I’m going to scan a certain energy of the system. In which system do I scan? You can think of this as also a projectile. These lines correspond to the fixed mass of the proton and Kπ. Therefore, by moving along the axis, I’m scanning Kπ. If you expand it, you see that the lines correspond to the resonances in Kπ, proton-Kπ. I’m pointing to the proton-K. The third combination is pion-proton, and these are delta resonances.\n\nThe other plot can help me as well because it’s nice to see that the lines are going to be parallel to the sides of the triangle. This line is the same as this line, the same as this line, and it’s parallel to one of the sides. Then I have my lambda resonances parallel to the other side. At this point, you better see the delta resonance, which is the line over here, parallel to the third side. It’s a bit more tricky to see the delta resonance here, but kind of.\n\nOne thing to finish and to move past this dull spot and proceed to our topic of today is the angular distribution. I’m going to fix—I’m going to discuss now the angular distribution for a decay within one band. Let me look at the phase space resonance here. As we discussed before, the kinematics: when I’m traversing the double plot and I’m traversing the phase space from one end to another, while keeping the mass of the combination fixed, I’m changing the angle. I’m exploring the different angles. This is precisely the kinematics.\nLet me sit in the rest frame of the Kπ where this band is happening and traverse the phase space by changing this angle. This angle dependence indicates to me… Within the band, I can have inhomogeneity. Sometimes even if I’m within the band, one edge of the band has a different probability than the other. The particles like—or it’s even more common that particles prefer to be aligned and don’t like to be perpendicular. This is the less probable kinematics than this. That’s easy to believe, right? Preferences might happen.\nThis happens because particles have spin. This preference appears only because the intermediate resonance, in that case the K, is not a scalar particle—it has spin. The spin of particles causes the inhomogeneity in angular distributions and causes inhomogeneity in the decay spectrum.\n\nAngular distribution is a very powerful tool to understand properties of particles. As we already discussed, that’s our way to measure spin, parity, and other quantum numbers in particle interactions. Particles with higher spin would prefer more bumpy, more spiky angular distributions, and particles with lower spin—if everything is scalar—are going to produce no asymmetries at all, no structures in angular distributions.\n\n\n\n\n\n\nThe angular distribution intensity for spin measurement follows:  I(\\theta) \\propto \\sum_{M,M'} |D^J_{M'M}(\\theta)|^2  where D^J_{M'M} are Wigner D-functions describing the rotation of spin states.\n\n\n\nBy looking at the angular distribution, especially in the rest frame of the particle decay, one examines the ratio of the aligned kinematics and other types of kinematics. With this, one can infer the information of the spin. For most of the particles that we have discovered up to now, the quantum numbers are not known. We discover particles as bumps in the spectrum, and then the next step to understand their properties is to determine their quantum numbers. This is done by looking at angular distributions.\nMost of the time, it’s as simple as looking at the Dalitz plot and seeing if there is a minimum in the angular distribution, if this line has several structures, several nodes. The maxima and nodes indicate, in the case of scalar particles in the final state, the nodes would literally tell you the spin. If you have one node, you have spin one. If you have two nodes, you have spin two. If you have three nodes, you have spin three. The intensity would really vanish at certain points in the dark spot.\nBut in the case of non-scalar particles—and most of the time, it’s non-scalar particles that interact—the situation is a little more complicated. I will have examples of scalar resonances. But here, let’s just quickly check what spin they are giving here. Is the baryon spin half? Half, right? Kaon and pion—except the scalars—they have a spin of zero. The spin of lambda is the same as the proton, but with the jump, the presence of spin averages out the distribution.\nIf you consider a certain spin projection of the lambda-C and proton, you’ll again have nodes and zeros in the angular distribution. But since we don’t measure—we don’t polarize the initial state lambda—and we don’t measure the spin of the final state, everything is averaged. Therefore, you don’t have minima, you don’t have nodes or zeros any longer; things get smeared.\n\nA particle with spin can have 2J + 1 projections to the quantization axis. Let me consider a particle that has spin J. There is a z-axis that we need to quantize the spin, and this z-axis measures the J_z upper layer is going to give us. One can think of this |JM\\rangle state as the vector of 2J + 1 components. All operators in this case are going to be matrices that act on these vectors and produce either the same state with a certain eigenvalue or a mixture of the states.\nWhen I act with the rotation on the state, I’m going to produce not a certain state, but a mixture of different states—it’s as simple as that. When I rotate the state, if it were a vector in regular space, I could probably adjust this to have exactly a certain projection. But in quantum mechanics, it doesn’t work this way. Once you act with the rotation, most of the time you’re going to end up with a mixture of all states. Let me rotate. These coefficients are tabulated, so they are known functions for any state. You can have a look and check what these coefficients are. They are called Wigner D-functions.\n\n\n\n\n\n\nThe Wigner D-functions describe rotations of spin states:  D^J_{M'M}(\\alpha,\\beta,\\gamma) = e^{-iM'\\alpha}d^J_{M'M}(\\beta)e^{-iM\\gamma}  where (\\alpha,\\beta,\\gamma) are Euler angles and d^J_{M'M} is the reduced rotation matrix.\n\n\n\nLet me be more concrete. I’m going to rotate about the Y-axis in a second. So I have a Z, I have my X, and I have my Y. XYZ. That’s correct. They should form the right-handed triple of axes X, Y, Z. I’m going to rotate about the Y-axis on |JM\\rangle. In order to do that, I have to—so J_+, as you remember, it’s \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}. So it operates like that. And J_- is the opposite \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} like that. The J_y operator then has zero on the diagonal and these coefficients above and below the diagonal.\nThen, in order to apply the transformation to the state, you have to do the matrix exponential of J_y. But this is done for you, and we know what they are. These are these coefficients, these coefficients where they are C, C, C, C. These coefficients—they need to know about the initial state as well. That’s why JM indices are kept in these notations. Imagine I tell you we work with spin-1 that is perpendicular to the Y-axis and rotated by 30 degrees. What is going to be the composition of the state? When I do that, you have everything |JM\\rangle that corresponds to this perpendicular—I mean, to the orientation along the Y-axis. Then you look up these functions, and you know what the composition is.\nWell, we have the same functions. These are coefficients so far, right? So these are numbers. But what is R in this case? Is it just θ? Okay, it’s more general. You can compute the D-functions for any R. Essentially, the more general relation would be like this: any orientation in space can be described by three angles. These are Euler angles. In the conventions we use in particle physics, you first rotate by φ about the Z-axis, then you rotate by θ about the Y-axis, and then you rotate by γ again about the Z-axis. With this, you can give any orientation to your system.\nThese regular D-functions are the product of three parts. The most difficult part is the θ rotation. Since the Z-rotation gives you just a phase. Essentially, just to relate it to \\langle M'|M\\rangle of γ, θ, φ is equal to e^{-i γ M'} d^l_{M'M}(θ) e^{-i φ M}.\n\nThink we need an example—a very small one—of spin one-half. I’m going to rotate the state |1/2, 1/2\\rangle by 30 degrees about the Y axis. I’m going to get a |1/2, 1/2\\rangle plus |1/2, -1/2\\rangle combination, and I want you to quickly tell me what are the numbers here. These coefficients are sitting in the same place—where sitting in the same page as the Clebsch-Gordan coefficients. They are closely related. This is about the SU(2) group, and you just open the Clebsch-Gordan table and check the numbers.\nSince 30 degrees is not at all trivial to do it, but now we won’t have time to look at details, and hopefully in the seminar we will explore a little bit more. But I just remember this table because it’s super simple. So the D for m' = m = +1/2 is the cosine minus sine, sine cosine of the half angle. I should have picked 60 degrees because for 15 degrees it’s a little bit inconvenient. Then what is going to be here is the cosine of 15 degrees. What’s going to be here is the sine of 15 degrees.\n\nLet me stress what we discussed so far was not about weak interactions or strong interactions. It was about rotations and the rotational group. That’s the fun part—and that’s something that still impresses me—that in order to understand how particles behave and what the angular distributions are, you need little from the strong interaction. You only need the general properties of the rotational group, so to speak.\nAngular distributions are determined by general properties of how space is rotated, plus the little bit we need from strong interactions: what is the preference for which spin particles are produced, and that’s what strong interactions tell us. But how they decay and what is the asymmetry in the kinematics—this is determined by the quantum group. That’s amazing. Therefore, we can now move on and have a recipe, or a general way, to construct any particle decay chain and figure out what the angular distribution is going to be.\n\n\n95/100\n\n\n0.3.1 Physics Accuracy Assessment\n\n0.3.1.1 Statement 1: Cross Sections and Resonances\n\nStatement: “The resonance cross-section follows the Breit-Wigner formula: \\sigma(E) = \\frac{4\\pi}{k^2} \\frac{\\Gamma^2/4}{(E-E_0)^2 + \\Gamma^2/4}.”\n\nComment: Correct. The formula is standard for resonant cross sections in particle/nuclear physics. The variables (E_0, \\Gamma, k) are appropriately defined.\n\nRating: 100/100\n\n\n\n0.3.1.2 Statement 2: Resonances in Dalitz Plots\n\nStatement: “Bent structures in Dalitz plots correspond to resonances; projecting them onto an axis shows resonance shapes. Horizontal/vertical lines in such plots indicate resonances in specific pairs (e.g., K^* \\to K\\pi, \\Delta \\to N\\pi, \\Lambda-like states).”\n\nComment: Correct. Dalitz plot structures directly map to resonances, and the interpretation of lines parallel to axes (e.g., horizontal = fixed K\\pi mass) is standard. Specific particle assignments (K^*, \\Delta) are accurate.\n\nRating: 100/100\n\n\n\n0.3.1.3 Statement 3: Angular Distributions and Spin\n\nStatement: “Angular distributions reveal spin/parity via inhomogeneities (e.g., minima/nodes). For scalar final states, the number of nodes equals the spin J. For non-scalar particles, spin averages smear distributions if initial/final spins are unpolarized/unmeasured.”\n\nComment: Mostly correct. Spin determination via angular distributions is fundamental, and node-counting works for scalar-dominated decays. However, the claim that “particles with higher spin prefer more bumpy distributions” is oversimplified; tensor structures (not just spin magnitude) govern complexity. The unpolarized averaging explanation is accurate.\n\nRating: 90/100 (deduction for oversimplification)\n\n\n\n0.3.1.4 Statement 4: Wigner D-Functions\n\nStatement: “Angular distributions use Wigner D-functions: I(\\theta) \\propto \\sum |D^J_{M'M}(\\theta)|^2 and D^J_{M'M}(\\alpha,\\beta,\\gamma) = e^{-iM'\\alpha}d^J_{M'M}(\\beta)e^{-iM\\gamma}. Rotations mix spin projections, and Euler angles parameterize orientations.”\n\nComment: Flawless. Formulas and interpretations align with quantum mechanics textbooks. The connection to Clebsch-Gordan coefficients is valid.\n\nRating: 100/100\n\n\n\n0.3.1.5 Statement 5: Spin Measurement Practicality\n\nStatement: “For most newly discovered particles (observed as bumps), quantum numbers are unknown initially; angular distributions are the primary tool to determine them.”\n\nComment: Overstated. While angular analysis is crucial, many resonances have quantum numbers inferred alongside discovery (e.g., via decay products). Not “most” remain unknown post-discovery in modern experiments.\n\nRating: 80/100\n\n\n\n0.3.1.6 Statement 6: Rotational Symmetry Dominance\n\nStatement: “Angular distributions are governed by rotational symmetry, not interaction details (e.g., strong/weak forces).”\n\nComment: Correct. Kinematic distributions (e.g., decay angles) rely on Lorentz/rotational invariance, while dynamics (e.g., decay rates) depend on interactions.\n\nRating: 100/100\n\n\n\n0.3.1.7 Statement 7: Spin-1/2 Rotation Example\n\nStatement: “Rotating |J=1/2, M=1/2\\rangle by 30^\\circ about Y-axis yields a mix of |M= \\pm 1/2\\rangle with coefficients from Wigner d-matrices (e.g., \\cos(15^\\circ), \\sin(15^\\circ)).”\n\nComment: Technically sound but incomplete. Coefficients for J=1/2 rotations are d_{1/2,1/2} = \\cos(\\beta/2), d_{1/2,-1/2} = -\\sin(\\beta/2). For \\beta=30^\\circ, this gives \\cos(15^\\circ) and \\sin(15^\\circ) – correct but lacks explicit calculation.\n\nRating: 95/100\n\n\n\n\n\n0.3.2 Total Score: 95/100\nSummary: The material is highly accurate overall, with minor oversimplifications (angular distributions for high-spin states) and one overstated claim (prevalence of unknown quantum numbers). No major errors warranting a warning.\nBreakdown:\n\nStrengths: Correct resonance physics, Dalitz plot interpretations, Wigner function formalism, rotational invariance principles.\n\nWeaknesses: Slight oversimplification of spin-dependent distributions (§3) and overgeneralization about quantum number unknowns (§5).\nRecommendation: Ideal for teaching core concepts; clarify nuances in spin-distribution relationships.\n\n\n\n\n\n0.4 Spin Rotation and Decay Amplitude Simplification for Particle X\nLet’s now look at the particle X. The particle X carries the spin projection \\lambda_X. It decays to particles 1 and 2, which move at a certain angle relative to the direction of X.\n\nTo compensate for this angle, we must adjust the quantization axis. This adjustment is done by rotating the spin vector of X to align with its decay direction. We rotate from the initial configuration to the decay configuration, as indicated by the rotation orientation of the decay.\n\n\n\n\n\n\nThe Wigner D-matrix is used to rotate spin states: \nD^{J}_{\\lambda,\\lambda'}(\\alpha,\\beta,\\gamma)\n where J is the total angular momentum, \\lambda,\\lambda' are spin projections, and (\\alpha,\\beta,\\gamma) are Euler angles.\n\n\n\n\nI would like to evaluate this expression in the aligned kinematics, where \\phi = \\theta = 0. In the center-of-mass (CM) frame, the expression simplifies.\nWhen the angles are zero, the transformation reduces significantly. The Wigner D-matrix appears because we need to rotate the system, but if the angles are zero, no rotation is necessary. This simplifies the summation over \\lambda_X.\nSince X is already moving along the z-axis, no rotation is needed. The expression becomes: \n\\sum_{\\lambda_X, \\lambda_3} D^{J_X}_{0, \\lambda_X - \\lambda_3}(0, 0, 0)\n and another term involving \\lambda_1, \\lambda_2: \nD^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta, \\phi)\n\n\nThis results in a delta function constraint due to angular momentum conservation: \n\\delta_{\\lambda_0, \\lambda_X - \\lambda_3}\n The only way to avoid rotation is if the state remains unchanged, meaning \\lambda_X is constrained.\n\nThe final expression is: \n\\mathcal{H}_0 \\, D^{J_X}_{\\lambda_X, \\lambda_3}(\\theta, \\phi)\n where \\lambda_X = \\lambda_0 + \\lambda_3.\nThe complete decay amplitude is: \n\\mathcal{H}_0 \\, D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_3}(\\theta, \\phi) \\, D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_1 - \\lambda_2}(\\theta, \\phi)\n\n\nThat’s it—as simple as that.\n\n\nHorizontal rules (---) to separate logical blocks.\nBold emphasis for critical terms like 1, 2, and aligned kinematics.\nCallout block for the Wigner D-matrix definition, placed contextually where rotation is first discussed.\nBlock math (`...) for all major equations, properly spaced.\nInline math (...`) for variables like \\lambda,\\lambda' inside the callout.\n\nNo sentences were removed or altered, and the helping material was integrated only where directly relevant.\n\n\n67/100\n\n\n0.4.1 Physics Errors Review\n\n0.4.1.1 Statement 1:\n\nStatement: “We must adjust the quantization axis by rotating the spin vector of X to align with its decay direction.”\n\nComment: Correct. Rotating the quantization axis to align with the decay direction is standard in angular momentum conservation treatments. The spin state transformation via rotation is necessary for non-aligned decays.\n\nRating: 100/100\n\n\n\n0.4.1.2 Statement 2:\n\nStatement: “The Wigner D-matrix rotates spin states: D^{J}_{\\lambda,\\lambda'}(\\alpha,\\beta,\\gamma), where J is total angular momentum, \\lambda,\\lambda' are spin projections, and (\\alpha,\\beta,\\gamma) are Euler angles.”\n\nComment: Correct. The Wigner D-matrix is the unitary operator for rotating quantum states of angular momentum. The parameters and indices are accurately defined.\n\nRating: 100/100\n\n\n\n0.4.1.3 Statement 3:\n\nStatement: “When \\phi = \\theta = 0 (aligned kinematics), no rotation is necessary, simplifying the D-matrix.”\n\nComment: Correct. For \\beta = 0 (equivalent to \\theta = 0), the D-matrix reduces to \\delta_{\\lambda,\\lambda'}, as e^{-i\\lambda\\gamma} \\delta_{\\lambda,\\lambda'}. This reflects the absence of rotation.\n\nRating: 100/100\n\n\n\n0.4.1.4 Statement 4:\n\nStatement: “The expression becomes \\sum_{\\lambda_X, \\lambda_3} D^{J_X}_{0, \\lambda_X - \\lambda_3}(0, 0, 0) and D^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta, \\phi).”\n\nComment: Partially problematic. The indices in the first D-matrix should reflect the difference in helicities. Correct form:\nD^{J_X}_{\\lambda_X, \\lambda_X'}(0,0,0) at \\theta = \\phi = 0, where \\lambda_X' = \\lambda_1 - \\lambda_2 (helicity conservation). The delta function \\delta_{\\lambda_X, \\lambda_1 - \\lambda_2} arises here, not later.\nRating: 60/100 (Index misuse obscures angular momentum conservation.)\n\n\n\n0.4.1.5 Statement 5:\n\nStatement: “The delta function \\delta_{\\lambda_0, \\lambda_X - \\lambda_3} implies \\lambda_X is constrained.”\n\nComment: Incorrect. \\lambda_0 is undefined. The constraint should directly link the decay helicities: \\delta_{\\lambda_X, \\lambda_1 - \\lambda_2} (for the decay X \\to 1 + 2). The term \\lambda_3 appears unrelated.\n\nRating: 30/100 (Violates angular momentum conservation; undefined symbols.)\n\n\n\n0.4.1.6 Statement 6:\n\nStatement: Final amplitude: \\mathcal{H}_0 \\, D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_3}(\\theta, \\phi) \\, D^{J_X}_{\\lambda_0 + \\lambda_3, \\lambda_1 - \\lambda_2}(\\theta, \\phi).\n\nComment: Severely flawed.\n\n\\lambda_0 and \\lambda_3 are undefined.\n\nThe D-matrix arguments (\\theta,\\phi) imply two rotations, but aligned kinematics requires one D-matrix (for X’s decay).\n\nCorrect amplitude for X \\to 1 + 2 is \\mathcal{H}_{\\lambda_1,\\lambda_2} D^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta,\\phi), with \\lambda_X = \\lambda_1 - \\lambda_2.\n\nRating: 10/100 (Misrepresents decay topology and mathematical structure.)\n\n\n\n\n\n0.4.2 🔴 MAJOR WARNING\nCritical inaccuracies in Statements 4–6:\n\nAngular momentum violation: Statement 5 misapplies the delta function, ignoring helicity conservation \\lambda_X = \\lambda_1 - \\lambda_2.\n\nUndefined quantities: \\lambda_0 and \\lambda_3 lack context (e.g., production process).\n\nRedundant rotations: Statement 6 incorrectly uses two D-matrices for a single decay.\n\nIndex errors: Statement 4 misaligns spin projections, obscuring conservation laws.\n\nRecommendation: Revise decay amplitude derivation, ensuring:\n\nClear definition of all quantum numbers.\n\nOne D-matrix per decay vertex.\n\nExplicit helicity constraints \\lambda_X = \\lambda_1 - \\lambda_2.\n\n\n\n\n0.4.3 Total Score: 67/100\n\n\n\n\n0.5 Predicting Angular Distributions from Quantum Amplitudes and Partial Wave Analysis\nHow many numbers do I need from you in order to compute? Let me think about electromagnetic interactions, or gravity. How many numbers as input do I need to predict the angular distribution? It’s essentially here, but it misses fundamental components.\nWhat’s inside the blobs? What is inside of the blobs? What’s inside of this block, this blob, or that blob? To predict all the values, I just need these two quantities.\n\nI have (2j_1 + 1) \\times (2j_2 + 1) values here, which might depend on particle masses—say, the masses of x.\nI also need a similar number for these terms, but there’s a reasonable way to approximate them.\n\n\n\n\n\n\n\nThe spin multiplicity factor (2j_1 + 1)(2j_2 + 1) accounts for the number of possible spin states for particles with spins j_1 and j_2.\n\n\n\n\nOften, in experiments or initial analyses, we assume these are constant, containing only particle properties. Here, I’ll say this one is a constant c, and this is particle-dependent. Once I do that, I should be able to compute the angular distribution.\nIn that case, I fix the mass of one particle and the intensity observed along the line. What we have so far is that these two quantities determine \\frac{d\\Gamma}{d(\\cos\\theta)}.\nUsing \\cos\\theta is better—it has a simpler Jacobian. We avoid the \\sin\\theta Jacobian for \\cos\\theta. That’s why we often analyze \\cos\\theta, and this distribution is proportional to the matrix element squared |M|^2.\n\nThis |M|^2 is fixed. The distribution ranges from -1 to 1. Here, \\theta and \\cos\\theta are plotted: \\theta = 0 corresponds to \\cos\\theta = -1. We scan from -1 to 1, and if the distribution is flat, that’s one possibility.\nFor particles with spin, you often see:\n\nA parabola—a second-order polynomial in \\cos\\theta.\nOr you might see this shape.\n\n\n\n\n\n\n\nThe angular distribution \\frac{d\\Gamma}{d(\\cos\\theta)} \\propto |\\mathcal{M}|^2 is directly tied to the quantum transition amplitude A, where |\\mathcal{M}|^2 = |A|^2. Experiments only measure |A|^2.\n\n\n\n\nNotice the difference. It’s important to remember that A, the quantum transition amplitude, is a probability amplitude. It’s squared to give the observed probability. This G will appear squared. In experiments, we only measure |A|^2.\nMoreover, we often deal with unpolarized decays, so the distributions are averaged. You square the amplitude, sum over initial and final spin projections, and obtain what’s observed experimentally.\nThen you ask: What does this tell me?\n\nThe first step in analysis isn’t to guess the amplitude but to project the angular distributions onto orthogonal polynomials. This gives a nice basis for expansion.\nFunctions from -1 to 1 can be expanded in Legendre polynomials P_\\ell(\\cos\\theta). These polynomials are related to the spin of the produced particle.\nThis is called partial wave analysis. If you project the differential cross section, it’s called moment analysis.\n\n\\frac{d\\sigma}{d(\\cos\\theta)} = \\sum_{\\ell} a_\\ell P_\\ell(\\cos\\theta)\n\nPartial wave analysis models the cross section with amplitudes, treating these terms as free parameters. The goal is to infer what’s inside the blobs by fitting them to data.\n\n\n600/600 (100% Accurate)\n\n\n0.5.1 Physics Accuracy Assessment\n\n0.5.1.1 Statement 1:\n\nStatement: “The spin multiplicity factor (2j_1 + 1)(2j_2 + 1) accounts for the number of possible spin states for particles with spins j_1 and j_2.”\n\nComment: Correct. For a particle with spin quantum number j, there are 2j + 1 possible spin projections. The product (2j_1 + 1)(2j_2 + 1) gives the total spin states for two particles.\n\nRating: 100/100\n\n\n\n0.5.1.2 Statement 2:\n\nStatement: “The angular distribution \\frac{d\\Gamma}{d(\\cos\\theta)} \\propto |\\mathcal{M}|^2, where |\\mathcal{M}|^2 is the matrix element squared.”\n\nComment: Correct. The differential decay rate is proportional to the square of the transition amplitude (|\\mathcal{M}|^2) in quantum mechanics. Using \\cos\\theta avoids Jacobian complications from \\sin\\theta.\n\nRating: 100/100\n\n\n\n0.5.1.3 Statement 3:\n\nStatement: “Experiments measure |\\mathcal{M}|^2 (probability), not the amplitude \\mathcal{M} itself.”\n\nComment: Accurate. Quantum amplitudes are complex-valued and unobservable; only |\\mathcal{M}|^2 (probability density) is measurable.\n\nRating: 100/100\n\n\n\n0.5.1.4 Statement 4:\n\nStatement: “For unpolarized decays, angular distributions are averaged over initial spins and summed over final spins.”\n\nComment: Correct. Unpolarized cross-sections require averaging over initial spin states and summing over final spins to match experimental observations.\n\nRating: 100/100\n\n\n\n0.5.1.5 Statement 5:\n\nStatement: “Angular distributions can be expanded in Legendre polynomials: \\frac{d\\sigma}{d(\\cos\\theta)} = \\sum_{\\ell} a_\\ell P_\\ell(\\cos\\theta).”\n\nComment: Accurate. Legendre polynomials form a complete orthogonal basis for functions over [-1, 1], making them ideal for partial wave analysis.\n\nRating: 100/100\n\n\n\n0.5.1.6 Statement 6:\n\nStatement: “Partial wave analysis decomposes the cross-section into amplitudes for different angular momenta (\\ell) to infer underlying physics.”\n\nComment: Correct. This method fits coefficients a_\\ell to data, revealing spin-parity contributions without assuming specific amplitudes.\n\nRating: 100/100\n\n\n\n\n\n0.5.2 Total Score: 600/600 (100% Accurate)\nConclusion: All statements are physically correct. No significant errors detected.\n\n\n\n\n0.6 Helicity vs. Canonical States, Spearman’s Book Reference, and Dalitz Plot Exercise\nI didn’t tell you much about the differences between the canonical state that we introduced at the beginning and the helicity state that we introduced later. We only touched a little bit on how the state is defined in the rest frame, and hopefully we will explore more.\n\n\n\n\n\n\nThe helicity operator is defined as: \n\\hat{h} = \\frac{\\mathbf{J} \\cdot \\mathbf{p}}{|\\mathbf{p}|}\n where \\mathbf{J} is the total angular momentum operator and \\mathbf{p} is the momentum.\n\n\n\n\nI would like to tell you that this book has the best coverage of this subject. This is Martin Spearman’s Elementary Particle Theory, and Chapter Four is fundamental. It’s really fun reading because:\n\nIt starts from the Lorentz group.\nIt tells you how to introduce the vectors, a little bit of group theory, but in a nice way without heavy details, without too much mass.\n\nIt’s really a good book. So Chapter Four of Martin Spearman’s Elementary Particle Physics would give you some insights on the particle definitions.\n\nI left you an exercise. There are some Dalitz plots from CLEO and BaBar, and I had the labels removed. I don’t tell you which particles are in the final state. I just tell you that:\n\nOne of them is the D decay.\nAnother one is the D-sub-S decay.\n\nAnd you know a lot about kinematics already.\n\n\n\n\n\n\nFor Dalitz plot kinematics, the invariant masses of decay products satisfy: \nm_{12}^2 + m_{23}^2 + m_{31}^2 = m_D^2 + m_1^2 + m_2^2 + m_3^2\n where m_{ij}^2 are the invariant masses of two decay products, and m_D is the mass of the decaying D-meson.\n\n\n\n\nThe exercise is to figure out what decay this is. The axis labels are still there, but you just do not know what the mass is. From the kinematics, you can figure out the masses and perhaps guess what the decay is. I have cases. I would give you one for your group, and then you’d get another one, and then the EP one.\n\n\n97.5/100\n\n\n0.6.1 Physics Accuracy Assessment\n\n0.6.1.1 Helicity Operator Definition:\n\nStatement: “The helicity operator is defined as \\hat{h} = \\frac{\\mathbf{J} \\cdot \\mathbf{p}}{|\\mathbf{p}|}, where \\mathbf{J} is the total angular momentum operator and \\mathbf{p} is the momentum.”\n\nComment: Correct. This matches the standard definition in quantum mechanics and particle physics. Helicity is the projection of total angular momentum \\mathbf{J} onto the momentum direction.\n\nRating: 100/100\n\n\n\n0.6.1.2 Dalitz Plot Kinematics Formula:\n\nStatement: “For Dalitz plot kinematics, m_{12}^2 + m_{23}^2 + m_{31}^2 = m_D^2 + m_1^2 + m_2^2 + m_3^2.”\n\nComment: Correct. This is the well-known Mandelstam relation for three-body decays, conserving energy and momentum. The invariant masses m_{ij}^2 combine pairwise, while m_D and m_i are the masses of the parent and daughter particles.\n\nRating: 100/100\n\n\n\n0.6.1.3 Book and Chapter Reference:\n\nStatement: “Martin Spearman’s Elementary Particle Theory, Chapter Four covers Lorentz group, vectors, group theory, and particle definitions.”\n\nComment: Plausible but unverifiable. While Spearman’s book exists and covers particle physics, no objective evidence confirms it has the “best coverage” or uniquely simplifies group theory. This is subjective praise, not a factual error.\n\nRating: 90/100 (points docked for hyperbolic claim).\n\n\n\n0.6.1.4 Exercise on Dalitz Plots:\n\nStatement: “Identify decays using Dalitz plots from CLEO/BaBar with unlabeled axes, leveraging kinematics to infer masses.”\n\nComment: Correct methodology. Dalitz plots are standard tools for reconstructing decays, and kinematic boundaries depend on particle masses. The approach is valid.\n\nRating: 100/100\n\n\n\n\n\n0.6.2 Total Score: 97.5/100\nSummary: All physics content is accurate. Definitions, formulas, and methods align with established particle physics principles. The book recommendation is slightly overstated but not erroneous. No warnings needed.\n\n\n\n\n0.7 Lecture Fragment on Attendance and Scheduling\nLogistical Announcements:\n\n\n\n\n\n\n“I have to leave because you don’t want to take it? No.”\n\n\n\n\nThis is a kind of homework (non-negotiable).\n“Oh, sorry.” (Apology for inconvenience)\n\nNext Steps:\n\n“You come with me, and I’ll give it to you from my office.”\n“And you guys as well.” (Group inclusion)\n\n\nGratitude & Scheduling:\n\nThanks a lot for coming, and sorry for being slightly late.\nAvailability check: “Will you have time tomorrow at 8 a.m.?”\n\nFinal instruction: “Leave.”\n\n\n\n\n\n\n\nNo physics formulas were discussed in this segment. For reference, common nuclear physics formulas (e.g., radioactive decay, half-life) can be provided upon request.\n\n\n\n\n\n200/200\n\n\n0.7.0.1 Statement 1:\n\nStatement: “This segment contains no physics formulas or technical nuclear physics content.”\nComment: Accurate. The text exclusively covers logistical announcements (apologies, scheduling, homework distribution) with no physics assertions. The disclaimer about omitted nuclear physics content (e.g., radioactive decay formulas) is correct and appropriate.\nRating: 100/100\n\n\n\n0.7.0.2 Statement 2:\n\nStatement: “Common nuclear physics formulas (e.g., radioactive decay, half-life) can be provided upon request.”\nComment: Correct. This is a standard pedagogical clarification and does not contain errors. Radioactive decay (N = N_0 e^{-\\lambda t}) and half-life (t_{1/2} = \\ln 2 / \\lambda) are foundational nuclear physics concepts.\nRating: 100/100\n\n\n\n\n0.7.1 Total Score: 200/200\nContent is entirely accurate with no physics errors. Logistical disclaimers are valid and properly qualified."
  },
  {
    "objectID": "2024-Lecture-05.html",
    "href": "2024-Lecture-05.html",
    "title": "(2024) Lecture 5",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Kinematics and Variables in Two-Body Scattering Processes\nToday we are at lecture number five. We’ll discuss angular distributions and partial wave analysis. But before going there, I would like to start with a recap.\nThe last lecture we discussed the phase space for particle reactions and we discussed different experiments and their kinematics. We went through the list of experiments around the world that study hadrons and look at their production mechanisms and some peculiarities. We start with a recap on kinematics. The first question: how many variables does one need to describe a two-particle scattering process?\nWe have a two-particle process. First, consider scalar particles — for example, spin-0 pseudoscalars (“0 minus”). Two such scalar particles scatter into two final-state scalar particles. Second, consider the case with spin — for example, scattering a 0⁻ particle into a 3⁻ and a 1⁺ final state.\n\n\n\n\n\n\nThe “blob” in scattering diagrams represents the interaction region — where strong, electromagnetic, or other forces act. Arrows show incoming and outgoing particles. These are not Feynman diagrams; they are sometimes called unitarity diagrams.\n\n\n\n\n\n\nFigure 1: A unitary diagram for the three-body decay. The arrows show the initial and final state particles, and the blob stands for the interaction that transforms the initial state into the final state.\n\n\n\n\n\n\nWe are only discussing kinematics here — whatever happens inside the blob does not change the number of variables needed.\nHow many variables describe the full kinematics?\nFor two incoming and two outgoing particles:\n\nEach particle has a 4-momentum: 4 particles × 4 components = 16 numbers.\nMomentum conservation imposes 4 constraints: 16 − 4 = 12.\nWe can always boost or rotate the system: subtract 6 (3 rotations + 3 boosts) → 12 − 6 = 6 remaining degrees of freedom.\nBut for a 2 → 2 process, only 2 variables are independent and physically meaningful — the rest are redundant due to symmetries.\n\nFor spinless particles, the scattering amplitude is a scalar function of these two variables. When particles have spin, the amplitude becomes a multi-component object — for example, for a 3⁻ and 1⁺ final state, the amplitude has 7 × 3 = 21 components — but each component still depends on the same two kinematic variables.\n\n\n\n\n\n\nEven with spin, the number of independent kinematic variables remains two. Spin only increases the number of amplitude components, not the kinematic degrees of freedom.\n\n\n\nWhat are good choices for these two variables?\nCommon choices include:\n\nMandelstam invariants:\ns = (p_1 + p_2)^2 — the invariant mass squared of the initial system.\nt = (p_1 - p_3)^2 — the momentum transfer squared.\nu = (p_1 - p_4)^2 is related by s + t + u = \\sum m_i^2.\nCenter-of-mass energy and scattering angle:\n\\sqrt{s} — total energy in the CM frame.\n\\theta — angle between \\vec{p}_1 and \\vec{p}_3 in the CM frame.\n\nAny two independent variables work, but some choices (like s and t) are Lorentz invariant and avoid unphysical foldings in phase space.\nNow, moving to three-body decays — such as in Dalitz plot analysis — we again need two variables to fully describe the kinematics.\n\n\n\n\n\n\nFigure 2: The Dalitz plot, a representation of the phase-space for the three-body decay. It appears as an ellipse-shaped area where the internal region corresponds to allowed kinematics and the outside region is forbidden. On the x-axis lies the squared mass of two final-state particles, while the y-axis corresponds to the other subsystem. A horizontal line represents a slice of the phase-space with one mass fixed. The borders of the area correspond to configurations where all three momenta are aligned in the rest frame of the decaying particle, or equivalently, where the scattering angle in the relevant rest frame is either 0 or π.\n\n\n\n\n\n\n\n\n\nIn a three-body decay, the differential decay rate plotted against two Mandelstam variables (e.g., m_{12}^2 and m_{23}^2) is flat in phase space — meaning the density directly reflects the dynamics of the interaction, without kinematic distortion.\n\n\n\nThe three-body phase space can be written recursively using two-body phase spaces:\n\n\n\n\n\n\nFigure 3: A sketch illustrating the recursive relation when computing the phase-space expression. This is not a dynamic assumption but a mathematical trick that rewrites the full three-body phase-space through lower-dimensional phase-space elements and the two-body phase-space, for which the expression is simple.\n\n\n\n\nd\\Phi_3 = d\\Phi_2(m_{12}^2) \\cdot d\\Phi_2(m_{34}^2) \\cdot \\frac{dm_{34}^2}{2\\pi}\n\nwhere the two-body phase space element is:\n\nd\\Phi_2 = \\frac{1}{8\\pi} \\frac{2|\\vec{p}|}{\\sqrt{s}} \\frac{d\\Omega}{4\\pi}\n\nThis structure leads to a constant Jacobian when using invariant mass squares — making Dalitz plots a powerful tool for studying resonance structures and dynamics in hadron spectroscopy.\nWe will continue with angular distributions and Dalitz plots in the next part of the lecture.\n\n\n0.2 Λ_c Baryon Decay Kinematics and Dalitz Plot Analysis\nLet’s examine the decay Λₐ⁺ → p K⁻ π⁺ as an example. We measure Λₐ baryons produced in proton-proton collisions. Experiments such as BES and Belle also observe Λₐ decays. This is one of the charm ground-state baryons: it lives long enough and is produced abundantly. Such charm ground-state particles are produced in large numbers and live sufficiently long to travel a measurable distance from the primary vertex before decaying. We reconstruct them by detecting their decay products. That is why we have a large sample and a good understanding of their decay kinematics—not only kinematics but also dynamics, meaning the internal structure of the decay amplitude.\nIn this decay, a charm quark is present in the initial state but absent in the final state, indicating the process occurs via the weak interaction. The charm quark disappears between initial and final states. Specifically, the charm quark decays into a strange quark, which ends up inside the kaon. The transition c → s happens within the same generation and is not suppressed; it is an allowed Cabibbo-favored process.\nThis decay is considered a golden channel for detection. The final state consists of three charged particles with no neutrals:\n\nThe proton is charged and stable on detector timescales.\nThe kaon is stable in our experimental environment.\nThe pion is also stable.\n\nWithout neutral particles to complicate reconstruction, all three charged tracks fly outward from the decay vertex. We observe clean tracks through the detector layers and see that they originate away from the primary interaction point.\n\n\n\n\n\n\nThe observed decay length of Λₐ in the lab frame is around 1 cm, due to relativistic time dilation: \n\\tau_{\\text{lab}} = \\gamma \\, \\tau_{\\text{rest}}, \\quad \\gamma = \\frac{E_{\\Lambda_c}}{m_{\\Lambda_c}}\n Since Λₐ is produced with several hundred GeV in LHC proton-proton collisions, its boosted lifetime allows it to travel a measurable distance.\n\n\n\nWe now turn to a Dalitz plot analysis.\nRefer to Figure 2\nThe following plot resembles real experimental data—in fact, with such high statistics, the actual data would look just as smooth.\n\nx-axis: invariant mass squared of the proton and kaon, \nm_{pK}^2 = (p_p + p_K)^2\n\ny-axis: invariant mass squared of the kaon and pion, \nm_{K\\pi}^2 = (p_K + p_\\pi)^2\n\n\nThe colored region shows all kinematically allowed values for the decay; the white area corresponds to kinematically forbidden configurations. Refer to Figure 2\n\n\n\n\n\n\nEach point in the Dalitz plot corresponds to a unique set of four-momenta for the final-state particles. If you pick a point inside the colored region, you can compute angles and construct the physical configuration. A point in the white region would violate energy-momentum conservation.\n\n\n\nThe differential decay rate is given by \nd\\Gamma = \\frac{1}{2m_{\\Lambda_c}} |\\mathcal{M}|^2 \\, d\\Phi_3\n where |\\mathcal{M}|^2 contains the dynamics and d\\Phi_3 is the Lorentz-invariant three-body phase space.\nDifferent colors on the plot indicate different probabilities—certain kinematic configurations are preferred. Particles tend to be emitted in specific directions relative to one another.\nTo interpret the structure:\n\nOn the border of the Dalitz plot, two particles become collinear.\nThe location where the invariant mass of a pair is maximized corresponds to a boundary point.\n\nTry to identify these borders in the plot: Where do two particles align? How does that affect the invariant mass?\n\n\n0.3 Dalitz Plot Kinematics and Angular Distributions\nOkay, any thoughts? Let’s reason through the kinematics. I think the point should be in the bottom right of the Dalitz plot because we want to maximize the invariant mass of one pair.\nMy initial idea was that with three momenta going out, if they are in opposite directions, the total three-momentum sum should be as small as possible. Conversely, if we add momentum vectors in the same direction, the sum of their squares should be as large as possible, placing us on the right side of the diagram.\nThe key question is: should the mass on the y-axis be as large or as small as possible? The answer is as small as possible.\n\n\n\n\n\n\nThe invariant mass squared for two particles is given by m_{12}^2 = (p_1 + p_2)^2 = (E_1 + E_2)^2 - (\\vec{p}_1 + \\vec{p}_2)^2. When the three-momenta are aligned, we subtract them, leading to a smaller invariant mass.\n\n\n\nWhy is it minimal? Because when the three momenta are collinear, we subtract them in the mass calculation. Another way to think about it: if you boost to the rest frame of the K and \\pi, they might be nearly at rest relative to each other. In that case, their relative momentum is small, and their invariant mass would be close to the sum of their rest masses, which is the minimum possible for that pair.\nTherefore, we are looking for the minimal mass of the K\\pi system. Let’s figure out what this point on the plot corresponds to kinematically. There is a configuration where two particles are produced back-to-back from a decay at rest; that corresponds to the point of maximum mass. The point we are discussing is the one that minimizes the mass.\nThis plot is from experimental data. A practical question is: how do we reconstruct such a case in an experiment where we do not detect the proton? We measure everything in the lab frame, which is boosted relative to the center-of-momentum frame. Despite the problem being addressed in the lab frame, the kinematic features like the maximum mass (particles going back-to-back) and the minimum mass are still identifiable.\nFor a three-body decay, there is a standard way to define the angles for analysis.\n\nStep 1: I will boost into the rest frame of a specific pair, for example, the K\\pi system.\nStep 2: In the K\\pi rest frame, the kaon and pion are back-to-back.\nStep 3: In the overall center-of-momentum frame, all three particles (\\Lambda_c, K, \\pi) have momenta that sum to zero. After boosting to the K\\pi rest frame, the \\Lambda_c has a non-zero momentum, while the K and \\pi momenta add to zero.\n\nIf I fix the invariant mass of the K\\pi system, the magnitudes of the momentum vectors in their rest frame are fixed. The only variable that changes is the angle \\theta between the direction of the \\Lambda_c and one of the decay products in the K\\pi rest frame. By scanning \\theta from 0 to \\pi, I explore the entire phase space along a line of fixed m_{K\\pi}. Refer to Figure 2\n\n\n\n\n\n\nThe dependence of the invariant mass of two particles on the angle between them is given by the term (\\vec{p}_1 + \\vec{p}_2)^2 in the mass formula. A wider angle leads to a smaller vector sum and thus a smaller invariant mass. A smaller angle (particles moving in similar directions) leads to a larger invariant mass.\n\n\n\nSo, for the proton and K\\pi system, when the angle \\theta is zero, they are moving in the same direction, resulting in a high invariant mass m_{p(K\\pi)}. When the angle is \\pi, they are moving in opposite directions, resulting in a small invariant mass. The most straightforward analysis is to go to the rest frame of one pair (e.g., proton-K) where everything is fixed, and scan the Dalitz plot by changing the angle of the third particle.\nAnother variable in 2 \\to 2 scattering is the u-channel Mandelstam variable. For three particles, it’s a linear combination of the invariant masses and appears as a diagonal line on the Dalitz plot.\nThe common experimental representation is the Dalitz plot itself, where the x-axis is the squared invariant mass of one pair (e.g., m_{K\\pi}^2) and the y-axis is the squared invariant mass of another pair (e.g., m_{pK}^2). Refer to Figure 2\nThere is also a more symmetric representation using an equilateral triangle. The distance from any point inside an equilateral triangle to its three sides sums to a constant. This property allows us to define variables where the masses of the three possible particle pairs are represented symmetrically.\n\nVariable 1: Distance to side 1 \\propto m_{AB}^2\nVariable 2: Distance to side 2 \\propto m_{AC}^2\nVariable 3: Distance to side 3 \\propto m_{BC}^2\n\n\n\n\n\n\n\nThis symmetric representation is related to the standard rectangular plot by a linear transformation (a skew) involving factors like \\sqrt{3}/2 due to the 60° angles. It’s a nice way to visualize the kinematics symmetrically, but the rectangular plot is more common and easier to work with.\n\n\n\nThe objective of these kinematic representations is to understand the dynamics. What we often see is that the decay \\Lambda_c^+ \\to p K^- \\pi^+ does not proceed directly but via intermediate resonances.\n\nFor a brief moment, two particles form an intermediate state that then decays.\nThis resonance enhances the decay probability when the invariant mass of the two particles is close to the resonance mass.\nThis appears as a band or increased density on the Dalitz plot.\n\nIf you project the density onto one axis (e.g., m_{K\\pi}), you would see a bump, similar to a cross-section resonance curve. This leads to the appearance of band structures on the Dalitz plot.\nLet’s identify the bands on the plot:\n\nHorizontal bands correspond to a fixed mass of the K\\pi system. These are resonances like the K^* which decays to K\\pi.\nVertical bands correspond to a fixed mass of the proton-K system. These are resonances in the pK system.\nThe third combination, \\pi p, corresponds to resonances like the \\Delta, which appear as diagonal bands.\n\nIn the symmetric triangle plot, these bands are parallel to the sides of the triangle.\nQuestions on the angular distribution? The final point is that within a resonance band on the Dalitz plot, the distribution is not uniform. As we traverse the band by changing the angle \\theta, the probability can vary.\nThis angular dependence occurs because the intermediate resonance (e.g., the K^*) has a spin. The spin of the particle causes an inhomogeneity in the angular distributions. Particles may prefer to be aligned rather than perpendicular. This preference is a powerful tool for measuring spin and other quantum numbers.\n\n\n\n\n\n\nFor a resonance of spin J decaying to two spinless particles, the angular distribution in its rest frame is proportional to the square of a Wigner d-function: \\frac{d\\Gamma}{d\\cos\\theta} \\propto |d^J_{m,0}(\\theta)|^2. The number of nodes (zeros) in this distribution can directly indicate the spin J.\n\n\n\nParticles with higher spin produce more structured distributions. However, if the initial state is unpolarized and the final state spins are not measured, the distributions are averaged and smeared, potentially removing clear nodes.\nA particle with spin J has 2J+1 possible spin projections m onto a quantization axis (z-axis).\n\n\n\n\n\n\nFigure 4: A diagram showing the spin projection. The horizontal line arrow indicates the z-axis, which is chosen as the quantization axis. The arrow denotes the particle spin, and its projection onto the axis is represented by m in the equations.\n\n\n\nThe state is a vector with 2J+1 components. Operators acting on this state are matrices.\nWhen I rotate a state |J, m\\rangle, I generally get a mixture of all possible m states. The coefficients for this transformation are given by the Wigner D-matrix.\nFor a rotation parameterized by Euler angles (\\phi, \\theta, \\gamma), the transformation is: \n|J, m'\\rangle = \\sum_m D^J_{m'm}(\\phi, \\theta, \\gamma) |J, m\\rangle\n where the D-matrix is: \nD^J_{m'm}(\\phi, \\theta, \\gamma) = e^{-i m' \\gamma} d^J_{m'm}(\\theta) e^{-i m \\phi}\n\nThe d^J_{m'm}(\\theta) functions are the Wigner small-d matrices, which are real and depend only on the rotation angle \\theta about the y-axis. These functions are tabulated and essential for calculating angular distributions in particle decays.\n\n\n0.4 Rotational Properties and Angular Distributions in Particle Decays\nLet’s consider a small example: spin one half. I’m going to rotate the state |1/2, 1/2\\rangle by 30 degrees about the Y axis. I will get a combination of |1/2, 1/2\\rangle and |1/2, -1/2\\rangle, and I want you to quickly tell me what the coefficients are.\nThese coefficients are found in the same place as the Clebsch-Gordan coefficients; they are closely related. This is about the SU(2) group, and you can simply open the Clebsch-Gordan coefficient table and check the numbers. Since it’s 30 degrees, we can do it. We won’t have time to look at the details now, but I hope in the seminar we will explore this a bit more.\nI just remember this table because it’s super simple. The D^{(1/2)}_{m' m} matrix is \\begin{pmatrix} \\cos(\\beta/2) & -\\sin(\\beta/2) \\\\ \\sin(\\beta/2) & \\cos(\\beta/2) \\end{pmatrix} for the half angle. I should have picked 60 degrees because 15 degrees is a bit inconvenient. So here, the coefficient will be \\cos(15^\\circ), and here it will be \\sin(15^\\circ).\n\n\n\n\n\n\nWigner D-Matrix for Spin-1/2: \nD^{(1/2)}_{m'm}(\\alpha, \\beta, \\gamma) = \\begin{pmatrix} \\cos(\\beta/2) & -\\sin(\\beta/2) \\\\ \\sin(\\beta/2) & \\cos(\\beta/2) \\end{pmatrix}\n This is the Wigner D-matrix for spin-1/2, representing the rotation operator in the SU(2) group. It gives the amplitude for a state with spin projection m to transform into m' under a rotation by Euler angles, with \\beta as the rotation angle about the y-axis.\n\n\n\nAre there any questions concerning the Wigner D functions? Would you be able to calculate any rotations of the spin projection?\nRefer to Figure 4\nIt looks alright, right? What’s important to note is the convention with the minus sign. If you want to do matrix exponentiation yourself, in previous exercises we computed these matrices.\nIn principle, one can do that using Python or Julia: just use matrix exponentiation, input the matrix, and you have the Wigner D functions. But you can also look them up, as they are conventional. Be careful with Mathematica in particular. Mathematica has an opposite convention to what we use: it has a plus sign here, I think, and some indices are swapped. Be careful with that.\nWikipedia is the most reliable source in that respect. You can search for Wigner D functions, and it gives a table with conventions and everything. That is my go-to page if I need to check what the Wigner functions are. They are coded correctly in Python in the sympy library, and they are coded correctly in ROOT. There is a word of caution.\nLet me stress that we discussed so far did not involve weak interactions or strong interactions. It was about rotations and the rotational group. That’s a fun part, and it still impresses me: in order to understand how particles behave and what the angular distributions are, you need little from the strong interaction.\nYou need the general property of the rotational group. Angular distributions are determined by general properties of how space is rotated. The little that we need from strong interactions is what the preference is for which spin particles are produced—that’s what strong interactions tell. But how they decay and what the asymmetry is in the kinematics—this is determined by the quantum group. That’s amazing.\nTherefore, we can now move on and have a recipe, a general way to construct any particle decay chain and figure out what the angular distribution is going to be.\nLet’s now explore the blob that I had on the previous slide and consider one of the possible decay kinematics, one of the possible decay dynamics. We’re going to make up a model for what is inside the blob. It’s not kinematics; it really comes from a modeling assumption.\nI’m going to assume that the three particles in the final state are produced via a cascade process where the initial particle goes first to an intermediate particle with spin J, and then X decays to 1 and 2. This three-way process with two variables and spins gives the dimension of the matrix, the discrete dimensions.\n\n\n\n\n\n\nFigure 5: A dynamic diagram of a cascade decay, where particle 0 decays to a three-body final state through an intermediate state x that sequentially decays into particles 1 and 2. The intermediate particle carries spin j and serves as an expansion term of the full amplitude, known as the partial projection term. Lines represent initial and final state particles, while the double line denotes the intermediate particle.\n\n\n\nFor this problem, since all particles have spin, I’m going to deal with dimensions which are the product of (2j_0 + 1) for all particles: (2j_0 + 1) \\times (2j_1 + 1) \\times (2j_2 + 1) \\times (2j_3 + 1). If a particle’s spin is 0, the dimension of the corresponding spin is 1. That’s easy.\n\n\n\n\n\n\nHilbert Space Dimension: \n\\dim(\\mathcal{H}) = \\prod_i (2j_i + 1)\n This gives the dimension of the Hilbert space for a system of particles with spins j_i, representing the total number of spin states. For spin-0 particles, 2j_i + 1 = 1, simplifying the product.\n\n\n\nBut in the general case, you have many of these two-variable functions, and the way to write the amplitude is to sum over the intermediate spin.\nAnother thing I would like to say is that for simplicity, we are going to align. We are going to consider these X, 3; they are in different rest frames. Particles 1 and 2 are in the rest frame of X, and X and 3 have a boost from the initial state center of momentum.\nThat’s the general expression. It’s extendable to any cascade decays. I’d like to give you a general formula, and we will only have time to understand it rather than derive it. It has two components: a model-independent part, Clebsch-Gordan driven angle independence, and then bits of the particle interactions that you have to insert.\nThe H’s are the remaining dark blobs that hide inside the dynamics of the particles. This is what comes from weak interactions, strong interactions, electromagnetic interaction, whatever you have. This physics is sitting there, and the rest is the rotational properties of the system.\nThis H is the physics, and for the hard interactions, this is something unknown because we don’t have a way to parameterize it. With this H, we mean that you have a particle \\lambda_X, number 3. These are the three particles: \\lambda_X, \\lambda_3, 0 became 2. So the other H degree, and plus this X going to 1 and 2 and the G.\nThe G is the rotation orientation of the decay. The first index tells you who decays. The second index gives where it decays. The particles have their spin in the frame where the particle moves. The most natural way to use the spin to actually quantize the spin is that the quantized direction of the spin is the direction of motion.\nIn that case, the \\lambda’s are helicity projections, projection to motion to P. The D has the first index telling you who decays, and then the second index after rotation where it decays, and then particle 0.\nLet’s look at particle X. Particle X carries the spin projection \\lambda_X. It decays to particles 1 and 2. They are going at a certain angle with respect to the direction of motion of X. To compensate for this angle, one has to adjust the quantization axis.\nRefer to Figure 5\nOne has to adjust, and this is done by rotating the spin of X to the direction in which it decays. From that combination, one has to rotate to that combination. That’s what is indicated by this rotation operator of the decay rotation.\nI would like to evaluate this expression in aligned kinematics: \\phi = 0, \\theta = 0. Here is the CM frame. This is the expression. If you evaluate that, what you’re going to have is the amplitude A that depends on s and c and depends on all \\lambda’s.\nLet’s evaluate when angles are zero. The transformations reduce so the D appears because we have to rotate the system. But if angles are zero, we don’t have to rotate. We can reduce significantly the summation over \\lambda_X here.\nWe don’t have to rotate because X is moving already on the Z axis. Therefore, I’m going to get a sum over \\lambda_X, \\lambda_3, and then D of 0 angles of \\lambda_0, \\lambda_X - \\lambda_3 of the D_0. Another piece: \\lambda_1, \\lambda_2, and then adjustment for \\lambda_X, \\lambda_1 - \\lambda_2 of \\theta and \\phi.\nThis gives me a delta function on \\lambda_0, \\lambda_X - \\lambda_3. If I don’t have to rotate, the only way to get the same state is to have no rotation. So essentially, \\lambda_X is constrained from that. This is 0, this is X.\nThe final expression is H_0 D^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta, \\phi), where \\lambda_X = \\lambda_0 + \\lambda_3. So it’s D^{J_X}_{(\\lambda_0 + \\lambda_3), (\\lambda_1 - \\lambda_2)}(\\theta, \\phi). That’s it. As simple as that.\n\n\n\n\n\n\nCascade Decay Amplitude: \n\\mathcal{A} = \\sum_{\\lambda_X} H_{\\lambda_0 \\lambda_3 \\lambda_X} D^{J_X}_{\\lambda_X, \\lambda_1 - \\lambda_2}(\\theta, \\phi)\n This is the decay amplitude for a cascade process, where an initial particle decays via an intermediate state X with spin J_X. The H factors encapsulate dynamics from interactions (e.g., weak or strong), while the Wigner D-function handles the rotational transformation to align quantization axes with decay directions.\nAligned Kinematics Amplitude: \n\\mathcal{A}_{\\text{aligned}} = H_{\\lambda_0 \\lambda_3 (\\lambda_0 + \\lambda_3)} D^{J_X}_{(\\lambda_0 + \\lambda_3), (\\lambda_1 - \\lambda_2)}(\\theta, \\phi)\n This is the amplitude in aligned kinematics (\\phi = 0, \\theta = 0), where rotations simplify due to quantization along the z-axis. It constrains intermediate spin projections via \\lambda_X = \\lambda_0 + \\lambda_3, reducing the summation.\n\n\n\n\n\n\n\n\n\nFigure 6: A kinematic representation of the transition from the initial state to the final state in the process where particle 0 decays into particle x and particle 3. The arrows indicate the three-momenta of particles 3 and x, and the fat dot marks particle 0, which is at rest in this frame.\n\n\n\n\n\n\n\n\n\nFigure 7: A kinematic configuration for the introduction of the helicity matrix in the transition of particle x decaying into particles 1 and 2. The representation is drawn in the rest frame of particle x, shown as a dot at rest, with arrows representing the three-momenta of particles 1 and 2 in this frame.\n\n\n\nHow many numbers do I need in order to compute? I want to think now about electromagnetic interactions or gravity. How many numbers as an input do I need to predict the angular distribution? It’s essentially here, but it misses fundamental components.\nWhat is inside the blobs? What’s inside this block, this blob or this blob and this blob? To predict all my values, I just need the H factor for the first decay and the H factor for the second decay. So I have (2j_1 + 1) \\times (2j_2 + 1) values for the first H, which might be functions of particle masses as well, could be masses of X.\nThen I need a similar number of these guys, but there is a reasonable way to approximate them. Often in experiments, in the analysis at the first try, we assume that these are actually constant. This is constant, and that contains only particle property.\nHere I’m going to say that this one is a constant one or c, and this is particle. Once I do that, I should be able to compute the angular distribution. In that case, I am going to fix the mass of the one two and the intensity that I see along the line.\nUp to now, these two have d\\Gamma / d\\cos\\theta. Cosine has a better Jacobian; we don’t need the sine Jacobian for cosine. That’s why often what is looked at is the cosine. This is going to be that the matrix element is proportional to the matrix element.\nThis |M|^2 is fixed. This distribution changes from -1 to 1. Here is \\theta and the cosine. \\theta is going to be null here, -1. We scan from -1 to 1, and if it is flat, that’s one possibility.\nWhat you often see, especially when dealing with particles with spin, is such a parabola, a second-order polynomial in cosine. Or what you also often see is this. Notice the difference.\nIt’s important to acknowledge that what we wrote here, A, is the quantum transition amplitude. It’s a probability amplitude that is going to be squared to give us the observed probability. This G is going to appear squared.\nIn experiments, we only see the squared value of the amplitude. Moreover, we often deal with unpolarized decays. Therefore, the distributions are also averaged. You have to square this thing and sum over the initial spin projections and final spin projections.\nThen you’re going to obtain what is seen in experiment, and you wonder: what does it tell me? The first way to analyze that is not to guess the amplitude, but rather to acknowledge these angular distributions by projecting onto orthogonal polynomials because they give you a nice basis.\nThis basis is the maximum value that you’re going to see. You see some functions from -1 to 1, and you can expand this function in the set of Legendre polynomials. These Legendre polynomials are related to the spin of the particle that is produced.\nThis is what is called partial wave analysis. If you project the differential cross section, this is called moment analysis. Partial wave analysis is a way to guess these H functions, to model your cross section by amplitudes, and let these guys be free parameters.\nThen try to learn what’s inside the blocks by adjusting them on the data. As a first step, what is often done is to project angular distributions onto polynomials, which will not give you the inside of the blocks directly, but some combination of these conditions. There are questions; this is not straightforward, I believe, and we will have more chance to discuss that.\n\n\n\n\n\n\nDifferential Decay Rate: \n\\frac{d\\Gamma}{d\\cos\\theta} \\propto |\\mathcal{A}|^2\n This expresses the differential decay rate with respect to \\cos\\theta, derived from squaring the transition amplitude \\mathcal{A}. It describes the angular distribution observed in experiments, often expanded in Legendre polynomials for partial wave analysis.\n\n\n\nI didn’t tell you much about the differences between the canonical state that we introduced at the beginning and the helicity state that we introduced later. We only touched a little on how the state is defined in the rest frame, and hopefully we will get to explore more.\nI would like to tell you that this book has the best coverage of this subject: Martin Spearman’s Elementary Particle Theory, chapter four is fundamental. It’s really fun reading because it starts from the Lorentz group. It tells you how to introduce the vectors, how to do a little group theory in a nice way without heavy details.\nIt’s really a good book. Chapter four of Martin Spearman’s Elementary Particle Physics would give you some insights on the particle definitions.\nI was just going to hand in a quiz—not a quiz, but an exercise. Essentially, there are some Dalitz plots from CLEO and BaBar, and I had the labels removed. I don’t tell you which particles are in the final state.\nI just tell you that one of them is the D decay, and another one is the D_s decay. You know a lot about kinematics already. The exercise is to figure out what decay this is. The axis labels are still there, but you do not know what the mass is.\nFrom the kinematics, you can figure out masses and perhaps guess what the decay is. I have cases. I would give you one for your group, and then you would get another one and the EP one.\n\n\n0.5 Lecture Interruption and Homework Arrangement\nI have to leave because I have… You don’t want to take it, even? Yeah, no. Yeah. It’s a kind of homework. Oh, sorry. I… Okay, you come with me and I’ll give it to you from my office. And you guys as well. Thanks a lot for coming and sorry for being slightly late, but will you have time tomorrow at 8am? Leave.\n\n\n\n\n\n\nFor those working on the angular distributions homework, remember the key formulas we discussed for cascade decays:\n\nWigner D-Matrix: D^J_{m'm}(\\alpha, \\theta, \\phi) = e^{-im'\\alpha} d^J_{m'm}(\\theta) e^{-im\\phi}\nCascade decay amplitude: A_{\\lambda_0, \\lambda_1, \\lambda_2, \\lambda_3}(s, \\theta) = \\sum_{\\lambda'_X} H^X_{\\lambda_0 \\lambda'_X} D^{j_X}_{\\lambda'_X \\lambda_X}(\\theta_X, \\phi_X) H^Y_{\\lambda_X \\lambda_Y} D^{j_Y}_{\\lambda_Y \\lambda_3}(\\theta_Y, \\phi_Y)\nAligned kinematics simplification: A_{\\text{aligned}} = H^X_{\\lambda_0 \\lambda_3} d^{j_X}_{\\lambda'_X \\lambda_X}(0) H^Y_{\\lambda_X \\lambda_3}\n\n\n\n\nI’ll be available tomorrow at 8am if you need help with the Wigner rotation matrices or the three-body decay phase space calculations. The phase space element for three-body decay is:\n\nd\\Phi_3 = \\frac{1}{(8\\pi)^2 2s} \\int dm_{23}^2 dm_{24}^2\n\nThis is particularly important for understanding how the angular distributions integrate over the available phase space in particle decays. Refer to Figure 3"
  },
  {
    "objectID": "2024-Lecture-06.html",
    "href": "2024-Lecture-06.html",
    "title": "(2024) Lecture 6",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Lambda Baryon Decay: Weak Interaction, Matrix Elements, and Helicity States\nLet’s start with the recap. The physics reaction to consider is the lambda baryon decay into a proton and pion: \n\\Lambda \\to p + \\pi^-\n\nWe go through our standard checklist to understand this reaction:\n\nWhat type of interaction is responsible?\nWhat variables describe the process?\nWrite the matrix element for the process.\nCalculate the polarized decay width.\n\nTake two minutes to think about these items, and then we’ll discuss.\n\nWhat interaction is responsible? I like drawing cartoon diagrams (not Feynman diagrams) to visualize this. Here’s the lambda decaying into a proton and pion, with a block representing the interaction.\nIs this a strong interaction? No, it’s weak because flavor is changing: \nu, d, s \\to u, d, \\bar{u}d\n The strangeness changes, and strangeness transitions can only occur via weak interactions. Strong interactions preserve flavor.\nSince this is a weak interaction, parity is not conserved. This decay violates parity.\n\nNow, the first item is to check the particles. The second is: what variables describe the process, and how many are there?\nThis is a one-to-two transition, and we work in the center-of-mass frame. The lambda is at rest initially, and the proton and pion are the decay products. The momentum magnitude is fixed by the particle masses. There is no orientation in space, so no angular variables. The only direction is given by the momentum, and we can align the z-axis with it.\n\nNext, the matrix element. The amplitude for this process is denoted H. What does H depend on? No variables, because the kinematics are fixed. The only dependencies are the discrete spin projection indices.\nThe particles involved have spins:\n\nLambda: J^P = \\frac{1}{2}^+\nProton: J^P = \\frac{1}{2}^+\nPion: J^P = 0^-\n\nSince parity is not conserved, the spin structure matters. By angular momentum conservation, the proton’s spin projection must match the lambda’s.\nThe matrix element is written as: \nH_\\lambda = \\langle p(\\vec{p}, \\lambda_p), \\pi^-(-\\vec{p}) | T | \\Lambda(0, \\lambda_\\Lambda) \\rangle\n Here, \\lambda_\\Lambda = \\lambda_p due to spin conservation. The initial state is on the left, and the final state on the right.\nThe pion is spinless, so its state is |0, 0\\rangle. The proton has helicity \\lambda_p, and the lambda has spin projection \\lambda_\\Lambda.\nThe amplitude H_\\lambda has two values: \\lambda = \\pm \\frac{1}{2}.\n\nNext, the unpolarized decay width: \n\\Gamma = \\frac{1}{2m_\\Lambda} \\frac{1}{2} \\sum_\\lambda |H_\\lambda|^2 \\frac{|\\vec{p}|}{8\\pi^2 m_\\Lambda}\n The phase space integral simplifies to 1 because the kinematics are aligned.\nThis decay is the main channel for lambda decay, determining its lifetime of about 10^{-9} seconds. In experiments, the lambda is identified by its decay to a proton and pion, with charged tracks pointing to a secondary vertex.\n\n\n\n\n\n\nDid I forget the \\frac{1}{2} from \\frac{1}{2J+1}? No, it’s absorbed correctly because only two terms survive in the sum.\n\n\n\nThis becomes more interesting when there’s a preferred direction in space, such as when the lambda is moving. The direction of motion provides a reference axis, and we can study the decay in the helicity frame.\n\nNow, let’s discuss helicity states in more detail.\nThe |JM\\rangle states are basis states for a particle with spin J, quantized along the z-axis with projection M. Acting with J_z gives: \nJ_z |JM\\rangle = M |JM\\rangle\n\nWe use active transformations: rotating or boosting the particle, not the coordinate system. For example, boosting a particle along the z-axis increases its momentum without changing its spin projection.\nRotating a state about the y-axis mixes the spin projections. The coefficients are given by Wigner D-functions: \nD^J_{m'm}(\\theta) = \\langle Jm' | e^{-i\\theta J_y} | Jm \\rangle\n\nFor spin-\\frac{1}{2}, the rotation matrix is: \nR_{1/2}(\\theta) = e^{-i\\theta \\sigma_y / 2} = \\cos\\left(\\frac{\\theta}{2}\\right) - i\\sigma_y \\sin\\left(\\frac{\\theta}{2}\\right)\n\n\nFor a moving particle, there are two ways to define spin states:\n\nHelicity states: Spin quantized along the direction of motion.\nCanonical states: Spin quantized along the z-axis.\n\nThese are related. A helicity state is constructed by boosting and rotating a canonical state: \n|p, \\lambda\\rangle = R(\\hat{p}) B_z(p) |0, \\lambda\\rangle\n\nWhen you rotate a helicity state, the momentum rotates but the helicity remains the same. Boosting is trickier because it changes the momentum direction relative to the spin.\nThe Lorentz boost matrix for a 4-vector is: \nB_z(\\beta) = \\begin{pmatrix}\n\\gamma & 0 & 0 & \\gamma\\beta \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n\\gamma\\beta & 0 & 0 & \\gamma\n\\end{pmatrix}, \\quad \\gamma = \\frac{1}{\\sqrt{1-\\beta^2}}\n\nFor canonical states, you first rotate the particle, then boost it. This ensures the spin projection is along the z-axis in the lab frame.\n\n\n0.2 Distinguishing Canonical and Helicity States in Particle Decays and Rotations\nNote on the notation: I have to distinguish. When I just tell you P, M, you need to ask, is it canonical? Strictly speaking, when you see P, J, and then another number, there is no way to figure out whether I’m talking about a canonical state or a helicity state.\nHowever, for the general notation, before I put the numbers here instead of M and \\lambda, M usually refers to the z-axis projection, and \\lambda refers to the helicity. It’s way better if you indicate that explicitly. So if I talk about helicity, I might say “helicity” here, or I can say “canonical” here. Adding these indices to the state to indicate what I’m talking about would be better. If I just say J, P, it might be confusing. Still, I will be doing this sometimes because of rushing.\n\nNow it’s also clear how to relate the two. But before we start talking about canonical, let me just show you the transformation of canonical states. I want to rotate, and then I immediately rotate. Put a definition of the state: it’s a rotation and then acting on the state B^{-1}(P) \\ket{0, J, M}. And then I only have to deal with matrices before it becomes something I can apply. So this is equal to… I will try to make on this side the same as I have on the right side to replace these two canonical R.\nWhat I have done: I inserted right here R'^{-1} R'. Then I put these matrices together, and now this—what is this?—is something that gives me a canonical state if applied to this state at rest. The only thing that remains is I have to apply the rotation operator to the state at rest. And this, you know, is the Wigner D-matrix acting on this state. Therefore, what I’m going to get is the sum of Clebsch-Gordan coefficients.\n\n\n\n\n\n\nKey Transformation: The canonical state \\ket{P, J, M}_{\\text{can}} is defined as: \\ket{P, J, M}_{\\text{can}} = R B_z^{-1}(P) \\ket{0, J, M} where R is the rotation operator and B_z^{-1}(P) is the inverse boost along the z-axis.\n\n\n\n\nQuestion: Doesn’t the momentum P also change because we change the angle? Response: Very good, thanks. So, it gets rotated. But you’re not sure if it’s just R P? It should include R'. R' is the second one—R' P. Maybe I would just call it P' instead. Somebody else can do the computation. Let me just say P' and then explain what P' is.\nSo, P' is R' P. Or it is also equal to R' R acting on P_z. Here I boosted, so the momentum became non-zero. Once I boosted after the boost along the z-axis, my momentum is just P_z, and then in the following operation, I apply the rotation. So here I apply the rotation, and now R' P_z, and that way I get P'.\n\nI learned how to boost yourself in the exercises, and now let’s discuss their relation to bosons. If you need to relate one to another, this is a gluon state. This is a canonical state. Here we have \\ket{P, J, M}_{\\text{can}}. What I want—I want to write so clearly—they are not equal to each other, but one can express \\ket{P, J, \\lambda}_{\\text{hel}} as the linear combination of canonical states. And these coefficients are then… I would like you to have a clear understanding of how we find these coefficients.\n\n\n\n\n\n\nHelicity State Relation: The helicity state \\ket{P, J, \\lambda}_{\\text{hel}} can be expressed as: \\ket{P, J, \\lambda}_{\\text{hel}} = \\sum_{M} C_{M \\lambda} \\ket{P, J, M}_{\\text{can}} where C_{M \\lambda} are Clebsch-Gordan coefficients.\n\n\n\n\nI wanted to quickly come back to the lambda decay and tell a little bit more about how we derived the formula. The large peak formula is the many D functions and H’s from the last lecture. I’m going to consider now a lambda that moves in the wave frame. It is a helicity state, and then it decays.\nIn order to get a z-axis, we need the direction of motion of lambda in the lab frame, and that gives us after the boost the z-axis. The way to define the z-axis in this manner is called the helicity frame for lambda. This is when we say “lab frame.” You always have to describe from which frame you are boosting, because it depends. In different frames, the direction of lambda will be different, and there will be differences when you arrive at the rest frame of lambda depending on from which frame you boost it.\nThe helicity frame is defined as the rest frame of the particle obtained by boosting from the frame where it was moving. The angle of the decay of the particle—when you take one of the particles, particle number one, and use this to define the angle—is called the helicity angle. That’s common jargon in hadron physics. When we talk about the helicity angle, it implies that we boosted to the rest frame and then took one of the particles as the reference and measured the angle from there. But all of the motion is still in the same plane as before. The boost and the two-particle system are now exactly opposite.\n\nWe start with a lambda flying in the z-direction with a certain velocity. Then it decays. It has a proton and a pion in some plane. And then we invert the boost, and we still have the proton and pion in the same plane. If I took this picture out, you would not have the plane any longer. The plane is formed by three vectors. So I need the original direction of motion of lambda to define the axis with respect to which I can measure the angle, and then I have a plane. This is our recap exercise. We started without knowing that, so we only had one axis. But now we have a plane. Now I have one more variable on which the amplitude depends, in addition to these two discrete variables. This is the scattering angle or the helicity angle.\nI have to compute now a final state here going to be frozen binary. Here, on the right side, we have configurations of lambda sitting in its rest state with the proton in lambda. And on the left side, we have this configuration. The way to proceed is to apply rotation to this configuration and arrive at the back. So the answer for this is H_{\\lambda_p \\lambda_\\pi} T_{\\lambda \\lambda}. And the problem of T_{\\lambda}. That’s the equation we had last time, where I was describing the matrix element for the sequence of the decays.\nLet me apply T on the final state. I want to simplify this. This is T acting on the proton. We want to evaluate the application of the transition operator that takes pion-proton and then transforms this to lambda. This is the meaning of our operator. This operator acting now on the pion-proton state that has the P vector (0,0). We notice that this is rotated about the y-axis by the angle \\theta. We want to align this because, on the left state, we have aligned combinations. So we have a rotation. Now we pull out the rotation, and then we have the same combination now along the z-axis. Proton goes forward, time goes backward, and rotation is explicit.\nSince these operators commute—strong interactions conserve spin—therefore, one can compute the transition operator and rotation and then act first with the transition and then by rotation. Essentially, this transition operator transforms pion-proton to lambda. We explicitly do this by inserting the identity state here. Here I have a \\ket{\\lambda}_{\\text{hel}} for the left state. This part is a path \\lambda_p. It’s going to give me just the state of the lambda with the same spin. This matrix element we just evaluated—this is just a Clebsch-Gordan coupling. The identity that we inserted here should have some overall possible combinations. Therefore, that would be a sum over \\lambda. This will give me delta functions for the \\lambda_p \\lambda_\\pi. And that’s why I want only one state—I can do this. Now the last step to do is to apply rotation to it.\nIt’s good to see it once. But once you get the idea, it’s easy to believe that in the same case of the cascade reaction, what you have for every bit of the transition is the product of this block helicity that is the transition matrix element for the aligned transition. And then the rotation I want to evaluate as well. Now, the differential cross section exists as a function of the angle. This is my equation. We know that these are—so these are our \\cos \\theta, -\\sin \\theta, \\sin \\theta, \\cos \\theta matrices. For the spin-half, we just had an explicit matrix, and we have two coupling constants that are measured in experiment. You can take them to compute the angular distribution.\nIt’s a little bit disappointing right now if I just tell you the answer. The answer is the same as you had before, because you have a \\sin^2 \\theta + \\cos^2 \\theta, which is one. And this is in front of the first one. In front of the second one, you have a -\\sin^2 \\theta + \\cos^2 \\theta.\nAll right, so what we learned now in this example is that once you have an unpolarized particle, you’re not going to observe any interesting angular distribution. We actually summed over the final state, we summed over the initial state. There is no non-trivial angle distribution that remains for the 2\\pi before we integrate the whole \\cos \\theta. Now we have a differential \\cos \\theta—right before, we just wrote the gamma is equal to… and then the phase space has… it has a d\\Omega / 4\\pi, an integral d(\\cos \\theta) / 2, d\\phi / 2\\pi. This is one decision. And now I just move this \\cos \\theta to the other side.\n\nPolarized decay. Now finally, polarized decay. When I get non-trivial distributions if I polarize my particle. So let me say when lambda was flying, it had only spin, only \\lambda. And now finally we have something. I have a formula. But now let’s try to see what I’m going to see in the experiment.\nLambda travels with a certain momentum, and the projection of the spin to the direction of motion is equal to 1/2. And then it decays. In the decay of this state, we are going to find that it’s more likely for the proton to travel forward than backward. But this is the angle of the proton. Interestingly, this violates parity. Because you can apply parity to the initial state and final state and find out that by applying parity you flip the momentum, you don’t flip the spin. And then in the final configuration you flip momentum, you don’t flip the spin. So parity implies that there cannot be a symmetry. Parity is violated here. This is also consistent with the fact that we consider inside of the blob here… big decay amplitudes. Parity… we likely put the weak decay in.\n\n\n\n\n\n\nParity Violation in Decay: The differential decay rate for polarized particles (e.g., \\Lambda) shows parity violation: \\frac{d\\Gamma}{d\\cos\\theta} \\propto 1 + \\alpha P \\cos\\theta where \\alpha is the asymmetry parameter and P is the polarization. This asymmetry arises from weak interactions.\n\n\n\n\n\n0.3 Parity Violation in Asymmetric Angular Distribution\nNo, it’s not. Well, it is.\nYou told me that it’s a linear function. No, I don’t.\nThe point is in the general. It’s a great question.\n\n\n\n\n\n\nWhere this parity violation appears is when the two terms are not equal to each other. If they were equal, \\sin^2\\theta + \\cos^2\\theta would give 1 with no angle dependence.\n\n\n\nThe parity violation is in the fact that these terms are not equal. This introduces an asymmetry, making the distribution angle-dependent, as seen in the formula:\n\n\\frac{d\\sigma}{d\\Omega} \\propto 1 + \\alpha \\cos\\theta\n\nHere, \\alpha \\neq 0 signals parity violation, breaking the isotropy of \\sin^2\\theta + \\cos^2\\theta = 1.\n\n\n0.4 Polarization and Angular Distribution in Λ Decay\nThanks for the question. That’s really important to know, and in fact, they are not.\nMoreover, we consider the polarized decay: the 100% polarization \\lambda = \\frac{1}{2} is a pure state, a spin projection \\frac{1}{2}, and it’s fully polarized. One can also consider a mixed state where it’s not fully polarized.\nMost realistically, the degree of polarization for the \\Lambda is not 100% but around 60%. That’s what we have. In the B decay case, the \\Lambda is produced with a polarization of 60%, and in that case, the asymmetry is smaller.\n\nThe angular distribution of the decay is given by:\n\n\\frac{d\\Gamma}{d\\cos\\theta} = \\Gamma_0 (1 + \\alpha \\lambda \\cos\\theta),\n\nwhere:\n\n\\Gamma_0 is the total decay rate,\n\\alpha is the analyzing power (polarizing power),\n\\lambda is the degree of polarization (0 \\leq \\lambda \\leq 1),\n\\theta is the decay angle.\n\n\nWe can rewrite these equations by contracting the matrix element with the polarization matrix and find that the difference between two edges defines how well this particular decay reflects polarization. The quantity \\alpha is often called the polarizing power. It tells you how well this decay is suited to measure the initial polarization.\n\n\n\n\n\n\nIf the couplings are equal to each other, you don’t have sensitivity to the initial polarization. It can also happen even for big decays that the couplings are equal. Parity can be violated, but for most cases, there is a non-zero analyzing power.\n\n\n\nThis \\alpha is non-zero, and that’s why by looking at the angular distribution, you see parity violation. But you can also measure the initial polarization. That’s called the polarimetry technique, and it’s actively used.\n\nLook at the angular distributions. All particles have known spin, and the couplings are known. But these values have to be measured in advance, and in that case, you can measure polarization.\nThis initial polarization is a super powerful observable. Particles like \\Lambda with spin carry polarization out of the interaction point, which is part of the information. How \\Lambda is produced—with what momentum and with what polarization—tells us about the internals of the interaction.\nImagine \\Lambda is produced in the quark-gluon plasma. Its polarization can now be related to the properties of the plasma. This is a kind of free carrier of information out of the mess of the quark-gluon interaction.\nPolarization plays an important role, if not more than other observables. This particle not only carries polarization but also, by decaying, gives us a way to measure it.\n\n\n\n0.5 Engaging with Questions Before the Lecture\nQuestions, questions, questions. We have time, but it is up.\nInstead of starting a new lecture, I would like to give you a question I have in mind for the lecture. For example:\n\nIf I were to explain the material to you, you would already know it\nBut I haven’t\n\n\n\n\n\n\n\nThe speaker is using a Socratic approach - testing understanding before delivering content. Common in physics lectures to gauge baseline knowledge.\n\n\n\nSo I’ll just give you the question and see if you know it without my lecture. Meanwhile, tell me if you have questions.\n\n\n0.6 Analytic Functions, Contour Integrals, and Dispersion Relations\nCan you find half or maybe a quarter page? Maybe we can take Ilya—can you help obtain quarter-page pieces of paper?\n\nNext lecture, we will move on to discussing analytic functions and properties of amplitudes in the complex plane. This requires you to have a little bit of complex analysis. We’ll discuss this further—the next sheet also has a bit of discussion on the complex plane.\nSo, we need a little bit of complex algebra, from 1 to 7. From 1 to 7. Let me just say where this comes from.\n\n\n\n\n\n\n\nKey Theorem: The Cauchy Integral Theorem states that for any analytic function F(z) inside and on a closed contour C: \n\\oint_C F(z) \\, dz = 0.\n\n\n\n\nWhat is written here is obtained by doing the contour integral. I started with a small circle—my function is analytic—and I’m going to stretch the circle in all directions. This is my complex plane (x-plane).\nThe Cauchy integral of F(x), if no singularities occur inside my integration contour, is zero for any analytic function: \n\\oint_C F(z) \\, dz = 0.\n\n\nThen, there is a theorem that tells me I can insert a singularity explicitly inside the circle. If I integrate \\frac{F(x')}{x' - x} \\, dx' and integrate this around—the integral was zero, but now let me put a pole explicitly inside like this.\nWhen I integrate, my integral is no longer zero—it’s equal to the function evaluated at the pole, and that’s my F. This is given by Cauchy’s Integral Formula: \nF(a) = \\frac{1}{2\\pi i} \\oint_C \\frac{F(z)}{z - a} \\, dz.\n\n\nNow, I have this beast here. It’s something similar, but I started from a small contour and stretched it to infinity. Here is infinity—this part of the contour drops out, and the only thing that remains is the integral from 1 to 7.\nI’m integrating the imaginary part of F(x) from 1 to 7 and asking: Can this equation be satisfied? This relates to the dispersion relation: \n\\text{Re}\\, F(x) = \\frac{1}{\\pi} \\mathcal{P} \\int_{1}^{7} \\frac{\\text{Im}\\, F(x')}{x' - x} \\, dx'.\n\n\nSecond question: What is the analytic structure? What do you mean by “analytic”?\n\n\n\n\n\n\nAnalytic Functions: A function is analytic in a region if it is complex differentiable at every point in that region. This implies the function is infinitely differentiable and can be represented by a power series locally.\n\n\n\n\n\n0.7 Branch Points, Cuts, and Analytic Structure in Complex Integration\nCut both branch points. This is super unusual for math courses, but that’s what we use all the time in physics—this type of integral where the leftover of the contour is from 1 to 7, and what you integrate is your function F.\nSince it comes from both sides, from this side and that side, and they have opposite signs, what remains is the imaginary part. The real part is the same. It cancels out when you calculate. So the thing that remains is the integral from the same.\nIf you have anything to say about that, write it down. Maybe it’s too complicated.\n\nLet’s make a round. Say a few words about this. Start with:\n\nCan you be satisfied?\nCan we be satisfied?\n\nThree is a solution. You can just take the constant 3 because it has no imaginary part. The real question is, if you remove the three, at least there are non-trivial solutions. Sometimes non-constant words like non-vanishing.\nIf you say F(X) = 3, it has no imaginary part. Then the imaginary part is not present anywhere. The question is, if you remove the three—well, that’s the question.\nIf you just ask this, I think you’re completely right. But I was actually thinking of non-trivial solutions. Do we say it exists or not?\n\nYou can probably put power series in. It might work—totally could work.\nIt can be satisfied. The answer is: Give me any function, whatever you want. I put it here. Any function X—that integral actually converges for any value.\nTake \\sqrt{X}. Put it here, and it’s satisfied. Just anything put inside the imaginary part, it’s satisfied. The reason we’re satisfied is this: it’s a way to construct the function.\n\n\n\n\n\n\nThe function F(X) can be constructed using the integral representation: \nF(x) = \\frac{1}{\\pi} \\int_1^7 \\frac{\\sqrt{t}}{t - x} dt\n where the integral is taken along the real axis, avoiding the branch cut.\n\n\n\n\nLet me show you: this \\sqrt{X}, I put it here instead of this expression, \\sqrt{X}. Then this way I compute my function F(X). This is a super special function. Its imaginary part is equal to \\sqrt{X} in the region from -1 to 7.\nIn -1 to 7, if I evaluate, the imaginary part is equal to that. In the rest of the complex plane, the function is non-zero. But there are no singularities.\nThis insertion that I made is actually done by introducing some non-trivial analytic structures. Now we have three candidates. Let’s give them votes.\n\nWhat kind of non-analytic structures have I introduced? It’s like a continuous stretch of poles—a stretch of poles. What is meant by a cut? They just end somewhere.\nThe cut is the non-analytic structure where the function on one side is different from the function on the other side. It’s like \\sqrt{-1 + i} and \\sqrt{-1 - i}.\nWe see that this one is equal to +i and this one is equal to -i. On different sides, I have different values. This is a cut. It’s not really anything else than a spectrum of poles.\nPoles have divergence, and this thing does not have divergence. So what will you say? You go for poles.\n\nAre you attracted by the concept of branch points or cuts? I was thinking about cuts, but now I’m convinced both solutions are not poles.\nIn the integral, there are no poles. The integrand has poles, but they are at zero.\nYou have to analytically continue something. It’s like you probably have to take it above the complex line or the real line, and below, probably differently.\nThe branch point is where it starts and where it ends. You forget about the branch work for the elements.\n\nThe analytic structure of my function in the X plane has:\n\nA branch point at 1,\nA branch point at 7,\nAnd they’re connected by a cut.\n\nThere are no poles. The function doesn’t have any poles.\nThe way we construct the function here—in this way, you introduce on the cut, you don’t do your space.\n\nIt’s really funny to think where this guy didn’t come from, go where. You can look at this plane and take a walk here. You don’t experience any poles, any singularities.\nWe can dive under. You end up in a different world that has a gate. Through the gate, you go to the other world. There you find poles—this is zero, it has poles. And it has \\sqrt{X}, so it has another cut.\nThe function has an interesting and complicated structure in the complex plane. On the regular complex plane where we call the function, it doesn’t have any singularities except one gate.\nThrough the gate, you can go to the other so-called sheet, and there you have a lot of stuff going on.\n\nYou just get used to it. It’s really fun to think of this. We will discuss a little bit more of the complex chart structure and how scattering—what is actually the complex structure of the scattering amplitudes.\n\n\n0.8 Branch Points, Poles, and Multivalued Functions in Complex Analysis\nOpening Remarks: Sorry for being late, and thanks for coming. We have a two-week break next week. We don’t have class, so have a nice holiday.\n\nSimplifying the Problem: Let’s make it simpler because it can be. Let’s make a constant in order to simplify—oh, I don’t even need this.\n\nIntegration and Logarithmic Behavior: The integral will always converge from 1 to 7. This is what we have. This is a logarithm. The logarithm itself can have a pole.\nLet’s evaluate the function at 8: \n\\log(1 - 8) - \\log(17 - 8).\n I’m going to say 8 \\pm \\epsilon, and then: \n\\log(1) = 0, \\quad \\log(-1) = i\\pi.\n \n\\log(-x) = \\log(x) + i\\pi.\n\n\nResults and Continuity: We arrive at the result that may equal each other. The difference here is -i\\pi and +i\\pi. In the equation, this is continuous, and here is the jump. The real part of x cannot be between 1 and 7 because we want to have a structure around it.\nWe cannot have x from 1 to 7 because we have the structures looping around it. The structure is looking around. You introduced the branch points at the edges and then a cut connecting these points.\n\nSingularities and Branch Points: It’s explicitly clear on the determinant itself by doing a simple integral. This expression has an n-structure: a cut, branch point at 1, branch point at 7, and then a cut. Is it anything else but a pole? No.\nA branch point can have a divergence—like the function could be infinity there—but it doesn’t have to. This function’s branch point is zero. The function is zero here; the function is infinity here.\nThe pole is this: \n\\frac{1}{x^3} \\text{ is a pole of third order.}\n \n\\frac{1}{(x - c)\\log(x - c)} \\text{ is not a pole.}\n\n\n\n\n\n\n\nKey distinction:\n\nA pole is an isolated singularity that can be resolved by an infinitesimal shift in the complex plane.\nA branch point (e.g., for \\sqrt{x} or \\log(x)) is non-isolated and requires branch cuts to define the function uniquely.\n\n\n\n\n\nTeaching Style and Context: For us, your lectures are a bit unstructured, and you’re a bit all over the place. But it also makes it more fun because of your sketches of boosts, beasts, and rotations—just sketches. You don’t really prove anything like in other courses. It’s not a strict or rigorous proof of everything—it’s just not fun for a while.\nI’m fine teaching advanced people, but you have to be more structured for younger students.\n\nMetaphor of “Gates” and Multivalued Functions: We go through the gate, but outside the gate, you said the function is fine, it’s continuous. Here you said it’s another gate. This is the first world, second world enter. Here we can walk around the gate; it’s fine. But then it goes through the gate and ends up in another world.\nYou can go through the gate and appear in that world. It has many more gates. You can go around this and enter on the other side—that’s where you come out here. If you do this, you are not at the end.\nIt’s a third world. Then it’s an infinite number of worlds because it’s a loop at infinity. This gives an infinite number of worlds, but this one is simpler.\n\nPractical Application and Fun Idea: Imagine taking VR glasses and walking through this. That would be quite fun. You can suggest this to Matrix Netflix. Make it an escape room—you only get out if you find the first gate.\n\nConnection to Physics: Where does this function appear? All scattering amplitudes as functions of Mandelstam variables (s, t, u) have energies—the area state."
  },
  {
    "objectID": "2024-Lecture-07.html",
    "href": "2024-Lecture-07.html",
    "title": "(2024) Lecture 7",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Mass and Quantum Numbers of the PCC Hadronic Molecule\nAll right, two minutes. All of the problems are one-liners. You solved them online, but you have to know what to do.\n\nLet’s discuss quickly question number one. First, the mass of the states. This appears as the particle, right? In this condition, neglecting inelastic channels, this particle is formed by the cascade C and D. This particle can travel, can leave. It is a regular particle, but its internal structure is the hadronic molecule.\nWhat is the mass of this particle? It is the sum. Indeed, the mass of the PCC particle—that’s how we call them—is roughly the mass of the constituents:\n\nM_{\\text{PCC}} = M_C + M_D.\n\nThat’s the way we define the binding energy:\n\nE_{\\text{binding}} = (M_C + M_D) - M_{\\text{PCC}}.\n\n\n\n\n\n\n\nBinding Energy Clarification: The binding energy represents the difference between the sum of the constituent masses (M_C + M_D) and the mass of the bound state (M_{\\text{PCC}}). A small binding energy implies the state sits near the threshold.\n\n\n\n\nShouldn’t it be PCC? No, it’s \\overline{PCC}. Very good. It used to be called PC, and last year we updated the notations. Now it’s PCC; before it was PC because, with the C, you indicate that it contains the charm. This is better—it’s really clear.\nAs indices, you list heavy quarks because they kind of conserve quantities with the decay. Light quarks are… we don’t miss them. You can always recover them from the capital letter. If I say \\Sigma, you know, that’s a capital. The two light quarks, depending on the charge, are either UU, DD, or UD. Then SAP means pentaquark—5 quarks. SACC is CC plus 3 others. For PCC, this is UD.\n\nLikely, the PCC delta logs we see in this particular channel are of this type—mass of the… This was easy. Neglecting binding energy, you really get the mass of two particles; that’s called the threshold. The language we use is saying that the state is… Since the binding energy is small, the state sits at the threshold:\n\nM_{\\text{PCC}} \\approx M_C + M_D.\n\nThe hadronic threshold is there. How much is it? Approximately like 10 MeV, 100 MeV. What’s the scale? 10. Less than 10. More than 1. The mass of the \\Sigma is 2.5 or so; the mass of the D is around 2 GeV. The sum is 4.5 GeV. That’s roughly where one would search for such formations.\n\nNow, quantum numbers, spin algebra—my favorite exercise in this course. We have \\frac{1}{2} and \\frac{3}{2} instructions. Parity should be - - because we have s-wave. Exactly. Very good. So you expect two states:\n\nJ^P = \\left(\\frac{1}{2}^-\\right) \\quad \\text{or} \\quad \\left(\\frac{3}{2}^-\\right).\n\n\nThey have a cool lower bound on lifetime. Where does the lower bound really come from? It’s the binding of the two constituents. Any idea why this state would ever decay and how it would decay actually? When we talk about lifetime, the particle decays, right? That’s what happens with all particles.\nHow does this particle decay? I would love to say no, but… The energy of the system is conserved. Once you split it into two, the minimal energy in the system is their mass—M_C + M_D. This is already the energy. If they fly apart, the energy is even higher. This is not allowed—energy must be conserved.\nWe sit in the rest frame of this particle, and all the energy you have is the mass of this particle. That’s it.\n\n\n\n\n\n\nEnergy Conservation Constraint: The decay into free constituents is forbidden if the binding energy is positive (M_{\\text{PCC}} &lt; M_C + M_D), as the system lacks sufficient energy to separate.\n\n\n\n\n\n0.2 Charm Quark Annihilation, Decay Channels, and Bound State Lifetimes in Heavy Mesons\nBecause the c and the \\bar{c} undergo electromagnetic processes. Annihilation of these two quarks is possible, but this is a rare process. Since it’s an electromagnetic interaction, the probability of this process is much smaller than the process I would like you to consider.\n\nWhy have c \\bar{c}? Can they annihilate via the strong interaction? No—even if they have different color, they can produce other quarks. This is actually suppressed for heavy particles. The heavier the particles, the smaller the chance for them to annihilate. So this is also strongly suppressed.\n\nThe three clones appear only for the color-neutral objects. The three clones we need to construct the color-neutral object here are living in the color octet. It’s not singular, so they don’t. The strict vertex is a different suppression.\n\n\n\n\n\n\n\nThe big decay—that’s what happens with this object. If we neglect inelastic channels, the decay of this particle is one of the c quarks transitioning to s, emitting the W:  c \\rightarrow s + W^+  This is a big decay.\n\n\n\n\nIf the structure of the object is like that, we can even understand the probability of this decay happening. Big decay is one thing, but there is another transition. This object decays weakly, so it will decay quickly. We can compute the lifetime due to that.\n\nBut there is another process. This state is not the ground state. In the spectrum, we combine quarks 1, 2, and \\frac{1}{2}. We have 1 and 0. The lower one is called D, and the upper one is called D^*. The transition D^* \\rightarrow D is possible by emitting \\pi^0 or \\gamma:  D^* \\rightarrow D + \\pi^0 \\quad \\text{or} \\quad D^* \\rightarrow D + \\gamma \n\nThe most likely decay channel is the radiative decay of the D. The width is smaller than the width of the D, so the lifetime is bigger than the lifetime of the D^*. That’s a bound.\n\nYou combine two objects, and the object lives as long as the constituents—it would live infinitely if these particles are stable. Like the deuteron is made of a proton and neutron. The proton is stable, and the neutron lives long. The deuteron is stable because the neutron is so bound inside that it cannot decay.\n\nThe binding mechanism suppresses the phase space of the decay and extends the lifetime of the particle. The D meson here would live longer than the isolated D^* meson. The width of the state is smaller than the width of the D, and since the lifetime is the inverse of the width:  \\tau = \\frac{1}{\\Gamma}  The lifetime of this object is bigger than the lifetime of the D.\n\nWill the \\Sigma_c^* also survive or be an atomic molecule? It just goes down an energy step instead of falling apart.\n\nLet’s try to understand your question. You asked about the cascade \\Sigma_c^* and the \\Sigma_c + D^0 molecule. Does this exist? It does. So the molecule can make steps down one energy step—but it can also dissociate into \\Sigma_D + \\pi or \\Sigma_D + \\gamma, because the energy for this is a lot:  \\Sigma_c + D^0 \\rightarrow \\Sigma_D + \\pi \\quad \\text{or} \\quad \\Sigma_D + \\gamma \n\nThe binding energy and the situation of the decays are actively discussed in the field. At every conference, you see roughly five talks on this—it’s a really hot subject.\n\n\n0.3 Invariant Variables and Kinematics in Four-Legged Processes\nLet’s move to item number two. Understand? The plane is the plane of invariance.\nFor any reaction with a block—an open direction centered on four legs going out (X, A, B, C)—you can define invariant variables that characterize the kinematics.\nThe first variable is the mass squared of A and B: \nS = (p_A + p_B)^2\n The mass of A and B is the square root of S, so \\sqrt{S} is the mass of the system of particles A and B.\nThen: \nT = (p_B + p_C)^2\n is the mass squared of the system with particles B and C.\nAnd: \nU = (p_A + p_C)^2\n is the mass squared of the system with A and C.\n\nWe figured out earlier that two variables are enough to characterize the kinematics of the four-legged process. This comes from counting degrees of freedom.\nThe two variables could be:\n\nS and T\nS and U\nor any pair.\n\nOn the plane, they are connected because the sum of the three invariants satisfies the Mandelstam relation: \nS + T + U = m_A^2 + m_B^2 + m_C^2 + m_X^2\n\n\nThe scattering amplitude (or transition amplitude) depends on only two variables—S and T, S and U, or any pair.\nThis amplitude is defined as a function on the plane of invariants, and different domains on this plane describe different processes.\n\n\n\n\n\n\nThe Mandelstam relation shows the interdependence of S, T, and U, constraining the scattering amplitude to depend on only two independent variables.\n\n\n\n\nBefore proceeding, let’s focus on the kinematics and see where A, B, C, and D are located.\nThe way I want to solve this is by checking the signs:\n\nFor each process, is S greater or less than zero?\nWhat about T?\nAnd U?\n\nBy identifying these, we can place the kinematics.\n\nLet’s consider process A.\n\nIs S positive? Yes. Why? Because S is the mass squared of two real particles, and this mass is at least the sum of their masses: \nS \\geq (m_A + m_B)^2\n In the rest frame, the energy of the system is greater than the sum of the masses if they have momentum.\n\nFor A:\n\nS &gt; (m_A + m_B)^2\nT \\geq (m_B + m_C)^2\nU \\geq (m_A + m_C)^2\n\nThe domain for the first kinematics is where S, T, and U are all greater than their respective thresholds.\n\nFor process B:\n\nT is the physical mass squared of two particles, so T &gt; 0 and T \\geq (m_B + m_C)^2.\n\n\nTo discuss the kinematics of cross-channel reactions (like A, M, D, E particles), we can swap momenta.\nFor particles moving to the other side, we modify the definition of the invariants by flipping the sign of their momenta.\nThis is what Ares mentioned—you could redefine the invariants accordingly.\n\n\n0.4 Kinematic Domains, Dalitz Plots, and the Kibble Function in Scattering Amplitudes\nTwo different directions, and then the sum minus S would appear in the first place there. We could start with this direction. The place for this is when T is more than zero, because T is the physical mass of two particles. And then S is under A and B. This is kind of an impossible combination. They are on different sides, and the variable S is going to be less than zero, and then U is less than zero. So it’s another impossible combination, and the domain, roughly speaking, is where U is below zero. And then S is above, T is above zero.\nHere’s T. So T is this direction. And then S is above zero. This and U is above zero. This color has, so here this is the minimum. So this one is B, and same on the C. And then another one, D, where is D? A + C, X + B. A and C give us the U-channel. So this is the positive. U is positive.\n\n\n\n\n\n\n\nThe Kibble/Klein function describing physical domain contours: \n\\Phi(s,t,u) = s^2 + t^2 + u^2 - 2st - 2tu - 2us\n This single function can generate all boundary lines when solved for \\Phi(S, T, U) = 0.\n\n\n\n\nAnd now to see real physical contours for the kinematics. One has to calculate what are the physical ranges of scattering variables. Like if you work in the cosine of the scattering angle, by placing the restriction that cosine must be from -1 to 1, you identify that the border is such that not all points here are allowed, but only a certain region. And then the true border for the scattering is often given by this sixth-order polynomial.\nIn the center is this canvas. This is our Dalitz plot. This is here. And then another direction could be here. So 1, 2, 3, 4. Four regions for physical reactions that are happening. It’s important to realize that connection because there’s just a single amplitude matrix element that describes all four. So if you get this function and constrain it precisely, it describes all four. Just define it on this domain, and for every point you can compute, it gives you the complex number 1 + 3i. And then this is the value of the quantum transition amplitude. The matrix element you can compute at a different place. Now we have a complex transition amplitude for the decay. I think that’s super cool.\nIt works very well in QED. When you consider Compton scattering, photon-electron going to photon-electron, this is a crossed process. And it’s exactly the same matrix element that describes the electron-positron annihilation to two photons. And it’s exactly the same process that describes two-photon production of an electron-positron pair in quantum electrodynamics.\n\nIn hadron physics, it’s a little bit more complicated. Since we have perturbation theory, we always model. So what we are going to do when we describe the Dalitz plot, we model it in terms of the chunk of the resonances. And then you find that once you want to compute this function right here, the scattering amplitude, it blows up. It has infinities, so it has unphysical behavior. The reason is because we employ a finite range of finite number of resonances, and the physics is more complicated analytically.\nThe fact that this amplitude is related to that domain tells you already that you have to put an infinite number of resonances. Like we saw the lines on the dispersion, these are all resonances. And in order to relate this in this domain, you have to operate with the infinite sums, and then in infinite terms they would compensate each other and you have a reasonable amplitude.\nThere has been an effort for now, 30 years, 50 years, of finding such a set of functions that works everywhere nicely and is analytic and reasonable. What’s most difficult is to come up with something that describes the data, because there is Regge theory. You might have heard of Regge theory, and this is one of the approaches. So you come up with a complex function that has all the nice properties—it works reasonably on scattering domains and it works in the decay domain. However, of course it lacks exact understanding of the resonance properties. Like it would put resonances there that have zero width in Regge theory. And one of the interesting developments is to implement resonances with widths.\n\nOne more thing. If you like playing with functions, there is a very simple expression that describes contours of the physical domain. This is called the Kibble function. I think it’s worth giving. There is just one function. If you solve \\Phi(S, T, U) = 0, you find this line, that line, that line, and that line. Just one function—put it into a solver, and for any given value of S… So let’s fix S. Where is our S? Let’s fix S to be -50. We’re going to get two solutions. One of them… Let’s put S = +20. Then you have two solutions using this.\nThis function is really easy to type into code. This is the Klein function or Kibble function. It’s almost a complete polynomial, but it misses more terms of this table. So it’s:\n\n\\Phi(s, t, u) = s^2 + t^2 + u^2 - 2st - 2tu - 2us\n\nThis is called the Kibble or Klein function.\nThe Kibble function is the combination of three Kibble functions, where every \\Lambda_1, \\Lambda_2, and \\Lambda_3 corresponds to a different channel. So \\Lambda_2 is then for B—it’s \\Lambda(T, m_0^2, m_X^2). And here is the… what’s missing? Mass and T. And then \\Lambda_3 is \\Lambda(U, m_X^2, m_B^2).\nSo then you go to Wolfram Alpha and say, “Give me a contour plot.” And then you form them and you find all this function and discussion on that in the Pikkuharju-Kalenti book. Have we discussed this book? The best book on particle kinematics is written by two authors with unspellable family names. Unspellable. And this is—if I got it right—see Bikulov-Janti. No, cycling? No. Yes, correct me. But the book is excellent. So if you get a chance to get yourself this, you know everything about particle kinematics, particularly very peculiar properties of the Kibble function and the Challenger function.\n\n\n0.5 Unitarity as a Constraint on Scattering Amplitudes\nWe are ready to move to the topic of today: unitarity. Unitarity is a constraint on the scattering amplitude. In heavy physics, we don’t compute the scattering amplitude from first principles but rather model it. Our guiding principles are what this amplitude can be and what it should be.\n\n\n\n\n\n\nYou cannot just write down an arbitrary expression for the amplitude that fits the data. There are some principles, and one of them is probability conservation. Probability conservation translates into the mathematical statement on the amplitude known as unitarity.\n\n\n\nYou might have seen earlier in the particle scattering course the optical theorem, which relates the imaginary part of the amplitude to the total cross section. It essentially states that the imaginary part is equal to the coefficient of the total cross section:\n\n\\text{Im}\\, \\mathcal{M}(s, t=0) = 2k\\sqrt{s}\\, \\sigma_{\\text{tot}}(s),\n\nwhere:\n\n\\mathcal{M} is the scattering amplitude,\ns is the Mandelstam variable,\nk is the momentum,\n\\sigma_{\\text{tot}} is the total cross section.\n\nThis is a consequence of unitarity—a consequence of probability conservation. It is so important, this very powerful statement, that just this principle alone allows you to derive a decent low-energy scattering amplitude that describes all of the resonance phenomena.\n\nYou see the bump in the spectrum due to probability conservation. You know what expression you should take to describe this phenomenon. Moreover, since unitarity is about the analytic properties of the amplitude, it tells you what the analytic structure of the amplitude is.\nIn the last lecture, we briefly discussed analytic functions and what singularities they might have, like cuts or branch points. Unitarity also tells you the location of the singularities in the complex plane. This comes from the fact that we are dealing with real unitary functions—functions for which the imaginary part determines the locations of the cuts.\n\n\nUnitarity condition for the S-matrix: \nS^\\dagger S = I\n where S is the scattering matrix and I is the identity matrix.\nPartial wave expansion and unitarity (for elastic scattering): \n\\text{Im}\\, f_l(s) = k |f_l(s)|^2\n where f_l(s) is the partial wave amplitude for angular momentum l.\nBranch cut structure due to unitarity: \n\\text{Disc}\\, \\mathcal{M}(s) = 2i\\, \\text{Im}\\, \\mathcal{M}(s)\n where \\text{Disc}\\, \\mathcal{M}(s) is the discontinuity across the branch cut.\n\n\n\n0.6 Unitarity Constraints on Scattering Amplitudes and Partial Wave Expansion\nLet’s get there step by step, starting with the scattering amplitude and partial waves. For the sake of time, let me first state where we arrive, and then we can derive it. We will derive the following three equations, all dealing with scattering amplitudes.\n\nThe first equation describes how unitarity acts on the full amplitude. The scattering amplitude A describes a process, and unitarity tells us:\n\nA - A^* = i \\int A^* A \\, d\\Phi,\n\nwhere d\\Phi is the phase space of the intermediate state. Diagrammatically, this represents the relationship between the amplitude and its complex conjugate.\n\nFor partial waves, the amplitude A becomes a function of a single variable. We can simplify the phase space and find:\n\nA - A^* = i \\rho(s) |A|^2,\n\nwhere \\rho(s) is the phase space factor. This tells us that the imaginary part of the amplitude is related to the squared amplitude.\n\nNow, let’s derive these expressions. We start with the scattering amplitude for a 2-to-2 elastic scattering process. On the board, I draw a diagram where a blob represents the interaction between two particles. Below it is the kinematic representation in the center-of-mass frame, where the total momentum is zero.\nIn the center-of-mass frame, the momenta of the particles are equal, even if their masses differ. The momentum is given by:\n\np = \\frac{\\sqrt{\\lambda(s, m_1^2, m_2^2)}}{2\\sqrt{s}},\n\nwhere \\lambda is the Källén function. After the interaction, the particles have the same masses and momenta, but the scattering angle changes.\n\nThe observable for these interactions is the angular distribution. For 2-to-2 scattering, once the energy is fixed, the only variable is the scattering angle \\theta. The amplitude depends on \\theta and the energy s.\nThe scattering amplitude is defined as the matrix element:\n\n\\langle p_1', p_2' | p_1, p_2 \\rangle,\n\nwith energy-momentum conservation enforced by delta functions. The phase space for two-body scattering is:\n\nd\\Phi_2 = \\frac{1}{(2\\pi)^2} \\frac{p}{4\\sqrt{s}} \\, d\\cos\\theta \\, d\\phi.\n\nThe identity operator integrates over all possible angles, ensuring normalization. The two-body phase space covers all directions on the sphere, representing the 4π solid angle.\n\nUnitarity is a statement about the scattering operator. We separate the trivial part (identity) from the interaction part T, defining the scattering amplitude. The unitary condition for T is:\n\nT - T^\\dagger = i T^\\dagger T.\n\nThis leads to the general unitarity relation:\n\nA - A^* = i \\int A^* A \\, d\\Phi.\n\n\nFor partial waves, we expand the amplitude in Legendre polynomials:\n\nA(s, \\theta) = \\sum_{j=0}^\\infty (2j + 1) A_j(s) P_j(\\cos\\theta).\n\nThe partial wave amplitudes A_j(s) are constrained by unitarity:\n\n\\text{Im}\\, A_j(s) = \\rho(s) |A_j(s)|^2.\n\n\n\n\n\n\n\nThis expansion converges well for strong interactions, as high-j contributions are suppressed by the size of hadrons. In experiments, only a few partial waves are needed to describe the data—often fewer than ten. This makes the partial wave expansion a powerful and practical tool.\n\n\n\n\n\n0.7 Properties and Independence of Partial Wave Amplitudes\nAnother thing that appears here is the partial wave amplitude. This amplitude is a function of a single variable (k, the momentum). For every partial wave, there is one function of one variable. Each partial wave has fixed quantum numbers (l, the angular momentum quantum number).\n\n\n\n\n\n\nThe partial wave amplitude for angular momentum l is given by: f_l(k) = \\frac{e^{i\\delta_l(k)} \\sin \\delta_l(k)}{k}, where \\delta_l(k) is the phase shift for partial wave l.\n\n\n\nA large advantage of partial waves is that they do not interact with each other. They don’t influence each other. Every partial wave is independent.\nSince quantum numbers in scattering are conserved, partial waves in the initial state are only related to partial waves in the final state with the same quantum numbers. In the unitarity constraint, you will see that it’s actually a single partial wave that relates to its output.\n\n\n\n\n\n\nThe unitarity constraint shows their independence: |S_l(k)|^2 = 1 \\quad \\text{or} \\quad S_l(k) = e^{2i\\delta_l(k)}, where S_l(k) is the S-matrix element for partial wave l.\n\n\n\nThere is no overlap between waves. They don’t mix.\n\n\nThe partial wave amplitude for angular momentum l is given by: \nf_l(k) = \\frac{e^{i\\delta_l(k)} \\sin \\delta_l(k)}{k},\n where f_l(k) is the scattering amplitude, k is the momentum, and \\delta_l(k) is the phase shift.\nThe unitarity constraint shows their independence: \n|S_l(k)|^2 = 1 \\quad \\text{or} \\quad S_l(k) = e^{2i\\delta_l(k)}.\n\nThe cross section in terms of partial waves is: \n\\sigma = \\frac{4\\pi}{k^2} \\sum_{l=0}^\\infty (2l + 1) \\sin^2 \\delta_l(k).\n\n\nThese formulas highlight how partial waves are functions of a single variable (k), have fixed quantum numbers (l), and remain independent under unitarity constraints.\n\n\n0.8 Resonance Phenomenon and Partial Wave Expansion in Scattering Amplitudes\nAnd the way we can proceed is to insert this here and then simplify the phase space. For this, you will need one magic formula, and I don’t have time to derive it. It’s actually present in many references. I will send you the details.\nBut there is a really cool relation that I would like you to see: when we insert it there, the initial state is like this. It has the angle zero. That means its state is like this. Then, the final state is like that.\nFor the final state, the Legendre polynomial P_J is a function of \\theta or the cosine. \\theta is the same as the big Wigner D-function for D^J_{00}(\\cos\\theta). Write it first differently.\nWe know the Wigner D-function of J of P vector is still important in Q vector, and it’s equal to D(P) \\times D(Q). So, D^J_{0\\lambda}—such a cool and powerful expression.\nHere, the capital G where \\lambda is 00 is equal to e^{-i\\theta}. It’s just the definition of the capital G function: the first argument is the phase, the second (or last) argument is the phase, but here it’s zero.\n\n\n\n\n\n\nThe Wigner D-function relation shows how angular momentum states transform:  D^J_{0\\lambda}(\\theta) = P_J(\\cos\\theta)  where P_J is the Legendre polynomial.\n\n\n\nAnd then the indices—for this, what happens is that you have to integrate over all possible intermediate steps, and both of the amplitudes are expanded in the partial waves. The way to do this is to expand the cosine between the first and second and the last into the composition of intermediate steps. And that’s exactly the functions we need to relate this amplitude to the partial wave expansion.\nBut the expression we arrived at in partial wave theory—well, you are not surprised by the numerical coefficients because it’s simply the base case. So, what’s done now is to divide the right part by… multiply both parts with \\frac{1}{a_J}, so this vanishes. Then this one is \\frac{1}{a_J^*}. And then the minus N comes from that.\nAnd then the imaginary part of the partial amplitude is just i^{-L}. It’s amazing because it tells us exactly how the inverse amplitude—how the imaginary part of the inverse amplitude—looks like.\n\nSo, let’s maybe plot it. Here’s s. Here is the imaginary part of a_J and the phase space function.\nThe phase space factor we have here is the function that starts from threshold and approaches \\frac{1}{16\\pi} at infinity. At threshold, it has a square root behavior: \\sqrt{s - (M_1 + M_2)^2}. So it starts as a square root but then approaches a constant, \\frac{1}{16\\pi}, or \\frac{1}{8\\pi} \\frac{\\lambda^{1/2}}{s}.\nSince P is a channel function, then this is… and this is something that goes to one at high energy.\n\nAnd now, I think I managed to tell you the last thing: the modeling. We know the imaginary part. We do not know the real part. The real part of the amplitude is something that is genuine to the interaction.\nWhat we have done so far is the same for:\n\nElectromagnetic interactions\nStrong interactions\nGravity interactions—whatever interactions.\n\nAny interaction that has unitarity must have an imaginary part related to the phase space. But the real part of the amplitude is something you have to compute or model.\nPeople working on theory extract the real part of the amplitude by analyzing the lattice data. People working in experiments extract the real part by analyzing the harmonic data.\nAnd the simplest model—or actually, what we have to write—is the inverse amplitude is equal to… some real function that we do a… Is it legal? Is this correct? Yes, any real function.\nAnd then, one of the options for this real function is the pole—one of the simplest approximations. So this is a real function. We take it.\nThe amplitude computed assuming the real part is a single pole is called the Breit-Wigner amplitude, and this is a relativistic Breit-Wigner amplitude.\nNow, I’m looking… so let’s do K-matrix poles—the amplitude that… Actually, modeling the amplitude in terms of pole terms is often called the K-matrix approach.\n\nLet me plot this unperturbed amplitude. Here, I’m going to have the absolute value of A on the Y-axis and L. And M is a parameter that I put in. So these are both real numbers.\nIt gives me the meaning of the least real numbers that I put in my real function. The numerator, the g, determines how broad the peak is going to appear. And then M is often called the bare mass—the mass at which the peak appears.\nAnd that’s the simplest resonance amplitude I can come up with. So this describes a resonant phenomenon, describes the phenomenon where what is inside of the block is an intermediate resonance.\nParticles come toward each other, smash into each other, form an intermediate resonance for a short moment, and then decay. Also, it’s not clear why we call this a resonance phenomenon.\nBut imagine you have two particles, and you can bring them together, let them interact, and measure the probability for them to interact. And you have a handle to tune the energy of the particles.\nSo essentially, in the experiment, you do the following: you smash particles and count in how many cases they interact—they do scatter. Then you’re going to observe something like that, where the X-axis would be the probability for them to interact, and the Y-axis is the energy with which you collide them.\nIf you collide at this energy, the probability to interact is small. But once you tune the energy to the mass of the resonance frequency—the mass of the resonance—you have a huge cross-section, and then it dies out again.\nSo this resonance phenomenon indicates that there is something interesting inside the plot of my interactions—namely, a particle, an intermediate particle that can be formed.\n\n\n0.9 Poles, Zeros, and Unitarity in Scattering Amplitudes\nNow, what happens from the two-part 2 \\times K matrix is the two poles. The cross section looks now something like that. It has two peaks:\n\nThe first peak corresponds to the first pole.\nThe second peak corresponds to the second pole.\n\nThere is a zero in between, which comes from the fact that K vanishes.\n\n\n\n\n\n\nThe lecturer corrects a typo:\n\nOriginally written: K\nCorrected to: K^{-1}\n\nThe average expression should be 1 - \\rho \\cdot (K^{-1}). Then, K multiplies this expression.\n\n\n\nIf K vanishes, the entire expression vanishes, therefore two poles. If you call K itself, it has one singularity and then it gives another set. In order to reach the second singular asymptotics, it needs to cross the zero.\nSo K vanishes in between the poles, and the zero propagates with the amplitude squared. What is this amplitude? Zero, not plus zero. Very good. This is zero on this axis.\n\nWell, another thing to note is that the values of the energy where amplified peaks occur are not exactly these masses. You see, they enter an expression in a rather complicated form. Therefore:\n\nThe peak happens very close to this value.\nBut not exactly at that value.\n\nThat’s the reason why this mass is sometimes called bare masses. This process of putting into the expression is addressing of bare particle masses with the propagator within the dressing process.\n\nLet’s have another look at this expression:\n\nA = \\frac{K}{1 - \\rho \\cdot K}\n\nWhen we expand this in the Taylor series, it’s going to be:\n\nA = K + K \\cdot \\rho \\cdot K + K \\cdot \\rho \\cdot K \\cdot \\rho \\cdot K + \\cdots\n\nWhat we see is an infinite series of terms that allows us to give a meaning to the terms K and \\rho:\n\n\\rho comes from the two-particle phase space and indicates two particles propagating.\nThe K-term is the elementary interaction—K is the point-like interaction.\n\nDiagrammatically:\n\n\\rho is the 2-particle parameter.\nK is the elementary interaction.\n\n\nThe vertex amplitude is a complex function. What we’ve discussed so far is how the length of this complex vector looks like. But what we haven’t discussed is the angle—the scattering phase, the argument of the scattering amplitude.\nThe scattering phase also has interesting behavior. It actually makes a circle in the complex plane.\n\nWhat you see here is that from threshold, the amplitude increases. If you draw it in the complex plane:\n\nIt starts small.\nThen increases to its largest value.\nThen goes down and arrives again at zero.\nThen makes a second shape like that—where the variable A here is the imaginary part of A.\n\nWhat I’m showing here is the function of s. The maximal value is approached around m_1^2. Then there is the second circle that the amplitude makes, completing this shape. This is called an Argand diagram.\n\nThere is more in the homework sheet I’ve set. The Argand diagram is simply plotting the amplitude or amplitude times \\rho.\n\nF = A \\cdot \\rho\n\nThat’s just a more convenient quantity to plot in the complex plane.\n\nTo summarize:\n\nUnitarity is an important constraint that comes from probability conservation.\nIt practically gives us a tool to model the amplitude.\nIt tells us that the imaginary part of the amplitude is fixed, and we only have to worry about the real part.\n\nMoreover, this real part has the meaning of a point-like interaction that has to be resumed to all orders. The way to model this—there are many techniques to model the real part, and one of them is at the end discussed now.\nAnother one is the modeling with the polynomial, and this is called the scattering length approximation. The first term would be the scattering length. The second—so you can write:\n\nK^{-1} = a^{-1} + \\frac{r_s r_p}{2}k^2\n\nand this is called the effective range approximation.\n\n\n\n\n\n\n\nKey formulas discussed:\n\nAmplitude: A = \\frac{K}{1 - \\rho \\cdot K}\nUnitarity condition: \\text{Im}\\, A = \\rho |A|^2\nEffective range approximation: K^{-1} = a^{-1} + \\frac{r_s r_p}{2}k^2"
  },
  {
    "objectID": "2024-Lecture-08.html",
    "href": "2024-Lecture-08.html",
    "title": "(2024) Lecture 8",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Unitarity, Analyticity, and the Schwarz Reflection Principle\nIn today’s lecture, I would like to discuss unitarity and complex numbers, and move towards discussing complex functions. We had too little time in the previous lecture to cover these aspects, but they are important to understand and connect to what you already know from mathematics.\n\nI will introduce two mathematical concepts:\n\nAnalytic functions\nHolomorphic functions (which are interchangeable terms)\n\nA function is called analytic if there is a Taylor series equal to the function in the vicinity of each point. If there are coefficients such that the series gives exact values of the function, then the function is analytic at the point X.\n\n\n\n\n\n\nThe Taylor series expansion for an analytic function at point x_0 is: \nf(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(x_0)}{n!} (x - x_0)^n\n\n\n\n\nA function is analytic at X_0 if the series exists for (X - X_0)^N, with coefficients depending on X_0: \nf(X) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(X_0)}{n!} (X - X_0)^n.\n If the complex derivatives exist—meaning the limit exists when approaching the point from any direction—the function is called holomorphic, which is the same as analytic. Holomorphic is just a fancy word for analytic—mathematicians like cool terms.\n\n\nAll polynomials are fully analytic.\nRational functions (polynomials divided by polynomials) are analytic except at the zeros of the denominator.\nA function is analytic in a domain if every point in that domain is a point of analyticity.\nThe square root function \\sqrt{X} is analytic everywhere except at zero, where it has no derivative or valid Taylor series—this is called a branch point.\n\n\nReal analytic functions are another interesting class. For example, \\sqrt{X} is real analytic. They satisfy the Schwarz reflection principle: when extended to the complex plane, the function evaluated at the conjugate point equals the conjugate of the function: \nf(\\overline{z}) = \\overline{f(z)}.\n\n\n\n\n\n\n\nDemonstration using 1 + i = \\sqrt{2} e^{i\\pi/4}:\n\n\\sqrt{1 + i} = 2^{1/4} e^{i\\pi/8}\n\\sqrt{1 - i} = 2^{1/4} e^{-i\\pi/8} This confirms the reflection principle since \\sqrt{\\overline{z}} = \\overline{\\sqrt{z}}.\n\n\n\n\n\nFor analytic functions of a single argument, it’s useful to visualize the domain as the complex plane:\n\nReal part on the x-axis\nImaginary part on the y-axis\n\nThe Schwarz reflection principle implies symmetry between the upper and lower planes—values in the lower plane can be derived from the upper plane via conjugation.\nFor instance:\n\n1 + i has magnitude \\sqrt{2} and angle \\pi/4\nIts square root is 2^{1/4} e^{i\\pi/8}\nSimilarly, 1 - i gives the conjugate result, illustrating the reflection principle.\n\n\n\n0.2 Cauchy’s Theorem and Numerical Integration of Discontinuous Functions\nI would like to cover Cauchy’s theorem. It states that the integral over a closed contour in the complex plane of a function is equal to all non-analytic contributions inside this contour. If it’s a pole, it’s given by residues. If it’s a branch point, the correct way is to contour around it, and so on—the left side captures all non-analytic behavior.\n\n\n\n\n\n\nCauchy’s Integral Theorem (general form): \n\\oint_\\gamma f(z) \\, dz = 2\\pi i \\sum \\text{Res}(f, a_k)\n where \\gamma is a closed contour and a_k are the poles (or other singularities) inside \\gamma.\n\n\n\nWhat I realized recently is that this is actually a numerically well-defined procedure to integrate a discontinuous function. For example, we can compute the integral of \\sqrt{z} over a circle of radius 1, and the integral converges despite the function being discontinuous at the branch point.\nOn the circle, the function is parameterized as z = e^{i\\phi}. Above the branch cut, it’s +i, and below, it’s -i. If I plot the function in the integration domain, the integral becomes: \n\\int_0^{2\\pi} i e^{i\\phi} \\sqrt{e^{i\\phi}} \\, d\\phi.\n The key point is that within the integration domain 0 to 2\\pi, the function jumps at \\phi = 0 due to the branch cut. Yet, the integral remains well-defined because it’s equivalent to integrating around the branch point.\nLet me emphasize this more clearly. The integral is: \n\\int_0^{2\\pi} e^{i\\phi} \\cdot i \\cdot \\sqrt{e^{i\\phi}} \\, d\\phi.\n This simplifies to: \n\\int_0^{2\\pi} e^{3i\\phi/2} \\, d\\phi.\n However, since the function is discontinuous, I cannot naively evaluate it at the endpoints.\n\nA better approach is to shift the integration range to -\\pi to \\pi, where the function is continuous. Mathematically, this becomes: \n\\int_{-\\pi}^{\\pi} e^{3i\\phi/2} \\, d\\phi.\n Evaluating this properly accounts for the branch cut.\nThe deeper insight here is that this integral equals the integral of the discontinuity across the branch cut. If I shrink the contour, it’s equivalent to integrating \\sqrt{z} along the real axis from 0 to -1. You’ll find the same result—this is the power of Cauchy’s theorem.\nIt tells you that you can deform the contour freely, as long as you avoid non-analytic regions. You can compute the integral along the circle or shrink it to a real-axis integration. For this example, I encourage you to verify that both methods give the same answer.\n\nThe final step is to evaluate the real-axis integral: \n\\int_{-1}^0 \\sqrt{x} \\, dx.\n This is straightforward, and the result aligns with the contour integration. The key takeaway is that even with discontinuities, Cauchy’s theorem provides a rigorous way to compute such integrals.\n\n\n\n\n\n\nEquivalence of Methods: The integral along the branch cut (\\int_{-1}^0 \\sqrt{x} \\, dx) matches the contour integral result, demonstrating the flexibility of contour deformation in complex analysis.\n\n\n\n\n\n0.3 The Scattering Amplitude: Analyticity, Unitarity, and Threshold Behavior\nThe scattering amplitude is the real magic function. Let’s come back to our two sketches in the amplitude, for which we derived the optical theorem and discussed the analytic structure of this function. The claim is that the amplitude is a real magic function, or part of it.\n\n\nAssumptions: The amplitude is not accessed directly in the experiment, so we cannot validate its properties exactly. What we measure is the amplitude squared. Access to the amplitude itself is only available through our observables.\nConstraints from Scattering Theory: However, scattering theory and probability conservation imply that certain properties of the amplitude constrain the imaginary part and tell us where the imaginary part is present.\n\n\n\n\n\n\n\n\nAnalyticity as a Postulate: The fact that it’s analytic is a postulate. This is something we have to assume—though it’s stronger than an assumption. We don’t assume its analytic properties; this is the building principle of our series.\n\n\n\n\n\nUnitarity: All series we have seen so far have unitarity built in. Therefore, not only unitarity but also causality, which is related to analyticity.\nCausality and Analyticity: The amplitude being an analytic function is a principle postulated in our theory. It is related to the causality of the theory—that the past does not influence events outside the causality cone.\n\nThis connection is not derived here, but you can find it in several books linking causality to analyticity. We take it as a postulate.\n\n\nAnalyticity: The amplitude is analytic, and unitarity further tells us that it is real analytic below the threshold.\nImaginary Part and Thresholds: The imaginary part is connected to the appearance of particles in intermediate states. This only occurs above threshold, because the interaction introduces an imaginary part there. Below threshold, the amplitude has no imaginary part; it is purely real.\n\n\n\nOptical Theorem: \n\\text{Im}\\, f(0) = \\frac{k}{4\\pi} \\sigma_{\\text{tot}}\n\nUnitarity Condition: \n\\text{Im}\\, f(\\theta) = \\frac{k}{4\\pi} \\int f^*(\\theta') f(\\theta) \\, d\\Omega'\n\nThreshold Behavior: \nf(E) = \\begin{cases}\n\\text{real} & \\text{if } E &lt; E_{\\text{threshold}} \\\\\n\\text{complex} & \\text{if } E \\geq E_{\\text{threshold}}\n\\end{cases}\n\n\n\n\n0.4 Analyticity, Threshold Singularities, and Unitarity in Scattering Amplitudes\nSo here I have a diagram on the x-axis. Again, it’s a complex plane of the energy. The variable S is E^2, where E is the center-of-mass energy, and the amplitude as a function of S is an analytic function.\nWhat we get to deal with is only the values of this amplitude above the threshold. However, the past tenets of our theory tell you that using analyticity, we can extend the domain of the definition into the full complex plane.\n\n\n\n\n\n\nKey Idea: The amplitude A(s) can be analytically continued into the complex plane, allowing us to evaluate it at complex energies, not just real ones above threshold.\n\n\n\nI hope you wrap your mind around the idea that now we can extend this analytic domain and think of our amplitude as a complex function. So instead of energy of the interactions of, say, 5 GeV, you can put a complex number there and then probe the function away from the real axis.\nNow, this function has a certain range of singularities. From the fact that the imaginary part is not present below threshold and then suddenly appears above threshold tells you that a certain singular disk pops up. And these singularities are the branch points.\nAt every threshold, there are nice derivations of the threshold singularities for different numbers of particles. In Gribov’s book, I leave it out. However, something easy to see is the two-body threshold. And I want to show you that it introduces square root singularities. It simply follows from the fact that the imaginary part has a square root.\n\nSo let me show you that the imaginary part of the amplitude from unitarity that we derived last time is related to the amplitude squared itself. The unitarity condition is:\n\n\\text{Im}\\, A(s) = \\frac{1}{2} |A(s)|^2 \\cdot \\Phi_2(s)\n\nwhere \\Phi_2(s) is the two-body phase space factor.\nThe prefactor here is the phase space—simply two-body phase space. The one-half comes from (A - A^*). So, the imaginary part here—I replace it—and \\frac{1}{2} over here.\nThe two-body phase space has the form:\n\n\\Phi_2(s) = \\frac{2p}{\\sqrt{s}}\n\nwhere p is the breakup momentum. This is something that actually makes a singularity, something that vanishes at the threshold.\n\nThe breakup momentum is the momentum particles have. Clearly, if you are at the threshold, you have a minimal energy of the system. It’s simply two masses of the particles. They don’t have free energy; they don’t have momentum. And that’s something that vanishes, and it vanishes.\nMathematically, you see that if you compute this two-body breakup momentum—I think you’ve done it in earlier exercises—you find that it’s equal to the Källén function:\n\np = \\frac{\\sqrt{\\lambda(s, m_1^2, m_2^2)}}{2\\sqrt{s}}\n\nwhere \\lambda is the Källén function:\n\n\\lambda(x, y, z) = x^2 + y^2 + z^2 - 2xy - 2yz - 2zx\n\nor equivalently:\n\n\\lambda(x, y, z) = (x - (y + z)^2)(x - (y - z)^2)\n\nRemember this: the first term gives you a singularity at the threshold, and the second one gives you a singularity at the pseudo-threshold.\n\n\n0.5 Square Root Behavior and Schwarz Reflection Principle in Threshold Energy Analysis\nInterior relations, as we wrote it, as we derived, are valid only above the threshold. The imaginary part is non-zero and present only once you go above the threshold.\nThis imaginary part behaves like a square root function:\n\n\\text{Im}\\, f(E) \\propto \\sqrt{E - E_{\\text{th}}},\n\nwhere E is the energy and E_{\\text{th}} is the threshold energy. This indicates that the function itself has a square root branch point.\n\nAbove the threshold, the function exhibits this square root behavior. It would be useful to connect this to the Schwarz reflection principle.\n\n\n\n\n\n\nSince our function was real and analytic on a segment of the real axis, the Schwarz reflection principle applies. It relates the amplitude in the upper half-plane to the amplitude in the lower half-plane:\n\nf(E + i\\epsilon) = \\overline{f(E - i\\epsilon)},\n\nfor real E and small \\epsilon &gt; 0, where \\overline{f} denotes the complex conjugate.\n\n\n\n\nThis means the imaginary part flips sign when you cross the real axis:\n\n\\text{Im}\\, f(E + i\\epsilon) = -\\text{Im}\\, f(E - i\\epsilon).\n\nThe relation holds for \\epsilon small enough.\n\n\nThe square root behavior of \\text{Im}\\, f(E) is tied to the threshold energy E_{\\text{th}}.\nThe Schwarz reflection principle ensures symmetry between the upper and lower half-planes.\nThe sign flip in the imaginary part reflects the discontinuity across the real axis.\n\n\n\n0.6 Analytic Structure and Branch Cuts in Square Root Functions\nHave you heard a song with the word “infinitesimal”? I think it’s by one of the rock bands. I was really impressed. This is now in pop, in rock. People like the word. Maybe next time I’ll bring you something. I was impressed recently. Who is singing “Infinite”? You know, Mother Mother. That’s about Mother Mother. So listen to the song “Infinitesimal.” I like the song actually, but it just means very, very small—infinitesimal.\n\nI’m going to use this i\\varepsilon, and you’ve seen it before. This is just the little number that indicates you are like a very, very small amount above the real axis or below the real axis. It happens due to the phase space. We realized that if the function had no phase space configuration summations on the right, you could relate the imaginary part to the real part itself, and there would not be branch points. But it’s unavoidable. You have to sum over phase configurations, and that gives you the square root branch point. It also tells you that the amplitude has a branch point starting at the threshold and going in the positive direction.\n\n\n\n\n\n\n\nThe i\\varepsilon prescription is a common technique in complex analysis and quantum field theory to handle poles and branch cuts by shifting the contour infinitesimally away from the real axis.\n\n\n\n\nI wanted to quickly give an example of such a function—an example of a real analytic function, discarding this cut to the right. An example of a real analytic function like our scattering amplitude, but something very simple with a square root function that has a cut to the right. So \\sqrt{x} doesn’t work—its cut is on the left. Let’s cut to the left. How do I see where it has the cut? Simply because \\text{Amplitude}(1 + \\varepsilon) = \\text{Amplitude}(1 - \\varepsilon), there is no cut. Then, \\sqrt{-1 - \\varepsilon} = -i, and \\sqrt{-1 + \\varepsilon} = i. On this side, the function evaluated from above and below have different values. That tells me in which direction I put my cut.\n\nThis is a branch point, and this is a non-analytic point. But for every branch point, there is a cut attached, and it splits my manifold. Analyticity into the functions—into the surfaces—differentiates. Everywhere else except this point, the functions are analytic. But you have to work to see this analyticity. Essentially, you have to analytically continue functions. If you see the pattern, you want to make this function analytic. It’s always possible, but you have to work a little by extending the domain of analyticity.\n\nComing back to the example, you’ll understand it better once I finish this. If I take -\\sqrt{x}, what was the analytic structure of -\\sqrt{x}? According to the same logic, I can evaluate the function somewhere in the complex plane and find that it’s now analytic here, fully analytic, and it has a cut to the right. Now, let’s evaluate this function at -1. What does it equal? So I evaluate my function here, and it gives me \\sqrt{1}. This is an example of a real analytic function. Or let’s take the simplest amplitude: the amplitude \\mathcal{A}(s) = \\sqrt{-(s - m_1^2 - m_2^2)}, which has similar properties. It has similar properties.\n\nQuickly, while I’m cleaning the board—what is the analytic structure of this? This is in the x-plane. The first one I put as -9 to make it more interesting. This first one has the cut on the right, and the second one has a cut—it’s very similar but shifted by one. Essentially, it has this structure: one cut, then a second cut. When you have this situation, there is no jump here. You can jump twice, and the cuts overlap, so there’s no jump in the function. It’s analytic right here. The correct answer is just the cuts connecting two points.\n\nThis one is tricky. The first time I saw it in a mathematical context, I was really shocked because the branch points are the same. There are two points, x and x, where the function is non-analytic. But the locations of the cuts are determined by where the expression under the square root has an argument of \\pi or -\\pi. For a regular square root function, the cut appears where the argument is either \\pi or -\\pi. To understand where your Mathematica, C, or Python library would draw a cut, you need to know where the expression under the square root has an argument touching \\pi or -\\pi.\n\nAnd then, apparently, this is indeed between the points but also along this line. This is the first cut going to infinity, and this is the second cut going to infinity. It’s crazy. And this is actually different from this. If you just split the product into two square roots, then the argument—then you remove these two. It’s not to confuse you, but to warn you that you must be careful how you write the expression, exactly how you split the square root, because that makes the function different. On the real axis, I think they will be the same, but on the complex plane, the functions are different.\n\nTo prove to you that this is not equal to that, we can simply evaluate the function at -1. I think that’s an easy exercise. The way I see that they are different is because I know the analytic structure of this: one cut and then a second cut. There are just two branch points connected by the cut. To go from this to that, I can take this cut, rotate it over here, take this one, rotate it over there, and cancel the two cuts—make them cancel each other. That tells me the function in the original configuration has a different value at -1. If I evaluate it, I should see this immediately.\n\nIf I evaluate the top one, I get \\sqrt{(-2)(-1)}. If I evaluate the second one, I get \\sqrt{2} \\cdot i. Then the product of them gives me -\\sqrt{2}.\n\n\n\n\n\n\n\nBe cautious when splitting square roots in complex analysis: \\sqrt{ab} \\neq \\sqrt{a} \\cdot \\sqrt{b} in general due to branch cut ambiguities. The example shows \\sqrt{(-2)(-1)} \\neq \\sqrt{-2} \\cdot \\sqrt{-1} because of differing branch cut configurations.\n\n\n\n\n\n0.7 Resonances as Poles in the Scattering Amplitude: A House Analogy for Complex Plane Dynamics\nIts resonances are poles of the scattering amplitude. When you hear that, you’re probably going to hear this many times in the future.\nI would like you to think of the intensity flow. The function in the complex plane is the complex structure of a house. In this house, there are just a few routers. You desperately want your Internet—the farther you are from the router, the weaker the signal you get. If you sit at a point and have really good Internet, you’re probably sitting near the router.\nSimilarly, this is our couch in the house where people usually sit and experience different strengths of the Internet. The routers are the resonances. Here I have combined two modes on the Y-axis. On the complex S-plane, the real s is on the axis, and this indicates minus imaginary s. This is the same as I would have for the complex frame.\nI draw a square representing the strength of your Internet when you sit on this couch. In the middle, you have the closest distance to the router, the resonance pole, and therefore your strength is the highest. If you sit far away, your Internet weakens.\n\nNow let’s make the model more complicated. We have several layers, and this is given by the cuts. Here is my map of the room. As usual, I am the source where people sit on the real axis. But in this case, the model is more complex. There is a room with different electronics, different floors, so I can go to another room.\nI experience all the routers in another room. If there is a router sitting here in another room, I want to see its influence from my car. This is floor one, and the next is floor two. Sheet one is floor one, and sheet two is floor two. Here’s the gate—think of it as stairs from one floor to another.\nIf I have my router here, I have really good strength here and here as well. But if I go around the corner, I start losing the signal. The distance becomes large, and the way to get Internet is around the wall. This is the gate to another floor, and I can only enter on that side.\n\n\n\n\n\n\n\nKey Insight: The resonances are the resonances. I see them on each floor. If they are above the threshold, it’s too complicated.\n\n\n\nWhen you hear about the complex plane and the poles, think of the intensity floor somewhere in the complex domain. This is all complex domain. There are gates to other floors—don’t think of them as changing the level, but as connecting one to another. On this surface with different levels, you place the resonances. These resonances influence your intensity, which you see on the detectors.\n\nWhat is the difference between a bigger room and multiple floors? No difference. The rooms are infinitely big. But since at every point I can be on different floors connected smoothly, I have to draw several maps, several sheets. In shopping malls, you also have multiple floors with shops indicated because it’s hard to show everything on a single one.\nFor simplicity, I would use the term “branch cut,” which tells us the cut should be to the right. But I would never do it practically like that. The branch cut tells me that, but nothing happens if I place the cut to the left.\n\nI will demonstrate with the square root function. It has a branch cut, and I’m carrying my function value here. There is nothing happening if I draw the cut in different directions. This function is the same function as before.\nAt s = -1, you have i. At s = -1 + \\epsilon, you have i. This function and that function are exactly the same in this dimension. The difference is that I took my branch cut and rotated it to the right, opening the space underneath.\n\nI just made a renovation and changed how the rooms are located in the house. But it changes nothing in the strength of the interaction because there are no walls. Everything is continuous. The cut is just a way to separate different floors. There is no wall preventing the signal strength.\nFor practical reasons, it’s convenient to open up the closest room to us. Therefore, it’s more natural to place the cut in a convenient location. The cut goes to the right, but I turn it to the left. Now my function is not real, but I expose a bigger range of the complex plane to analyze.\n\nIf I sit here on the real axis, this point is influenced by all singularities in the complex plane that are nearby. If I have a pole here, I don’t have to care about the rooms anymore. I immediately see the effect in the strength of my interaction if my router is nearby. This is the most convenient way to think of the complex plane and the scattering amplitude.\n\nI measure my amplitude above threshold at s = (M_1 + M_2)^2. What I measure is influenced by structures near threshold. If there is a resonance in the data, it means there is a pole below that makes this resonance. A resonance is always a pole—this is how we define particles.\nParticles are always poles in the amplitude. We have conventions for defining the width and mass of the resonance, and they come from the pole location. A single-pole amplitude has the form:\n\nA(s) \\sim \\frac{1}{s - s_{\\text{pole}}}\n\nwhere s_{\\text{pole}} is the complex pole location. The denominator vanishes at s = s_{\\text{pole}}, which is a complex number. The real and imaginary parts are called the mass and width of the pole.\nWe define the mass as \\Re\\sqrt{s_{\\text{pole}}} and the width as -2\\Im\\sqrt{s_{\\text{pole}}}. These are related to the observed mass and width. If you see a signal, it has a peak location and a full width at half maximum. For narrow resonances, \\Gamma \\approx \\text{FWHM} and m \\approx \\text{peak location}. For broad resonances, this is not the case.\n\nWhat is the analytic structure of this function? It is simply a pole. There are no branch points, nothing else. A slightly more complex example is the K-matrix, which incorporates poles and correct analytic structure, including square-root branch points from phase space:\n\nA(s) = \\frac{K(s)}{1 - i\\rho(s)K(s)}\n\nIt works for multiple loops.\n\nThe last comment is on data analysis. The goal is to characterize resonances by their pole location. We use parameterizations like the K-matrix or P-vector. A simple Breit-Wigner is a limit case of the K-matrix. The amplitude has a complicated expression but is analytic.\nYou fit parameters to the data, then explore the analytic expression. You know it has a branch point, but the direction of the cut is your choice. What’s important is that everything you see—peaks, spikes—has an origin in the complex plane. If there’s a peak, there’s likely a pole there.\n\n\n0.8 Thresholds, Branch Points, and Poles in Amplitude Analysis\nAnother important phenomenon you see sometimes in the data is the cusp. This is amplitude squared, this is S. And then you see a cusp. That type of singularity is also known as a branch point.\n\nThe first threshold introduces a branch point—every threshold introduces a singularity. For two particles, it’s a branch point. We already discussed this branch point. But if there are more than two particles, combinations like \\pi\\pi, KK, or K\\bar{K} would give a branch point at 1 GeV.\n\nThen my amplitude might have a spike like this. It’s an indication that in the complex plane all is fine, but there is a branch point. For every branch point, I have to attach a tail. This is the location of my cut—it’s up to me how to draw it—but it introduces more surfaces. This is a direction.\n\nThe last thing to realize is that this is a triangle. We know that at threshold the function has square root singularities. But thresholds are only opening new surfaces for you. Thresholds are only determining the map of your complex plane.\n\nThe real singularities, or the strong singularities that make intensity peak, are poles. So this situation, where your amplitude squared has this threshold behavior but then rises very quickly and falls, indicates that there are some poles nearby. If the poles were underneath—if the poles were like that—that kind of function would peak at the place.\n\nBut in that case, what likely happens is that there is a bound state, a pole here. If there is a pole on the real axis, it would not show up as a nice resonance-like peaking structure.\n\n\n\n\n\n\n\nThreshold condition for two-particle production: \nS = (M_1 + M_2)^2\n where S is the Mandelstam variable (center-of-mass energy squared), and M_1, M_2 are the masses of the two particles.\n\n\n\n\nThink again about the map of routers and the internet. If you have a router here and you sit on the couch, the signal strength will be highest here and then lower, lower, lower there. That’s simply how to understand threshold enhancements.\nThreshold enhancements are often indications of poles below threshold—these are bound states. There is another phenomenon: poles at the same locations but underneath another sheet, called a virtual state. These are not that different from each other.\n\nThe bound state can live forever—it’s a real state that can travel, a particle that does not decay. The virtual state does not travel and does not decay. This is simply an enhancement.\nThis is a distinction—one of the objectives and discussion points in the field. When you observe a new structure, what kind is it?\n\nIs it related to a threshold?\nIs it a pole sitting below a certain threshold?\nOr is it a resonance in the complex plane unrelated to a threshold?\n\n\nRelation to a threshold indicates a molecular nature. Remember, every threshold has two masses summed—it implies a continuum. There is one particle and a second particle interacting. This is our threshold.\nIf there is a pole related to a threshold, likely part of the wave function for this state is of this type—a molecular type. That’s why identifying all thresholds and the complex structure, and where the resonance pole sits, is so important.\n\nFinishing the lecture—quick questions:\n\n“S - (M_1 + M_2)^2.”\n“The other so-called threshold—what is what?”\n“With the so-called threshold S - (M_1 - M_2)^2?”\n“Yes. What’s with this one?”\n“It’s a ‘zeta threshold.’ It appears artificially in our expressions because of the breakup momentum. But if you look at the amplitude, it’s not supposed to have a singularity there.\n\n\nYou cannot literally take this expression and continue at the threshold. This is a sort of false threshold. We don’t see this. It tells you this is an indication that you should not take this expression literally and build it into your model.\nInstead of using phase space—\\frac{1}{8\\pi} \\frac{2p}{\\sqrt{S}}—it’s actually the imaginary part of this bubble. The imaginary part.\n\nYou see this simply from unitarity: once you cut this diagram, the imaginary part would be equal to the matrix element here and there, which are both 1, and the two-body phase space because you cut two lines. Here is the two-body phase space.\nThe ‘surface threshold’ here is present because you use only the imaginary part in your amplitude. If you were to take the full bubble diagram, you don’t have a surface threshold there.”"
  },
  {
    "objectID": "2024-Lecture-09.html",
    "href": "2024-Lecture-09.html",
    "title": "(2024) Lecture 9",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Personalized Lectures, Homework Encouragement, and Flexible Learning\nSimilar to courses you’ve had, there were always two of us in the lecture, so it was a personalized education. I remember my teacher saying that the quality of the lecture was measured by whether Mikael was sleeping or not.\n\n\n\n\n\n\nEngagement tip: Use physical cues like eye contact or note-taking as indicators of lecture effectiveness, just like Mikael’s wakefulness served as a quality measure.\n\n\n\nIf he was sleeping, the lecture was bad, and if he was awake, the lecture was good. So you’re not sleeping—follow along. Probably the lecture is good.\n\nIt was a little bit funny. We would come to the professor’s office. He had a little board, and we would sit in front of his chair, sketching things on it. Not so many people were there.\n\nHomework discussion:\n\nHave you done the exercise sheets?\nYes, but I don’t have anything like I did, at least the first exercise.\nYou cannot—because Eva is not here today.\nBut I could have collected them.\n\n\n\n\n\n\n\nSubmission reminder: Always bring completed exercises even if collection isn’t scheduled - you never know when they might be accepted.\n\n\n\nVery good—you brought something. I encourage you to hand them in, in addition to looking at them. It’s really good that you looked.\n\nExam requirements:\n\nWe proved and wanted to report points on exams\nWe set the admission criteria at a low 50% of points\nBut you’re not there yet—none of the students are there yet\n\nAction items:\n\nPlease work on the homework\nIf you want to have enough at the end, we might allow for:\n\n\nA lower number of points\nOr let you submit exercises later\n\nThis is something to fulfill, and I would like you to fulfill that.\n\nAbout the exercises:\n\nThey have been non-standard in the sense that you don’t find them in books\nAllowed resources include:\nCourse materials\nChatGPT help\nEverything - just like in research\n\n\n\n\n\n\n\nProblem-solving approach: While the exercises aren’t textbook problems, they’re designed to be solvable with available tools and require creative thinking rather than rote memorization.\n\n\n\nHave a look. They are not that difficult, as you probably discovered, but they require some thinking.\n\n\n0.2 Transition from Unitarity to Hadron Spectroscopy: Models and Lecture Schedule\nToday we are finishing with unitarity and moving toward discussing approaches used in hadron spectroscopy. A few lectures in the future, I want to dedicate to the field—the approaches from theory and experiment, mostly from theory—that use hadron spectroscopy.\n\n\n\n\n\n\nThe unitarity condition for the S-matrix is a foundational constraint: S^\\dagger S = SS^\\dagger = I This ensures probability conservation in scattering processes.\n\n\n\nSo far, we have discussed the Lorentz group, helicity states, and angular distributions, which are model-independent. We also discussed unitarity, another model-independent approach that provides a guiding principle rather than a definitive answer.\n\nNow we will move toward models. If you have a Lagrangian, you can derive everything. The models that approximate Lagrangians—we’ll discuss:\n\nChiral Perturbation Theory (ChPT): A low-energy effective field theory. The leading-order Lagrangian is: \\mathcal{L}_{\\text{ChPT}} = \\frac{F_\\pi^2}{4} \\text{Tr}[\\partial_\\mu U \\partial^\\mu U^\\dagger] + \\frac{F_\\pi^2 B_0}{2} \\text{Tr}[M(U + U^\\dagger)] where U is the chiral field, F_\\pi the pion decay constant, and B_0 relates to the quark condensate.\npNRQCD: A non-relativistic effective field theory for charmonium states. The Hamiltonian for heavy quarkonia is: H_{\\text{pNRQCD}} = \\frac{\\mathbf{p}^2}{m_Q} + V(r) + \\mathcal{O}(1/m_Q^2) where m_Q is the heavy quark mass and V(r) is the QCD potential.\nLattice QCD: We’ll explore its methods and applications in hadron spectroscopy. The action for Wilson fermions is: S_{\\text{LQCD}} = \\sum_{x} \\bar{\\psi}(x) D_W \\psi(x) + S_G[U] with D_W as the Wilson-Dirac operator and S_G the gauge action.\n\n\nI would like to spend one more lecture on B meson and D meson physics, where hadron physics plays a crucial role. While these studies often focus on CP violation and searches for new physics beyond the Standard Model, hadron physics provides essential input on strong interactions.\n\n\n\n\n\n\nThe weak decay amplitude for B \\to f transitions is: \\mathcal{A}(B \\to f) = \\langle f|\\mathcal{H}_{\\text{weak}}|B\\rangle = V_{ub} V_{uq}^* \\mathcal{M}_{\\text{strong}} Here, \\mathcal{M}_{\\text{strong}} encodes hadronic matrix elements, emphasizing the need to understand strong phases.\n\n\n\nDetermining weak phases almost always requires knowledge of the strong phase of the transition. Understanding how hadrons form and their properties is key.\n\nI think we have four more lectures to go, possibly fewer. I plan for four, aiming to finish in the first week of July. The 16th is a spare lecture in case we need additional discussion. But the 9th of July is marked as the last lecture on my calendar.\nDoes the semester usually run until mid- or late July? I forget the exact schedule. In my calendar, the semester extends until the third week of July. So the 9th, 16th, and even the 23rd are still part of the semester. However, a research school in Bochum will occupy our time, so we aim to finish one week earlier.\n\n\n0.3 Unitarity Constraints on Partial Amplitudes and the Argand Diagram\nToday’s lecture—lecture number one—will start with the theoretical unitarity. We’ll discuss the threshold expansion, particularly amplitude behavior at the threshold, as well as the scattering length approximation. Then we’ll move to effective field theory (EFT) and compare EFT and unitarity. Finally, we’ll cover ππ scattering in S-wave, where the current perturbation theory gives us the scattering amplitude.\n\nThe first question is: Which values are allowed for the transition as partial amplitude F = T \\rho, where T is the scattering amplitude?\n\nConsistency is important. The scattering amplitude is the scattering amplitude, and this is the partial amplitude.\nIf you don’t have any clue for the first one, you might just—if you don’t do homework, you have no clue for the first one, probably.\nThe second one is standard, something we discussed many times.\n\nLet’s start with number two. First, each partial wave—you can just speak aloud. What are your thoughts? What do you think?\n\nWe noted last time that we want the function to be analytical on the right-hand side, and A isn’t analytic in this region. If I put F plane, where is F? It’s 3 - 2A. This one is here, somewhere at 45 degrees. And this one is -1.\nIt was just the golden ratio but with an i in it. No, it’s not the golden ratio, but I thought it was 1 + \\sqrt{2} divided by—it’s \\sqrt{5}/2 for the golden ratio. That would have been a point where I could have said it’s just invented by you. But it’s important to understand the principle—what’s allowed when you talk about something being allowed.\n\n\n\n\n\n\n\nUnitarity and the Argand Diagram\n\nUnitarity tells us that the imaginary part of the amplitude is driven by the interaction loop.\nThe imaginary part of the inverse amplitude is given by:  \\text{Im}(T^{-1}) = -\\rho \nThe amplitude can be written as:  T = \\frac{e^{i\\delta} \\sin \\delta}{\\rho} \nFor elastic scattering, the amplitude lies on a unitarity circle in the complex plane.\n\n\n\n\n\nThis is about unitarity, and I would like to use unitarity here. Last time, and also in the homework exercise sheets, we’ve seen the Argand diagram. This is how we determine the values of the amplitude when you plot them in the complex plane.\nFor regular resonances, the amplitude makes a circle. Unitarity tells us that the imaginary part of the amplitude is driven by the interaction loop, and it’s only determined. The imaginary part of the inverse amplitude from this relation is simply -\\rho, and the amplitude can be written as an inner real function—a real function without singularities—minus \\rho. From that, once you plot the amplitude, you end up on a circle.\nHow do you see that? This is A in terms of F, or this is T, and what I want is atoms. So my F is A \\times \\rho. We haven’t done this last time, but it’s a straightforward exercise to see that such an amplitude is actually sitting on the circle. Only values that the amplitude can have are on the circle.\nTo see that, one takes this general form and divides by \\rho, replaces \\rho by its form in terms of \\delta, and ends up with something very simple: \\sin \\delta \\, e^{i \\delta}. This is the only possible value of W. You can write it differently: e^{2i \\delta}/2 - \\dots.\nWhen \\delta = 0, this is zero. When \\delta reaches its maximum value, which is \\pi/2, this is 1, and e^{i \\pi/2} = i. So it reaches the maximum value at i.\nWhen \\delta = \\pi/2, this is e^{2 \\pi i} = -1. But something doesn’t work. Between this and this, I made a mistake. The numerator is i/2, and then you have 1 - e^{2i \\delta}. This is straightforward to see what’s happening. You are sitting—and if this distance term were absent, you’d have i/2. But this term gives you a shift in the delta direction, and you move along this circle.\nHere is \\delta = 0, here is \\delta = \\pi/2, and then you move on. Here is \\delta = \\pi. This is the Argand diagram.\n\nFor the elastic amplitude—where you have only one channel—all allowed values of the amplitude are located along this circle. There are no other possible ways. Among this list, none of them are sitting on the circle. For the single scattering amplitude, none of them would be allowed values from unitarity.\nFor non-elastic scattering—where particles can transition to other channels—values inside the circle are also allowed. Unitarity tells you that for elastic channels, only the circle is allowed. For inelastic scattering, all values inside the circle are allowed.\nYou look at the list and quickly see which are inside, which are not. I’ve already put the values on the complex plane. A is not, B is in. To arrive at the center, you need some inelastic channels. It cannot be \\pi \\pi to \\pi \\pi. It must be—below the first elastic threshold, the next one is outside. But to make it inside, I think if I put it over eight, this is inside, so B is also valid. D is here, and it’s even in the negative domain. I have to modify something significantly to make it inside. Put a plus here, perhaps. Then I’m not sure if it sits inside or outside.\n\nQuestions? \\delta is the scattering phase, a really important quantity in quantum mechanics. You consider a plane wave that interacts with the source, and there is a spherical wave with the scattering phase. This is exactly the same phase. It determines the scattering.\n\nIf \\delta = 0, that is essentially trivial—no scattering.\nThis phase determines the strength of your scattering.\n\n\n\n0.4 Unitarity and Partial Wave Expansions in Scattering Amplitudes\nIs this representation correct? Isn’t this just the T-matrix represented as the S-matrix? This “$ (1 - e^{2i }) $,” so just this.\nIn my bachelor’s, I’ve seen it or used it as the on-shell representation of the T-matrix. But it’s not. Not this.\nFor the S-matrix, you have just the phase, no sign. That’s why I said it’s the T-matrix represented as written. Yes, that’s exactly the T-matrix that I discussed.\nNow, this T is the expectation of the T-matrix. Because there’s also the physical factor \\rho in it, but maybe it just dropped out. Right, exactly.\nIn quantum mechanics, the same thing appears, but instead of \\rho, we have K, the wave number for the breakup momentum, which is also part of \\rho. The factor is slightly different, but P is the breakup momentum. That’s what you see in the quantum mechanics derivation.\nThere is no “$ $.” This is a relativistic factor that only appears in relativistic scattering. Overall, it’s a normalization constant. But you’re absolutely right.\nFrom that, you also see the T-matrix. The relation between them is S = 1 + i \\rho T. So the S-matrix is S = 1 + 2i \\rho T. When you substitute this, you should have just e^{2i \\delta}. Then you see that unitarity is satisfied with S^\\dagger S = 1.\n\n\n\n\n\n\n\nKey formulas discussed:\n\nT = \\frac{i}{2}(1 - e^{2i\\delta})\nS = 1 + 2i\\rho T\nUnitarity condition: S^\\dagger S = 1\n\n\n\n\n\nQuickly, exercise number two: in which case does this happen? For the sake of time, we should move on.\nThe first one applies to all of them. To determine that, you do simple spin algebra:\n\n0^- \\rightarrow 1^- is a P-wave.\n2^+ \\rightarrow 0^-.\n1^- \\rightarrow 0^-.\n\nHere, 2^+ \\rightarrow 0^+ in S-wave would be 2^-. But the answer is… Oh, that’s interesting.\nFor the first one in the P-wave, these two combine to 1^-, matching the quantum numbers of the \\rho meson. The \\rho meson is 1^-, and this decay conserves parity.\nThe second one is electromagnetic. If you combine 2^+ and 0^+ in S-wave, you get 2^-. In D-wave, you can have 0^-, 1^-, 2^-, 3^-, or 4^-. But the D meson has quantum numbers 1^-, so this matches.\nImportantly, this decay can also happen in different partial waves. No—wait, it cannot. This decay doesn’t have to conserve parity for this transition. Both 0^- and 0^+ transitions are possible because it’s a weak decay. From this, you get S, but it’s not the only answer. Another possibility is 1^+. With two units of orbital angular momentum, you can get 1^+, 2^+.\n\nI would like to motivate the next subject: how the amplitude A(s) behaves near particle thresholds. For A(s) = M_1 + M_2, we are still discussing general properties.\nLet’s look at the partial wave expansion for a particle with spin j and helicity \\lambda decaying into two scalar particles. We can rewrite the decay using Wigner rotations, matching the alignment of the final states. The initial state has spin projection \\lambda along the z-axis, and the Wigner d-function depends on the helicity difference of the final particles.\nIn the rest frame, the spin projection is \\lambda, and the decay angle is \\theta. Let’s examine \\lambda = 0 and the spherical harmonics.\nFor l = 1, it’s \\cos \\theta. For l = 2, it’s \\frac{3}{2} \\cos^2 \\theta - \\frac{1}{2}. And the third one is \\frac{3}{2} \\cos^2 \\theta - \\frac{1}{2} - 1, right? No, it’s just \\cos \\theta. For 2 \\rightarrow 2, it’s straightforward.\n\n\n\n\n\n\n\nPartial wave expansion for spin j and helicity \\lambda: \nA(s) \\propto \\sum_j (2j + 1) d^j_{\\lambda,0}(\\theta) \\mathcal{A}_j(s)\n where d^j_{\\lambda,0}(\\theta) is the Wigner d-function.\n\n\n\n\nThe spherical harmonics for \\lambda = 0 are:\n\nY_1^0(\\theta) \\propto \\cos\\theta\nY_2^0(\\theta) \\propto \\frac{3}{2}\\cos^2\\theta - \\frac{1}{2}\n\n\nThe breakup momentum P in relativistic scattering is given by: \n\\rho \\propto P = \\frac{\\sqrt{(s - (M_1 + M_2)^2)(s - (M_1 - M_2)^2)}}{2\\sqrt{s}}\n\n\n\n0.5 Kinematic Singularities and Angular Momentum Compensation in Scattering Amplitudes\nEarlier we discussed that there are invariant variables described by Mandelstam variables: S and T. It is often convenient to express the cosine of the scattering angle in terms of these variables. One can use either the basis of the scattering angle and the energy \\sqrt{S}, or work directly with Mandelstam variables, as they are related. Thus, \\cos \\theta can be expressed in terms of T and S.\n\n\n\n\n\n\nThe relation for cosine of the scattering angle is: \\cos \\theta = f(S, T) where f(S, T) is a polynomial in T and S.\n\n\n\nFor this decay process, we don’t have T. But for the point I would like to make, let’s imagine that this is produced by an external reference—a two-particle scattering that gives us an extra vector to calculate T. If \\cos \\theta is expressed in terms of S and T, one finds that it is essentially a polynomial: a first-order polynomial in T and a second-order in S, divided by the Chew-Mandelstam functions, which are the breakup momentum of the particles in the S-channel.\n\nFor the S-wave, there is a constant factor. For the P-wave, the breakup momentum appears in the first power in the denominator. For the D-wave, it appears in the second power. To compensate for the p^J dependence in the denominator from angular momentum, this factor must appear in the numerator of the partial wave. This ensures the total amplitude is free of kinematic singularities at the threshold. The amplitude does not fully account for the threshold behavior p^{2L}, so this compensation is necessary.\n\n\n\n\n\n\nBreakup momentum dependence for partial waves:\n\nS-wave (L = 0): No explicit p dependence\nP-wave (L = 1): \\frac{1}{p}\nD-wave (L = 2): \\frac{1}{p^2}\n\n\n\n\n\nThis is often seen in phenomenological constructions. When fitting data, the recipe includes p^L in the partial wave (for L-wave) because it compensates for the angular momentum dependence, which scales as 1/p^L. This is important. For example, in the parameterization of the \\rho meson decaying to \\pi\\pi, the p^2 factor appears twice: once in the numerator to compensate for angular dependence, and once in the denominator in the width parameter, because the width sums over the \\pi\\pi scattering phase space.\nThis leads to a parameterization where, at every vertex, there is a p^1. The p^{2L} arises from vertices due to the p^2 factor, and another from phase space. For the \\rho meson transition, this results in p^3 appearing in the denominator of the width parameter.\n\n\n\n\n\n\nRho meson width parameterization (P-wave decay, J = 1): \\Gamma_\\rho \\sim p^3\n\n\n\n\nAnother example is the D meson decay, where p^L also appears. The D^* \\to D\\pi decay is in P-wave, so there is always a momentum factor. Often, when constructing amplitudes, the kinematic-singularity-free amplitude \\hat{A} is introduced, defined as \\hat{A} = A \\cdot (2p)^J for L-wave. This replacement introduces the factor (2p)^{2J} on the right side.\n\n\n\n\n\n\nKinematic-singularity-free amplitude: \\hat{A} = A \\cdot (2p)^J \\quad \\text{(for L-wave)}\n\n\n\n\nI need to clarify the difference between resonance production and particle scattering. Typically, A denotes the production amplitude, while T denotes the scattering amplitude. The kinematic-free amplitude reduction ensures T_J is free of kinematic singularities. The key difference is that T involves the same initial and final states (e.g., \\pi\\pi \\to \\pi\\pi), while A involves different states (e.g., X \\to \\pi\\pi).\nFor T_J, the factor is (2p)^{2J}. The amplitude here refers to scattering, not production. In the production amplitude, p and q are distinct, but for identical initial and final states, they are the same, leading to p^{2J}. The denominator’s parameterization describes how particles scatter, which is why the factor (2J + 1) appears.\n\n\n\n\n\n\nPartial wave expansion:\n\nScattering (\\pi\\pi \\to \\pi\\pi): T_J \\sim (2p)^{2J}\nProduction (X \\to \\pi\\pi): A_J \\sim (2p)^J\n\n\n\n\n\n\n0.6 Scattering Phase Shift and Amplitude Near Threshold\nLet’s think how to derive this in the quickest way.\nIt is straightforward to see that the phase shift \\delta near threshold behaves as:\n\n\\delta \\propto p^{2L + 1}\n\nTransforming this to \\sqrt{s}, we get:\n\n\\delta \\propto (\\sqrt{s} - (m_1 + m_2))^{2L + 1}\n\n\nFor S-wave scattering, the amplitude is constant at the threshold, as is the scattering amplitude. We can write this as:\n\nA = \\frac{1}{R - i \\rho}\n\nFor the S-wave at threshold, \\rho vanishes because \\rho has the momentum in the first power, leaving only the regular function R.\nThe value of R at the threshold is called the scattering length. Let’s denote it as \\alpha (or a).\n\n\n\n\n\n\nScattering Length Approximation: Replace the unknown R function by a constant. This is valid for all amplitudes near the threshold, where R behaves as a constant.\n\n\n\n\nFor some amplitudes, this approximation holds even farther from the threshold, especially if no other singularities influence the amplitude. There are two interesting cases:\n\nR &gt; 0\nR &lt; 0\n\nThese lead to distinct phenomena.\n\nAs a function of s, \\rho starts at the threshold and grows like:\n\n\\rho \\approx \\frac{1}{2} \\sqrt{s - (m_1 + m_2)^2}\n\nNear threshold, it behaves as \\sqrt{s}.\nBelow the threshold, \\rho becomes imaginary. For s &lt; (m_1 + m_2)^2:\n\n\\rho = i \\sqrt{(m_1 + m_2)^2 - s}\n\nSo -i\\rho has a negative imaginary part.\n\n\nIf 1/a is negative and small, there is a zero in the denominator.\nIf a is positive, there is no pole in the physical region. The function continues downward, with \\rho negative.\n\nThe square root is a two-valued function, creating a branch point and a cut. For every point, the square root can be evaluated with:\n\nPlus sign (on the first sheet)\nMinus sign (on the second sheet)\n\nFor a &gt; 0, there is still a pole, but it is hidden—only visible if you go under the cut. This is the virtual state: a pole below the threshold on the second Riemann sheet.\n\n\n\n\n\n\n\nKey Takeaway: The scattering length approximation simplifies the amplitude near threshold, but the behavior of \\rho and the pole conditions reveal deeper structures like virtual states.\n\n\n\n\n\n0.7 Scattering Length, Bound States, and Effective Field Theory in Threshold Scattering\nThe scattering length is an important quantity that is measured from any scattering experiment in nuclear physics. This is one of the good observables that people use. This quantity characterizes the strength of the matrix element at the threshold.\n\n\n\n\n\n\nNear threshold, the transition matrix element simplifies to:  T \\approx A  where T is the transition matrix and A is the scattering length.\n\n\n\nAs you see, you just evaluate T at the threshold, this vanishes and T is literally equal to A. But if the validity of the scattering length approximation extends a little further, you find that for large values of A, you can get a bound state near the threshold.\n\nThis bound state is the example of the deuteron. The deuteron, which is formed by the scattering of a proton and neutron, is a bound state of the proton and neutron. It is a good example where the scattering length approximation works.\nWhen you scatter a proton and neutron near threshold, you find that A is negative, which corresponds to the neutron being below the threshold. Alternatively, due to the presence of the neutron below threshold, when a proton and neutron scatter, they feel the presence of the neutron and have a negative scattering length.\nThe scattering length is measured in units of Fermi. For the deuteron, it is approximately -5.4 to -5.5 Fermi.\n\nWe will now consider different approaches to quantum physics. One of them, effective field theory at low energy, deals with calculating a low momentum expansion. This approach gives parameters like the scattering length approximation or the next term in the expansion, called the effective range.\n\n\n\n\n\n\nThe effective range expansion for S-wave scattering is:  k \\cot \\delta_0 \\approx -\\frac{1}{A} + \\frac{1}{2} r_0 k^2 + \\mathcal{O}(k^4)  where:\n\nk is the momentum,\n\\delta_0 is the S-wave phase shift,\nA is the scattering length,\nr_0 is the effective range.\n\n\n\n\nFor neutron scattering, measurements are difficult because neutrons are non-magnetic. Proton scattering is much easier to calculate.\nIn the notes, I have a value, but the conventions are slightly different. The units must be corrected—we want something in Fermi. What I have here is the mass of the pion in the numerator and the low-energy constant F_\\pi in GeV. They cancel each other, so this is not in our usual conventions.\n\nFor pion s-wave scattering, we can use effective field theory to compute scattering near threshold. The result depends on a single constant, F_\\pi, which is fixed experimentally. This low-energy theory uses spontaneous symmetry breaking, similar to the Higgs mechanism. Like all effective field theories, it builds the most general Lagrangian and fixes all constants.\nAt the lowest order, the amplitude is constant. This theory does not account for unitarity, so it does not include an imaginary part. The constant is equal to the scattering length in this sector. Effective field theory works well near threshold, where it provides constraints for fitting experimental data.\n\nUnitarity tells us how the amplitude behaves generally, but not the exact functional form. Constraints on these functions come from other methods, such as effective field theory.\n\n\n\n\n\n\nThe unitarity condition for partial wave amplitudes is:  \\text{Im}\\, f_l(k) = k |f_l(k)|^2  which must be satisfied for elastic scattering.\n\n\n\nI planned to cover more today, including chiral perturbation theory, but we are out of time.\n\nWe discussed kinematic singularities, which arise because the full amplitude must be free of them. Angular factors introduce 1/P^j due to cosine or sine terms. The same factor appears in partial wave amplitudes and modifies unitarity.\nWe introduced \\hat{A} and \\hat{T}, which are kinematic singularity-free functions. For mean parity equations, an extra factor impacts how we construct the amplitude.\n\nThe first term in the expansion is related to the partial wave. For the S-wave, it is simply P, and the prefactor is 1/A. The next term is r P^2 / 2, where r is the effective range. The function R is regular, and P^2 appears because lower powers would introduce a branch point.\n\nWe examined the analytic structure of the scattering length approximation. A negative scattering length introduces a pole, while a large positive value leads to a virtual state.\n\n\n\n\n\n\nThe pole position for a bound state near threshold is:  E_{\\text{pole}} \\approx -\\frac{\\hbar^2}{2\\mu A^2}  where \\mu is the reduced mass and A is the scattering length (negative for a bound state).\n\n\n\nThe closer the pole is to the threshold, the larger the amplitude near threshold. A large scattering length suggests a nearby pole—either a virtual state (positive) or a bound state (negative).\n\nFor proton-neutron scattering, the interaction strength is high due to the deuteron. However, scattering cannot form a deuteron because energy conservation prevents it. The amplitude above threshold reflects the pole below threshold.\nIf we tune the scattering length from a small negative value, the pole moves toward threshold. At A = -\\infty, the pole reaches threshold, then jumps to +\\infty and moves away.\nAn example of a virtual state is neutron-neutron scattering. There is no bound state, but the amplitude is large at threshold due to a virtual state.\n\nThat concludes today’s discussion. Next time, we will cover methods for sophisticated matrix element calculations, including lattice QCD."
  },
  {
    "objectID": "2024-Lecture-10.html",
    "href": "2024-Lecture-10.html",
    "title": "(2024) Lecture 10",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Calculating Cross Sections and Spin Algebra in Scattering Processes\nLet me recap what we discussed in the lecture. I had two excellent problems before moving to the exercise.\nI would like you to calculate the cross section for the scattering of two massless particles. Each matrix element is given for spin-zero scalar particles. Calculate the cross section and give the answer in millibarns squared.\nSteps for the calculation:\n\nFirst, calculate the cross section—remember the equation for how we compute it.\nThe whole computation is straightforward once you put the numbers in.\n\nThe cross section formula is: \n\\sigma = \\frac{1}{2s} \\times |\\mathcal{M}|^2 \\times \\frac{1}{8\\pi} \\frac{2p}{\\sqrt{s}}\n\n\nThe matrix element squared gives 3 + 9 + 4 = 13.\nHere is the flux, and here is the phase space.\n\nThe phase space is: \n\\frac{1}{8\\pi} \\frac{2p}{\\sqrt{s}}\n\n\nThe term \\frac{2p}{\\sqrt{s}} must be calculated given the masses and energy.\nFor this case, it simplifies to \\sqrt{s}/4.\nSince momentum equals energy for massless particles, p = \\sqrt{s}/2.\nThus, this factor becomes 1, leaving just \\frac{1}{8\\pi}.\n\nNow, consider the flux: \n\\text{Flux} = \\frac{1}{2E} \\frac{1}{|v_1 - v_2|}\n\n\nBoth energies are half the total energy, and v_1 - v_2 = 2 in the lab frame (since velocities are equal and at the speed of light).\nThis simplifies the flux to 2.\n\nPutting it all together: \n\\sigma = \\frac{1}{2s} \\times 13 \\times \\frac{1}{8\\pi} = \\frac{13}{16\\pi s}\n In \\text{GeV}^{-2}, this is \\frac{13}{8\\pi}.\nFor unit conversion: \n1 \\text{ GeV}^{-2} = 0.2 \\text{ millibarns}\n Thus, the cross section is: \n\\frac{13}{8\\pi} \\times 0.2 \\text{ millibarns}\n\n\n\n\n\n\n\nThe final numerical result depends on simplifying \\pi \\approx 3, but the method is what matters. The key is starting with the correct equations for cross section, flux, and phase space.\n\n\n\n\nNow, consider the imaginary part of the matrix element. Using the optical theorem: \n\\sigma_{\\text{total}} = \\frac{\\text{Im}\\,\\mathcal{M}(0)}{2p\\sqrt{s}}\n This gives the total cross section, not just the elastic one.\n\nThe total cross section includes all processes: AB \\to AB, AB \\to CD, etc.\nIt should be larger than the elastic cross section.\nIf the imaginary part gives a negative result, it’s unphysical—the imaginary part must be positive.\n\nExample: Breit-Wigner resonance \n\\mathcal{M} = \\frac{\\Gamma/2}{(E - E_0) - i\\Gamma/2}\n Its imaginary part is always positive.\n\nRecall the Argand diagram: the imaginary part (y-axis) must be non-negative.\nThis constraint ensures physical results.\n\n\nNow, for the second problem: spin algebra. You need to combine spins and compute partial waves. The brute-force method involves constructing a table of quantum numbers.\nExample:\n\nThe b_1 meson with spin-parity 1^+.\nCombining 1^- and 1^+ in S-wave gives 1^-.\nD-wave also allows 0^-, 1^-, 2^-.\n\nExotic Quantum Numbers:\n\nThe \\pi_1(1600) has exotic quantum numbers 1^-+, forbidden in the quark model.\nIt’s interpreted as a hybrid meson, with excited gluonic degrees of freedom.\nOriginally misidentified as \\pi_1(1400), it was corrected with better data.\n\n\n\n\n\n\n\n\nKey takeaways:\n\nCross section calculations require careful handling of flux and phase space.\nThe optical theorem links the imaginary part of the amplitude to the total cross section.\nSpin composition rules are essential for understanding particle interactions.\n\n\n\n\n\n\n0.2 Scattering Length Dimensionality and Spin-1/2 Particle Representations\nI thought it’s important to clarify. The notations got mixed up a little bit at the last lecture concerning the scattering length. The scattering length is indeed measured in Fermi. However, when you use relativistic notations, it appears dimensionless.\nIn quantum mechanics, the scattering length is defined as an expansion of the amplitude in terms of K, the breakup momentum. K has units of GeV, so 1/A has units of GeV, and A itself is in Fermi. The non-relativistic amplitude has a denominator: \nF_{\\text{non-rel.}} = \\frac{1}{A - iK}\n\nIn the relativistic formulation, unitarity tells us that the imaginary part of the amplitude is related to the phase space, which is dimensionless. The first expansion term is again something like the scattering length, but it is dimensionless. If you still want to talk about Fermi, you have to relate this to that, and the relation happens at the threshold.\nSince it’s a threshold expansion, you would like these amplitudes to match up to a numerical constant at the threshold. In this exercise, I want you to relate these two at the threshold and determine what the relativistic amplitude is near threshold. I even give you the answer—the only thing missing is the value of \\tilde{A}.\nThe scattering length is equal to three Fermi, corresponding to A. Knowing that, you need to compute \\tilde{A}. The way to do this is not simply to equate the amplitudes at threshold, because then you miss the point. If you evaluate the non-relativistic amplitude at threshold (K = 0): \nF_{\\text{non-rel.}} = A\n while the relativistic amplitude is \\tilde{A}. They are not equal—there is a numerical constant to account for.\nTo equate them, you must compare numerators or denominators up to a numerical constant and examine the coefficients in front of K. One approach is to Taylor-expand the expression where S is a function of K and equate the first term. Alternatively, notice that the only difference between numerators is the factor 1/(8\\pi^2 s).\nAt threshold, the condition to convert one to the other is: \n\\tilde{A} = \\frac{A}{8\\pi^2 s_{\\text{thr}}}\n To solve this, you need the masses and the threshold value. For example, if s_{\\text{thr}} = 1 \\, \\text{GeV}^2, then \\tilde{A} = A / (8\\pi^2).\n\n\n\n\n\n\n\nKey Insight: The relativistic and non-relativistic scattering lengths differ by a factor of 8\\pi^2 s_{\\text{thr}}. This conversion is essential for matching amplitudes at threshold.\n\n\n\n\nThis is widely used and provides interesting insights into hydrodynamics, which relies on the extra symmetry observed when particle masses are zero—called chirality. This is related to spin orientation for spin-1/2 particles.\nWe now return to particles carrying spin information, focusing on spin-1/2 and their field-theoretical representation. Spin-1/2 particles obey the Dirac equation: \n(P\\!\\!\\!/ - m)U = 0\n\nNotations:\n\nP\\!\\!\\!/ = P_\\mu \\gamma^\\mu (contraction of four-momentum with \\gamma^\\mu matrices).\n\\gamma^0 is part of the Dirac matrices.\n\nThere are two common conventions for solving the Dirac equation:\n\nThe Dirac convention\nThe Weyl convention\n\nThe Weyl convention is more convenient for right- and left-handed particles, but we will stick to the standard Dirac convention.\nThe \\gamma^\\mu are four-dimensional matrices, and P is a four-vector. Contracting them gives P\\!\\!\\!/, a 4 \\times 4 matrix. The term P\\!\\!\\!/ - m implies m is multiplied by the identity matrix. U is a four-component spinor (in German, “Spinor”).\nThe Dirac equation has two solutions for a given momentum P. If P has spatial components, the matrix equation yields two spinors, U_1 and U_2: \nU_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad U_2 = \\begin{pmatrix} P \\cos \\theta \\\\ \\sin \\theta \\end{pmatrix}\n These correspond to canonical states with spin quantized along the Z-axis, with projections +1/2 and -1/2.\nTo transform these to helicity states (spin quantized along momentum), we use a linear combination. The canonical state is defined as a pure boost acting on the |J, M\\rangle state at rest.\nThe transformation between canonical and helicity states involves an extra rotation R^{-1} to match the canonical state, followed by R. The resulting state is a linear combination of canonical states with coefficients given by Wigner D-matrices.\nThere are two conventions for the Wigner D-function:\n\nStandard: R(\\phi, \\theta, \\psi) = R_z(\\phi) R_y(\\theta) R_z(\\psi).\nSimplified (for helicity states): R(\\phi, \\theta, 0) = R_z(\\phi) R_y(\\theta).\n\nThe D-function describes rotations with spherical angles \\theta, \\phi. The simplified convention omits the unobservable phase from the first R_z rotation.\nApplying this to helicity states, we get: \nU_{\\text{helicity}} = N \\begin{pmatrix} \\cos(\\theta/2) \\\\ e^{i\\phi} \\sin(\\theta/2) \\\\ \\cos(\\theta/2) \\\\ e^{i\\phi} \\sin(\\theta/2) \\end{pmatrix}\n where N = \\sqrt{E + m} is the normalization. For the lower spinor component: \nU_{\\text{down}} \\approx \\frac{P}{E + m} \\begin{pmatrix} \\cos(\\theta/2) \\\\ e^{i\\phi} \\sin(\\theta/2) \\end{pmatrix}\n When m \\ll E, P \\approx E, and this factor approaches 1, simplifying the spinor.\n\n\n0.3 Properties and Applications of Chiral Projection Operators\nNow let me introduce projection operators, which will allow us to move on.\nThe \\gamma^5 matrix is introduced as the product of the gamma matrices with the Levi-Civita symbol:\n\n\\gamma^5 = i \\gamma^0 \\gamma^1 \\gamma^2 \\gamma^3.\n\nThis is the convention we choose.\n\nIt is very convenient for describing interactions to introduce right-handed and left-handed projection operators:\n\nP_R = \\frac{1}{2}(1 + \\gamma^5), \\quad P_L = \\frac{1}{2}(1 - \\gamma^5).\n\n\n\n\n\n\n\nThese projection operators satisfy key properties:\n\nP_R^2 = P_R, P_L^2 = P_L (idempotence)\nP_R + P_L = 1 (completeness)\nP_R P_L = P_L P_R = 0 (orthogonality)\n\n\n\n\n\nWhy are they projection operators? Because when they act twice on a state, nothing changes:\n\nP_R^2 = P_R, \\quad P_L^2 = P_L.\n\nLet’s verify this explicitly for P_L:\n\n\\left(\\frac{1 - \\gamma^5}{2}\\right) \\left(\\frac{1 - \\gamma^5}{2}\\right) = \\frac{1 - 2\\gamma^5 + (\\gamma^5)^2}{4}.\n\nSince (\\gamma^5)^2 = 1, this simplifies back to \\frac{1 - \\gamma^5}{2}.\n\nAnother key property is that their sum is the identity:\n\nP_R + P_L = 1.\n\nThe spaces they project onto are orthogonal. If you project onto the left-handed space, there are no right-handed components left:\n\nP_R P_L = P_L P_R = 0.\n\n\nAn important consequence arises from the fact that \\gamma^5 anti-commutes with all \\gamma^\\mu:\n\n\\{\\gamma^5, \\gamma^\\mu\\} = 0.\n\nThis implies that:\n\nP_R \\gamma^\\mu P_L = 0.\n\nHere’s why: if you start with P_L, multiply by \\gamma^\\mu, and swap the order using the anti-commutation relation, you get P_R, and P_R P_L = 0.\n\nAny spinor can be decomposed into right-handed and left-handed components:\n\n\\psi = P_R \\psi + P_L \\psi = \\psi_R + \\psi_L.\n\n\nNow, let’s give physical meaning to these states. Consider a helicity spinor (spin-up or spin-down). We can decompose it into right-handed and left-handed parts using the explicit matrix forms of the projection operators.\nIn a simplified representation:\n\nP_R = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad P_L = \\frac{1}{2} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}.\n\n\n\n\n\n\n\nThese are symbolic representations; the full forms are 4 \\times 4 matrices in the standard Dirac basis.\n\n\n\nUsing these, we can decompose the spinor into its chiral components. The result is a clean separation, which is very useful for analyzing interactions.\n\n\n0.4 Chirality, Helicity, and the V-A Interaction in Particle States\nIt’s cumbersome dealing with 4 \\times 4 matrices. This product will give me blocks, and the second component gives me what is inside the block.\nI have this state acting on the spinor, which is \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}. When I do multiplication, I multiply in components. That’s super convenient.\nWhen I do the projection, I realize that the left component from the operator yields a small factor 1 - \\kappa that is negligible—vanishing at high energy when mass gets smaller. The component of my right-handed helicity state is a spinor \\frac{1}{2} and has an intrinsically small left-handed component.\n\nThere is no single interpretation of the right-handed and left-handed states. However, they are closely related to the spin orientation along the direction of motion in the limit of zero mass. When a particle moves at the speed of light, they are literally the same constants.\nWhen you hear “right-handed,” it means the particle travels with spin aligned with the direction of motion. When you hear “left-handed,” it means the spin is opposite. But when the mass is significant—or when m is not much smaller than E—things get mixed, and the state becomes a superposition of both components.\n\n\n\n\n\n\nChiral Projection Operators define right-handed and left-handed states: \nP_R = \\frac{1 + \\gamma^5}{2}, \\quad P_L = \\frac{1 - \\gamma^5}{2}\n These operators are key to understanding the V-A interaction structure in the Standard Model.\n\n\n\n\nIt’s very interesting that in the Standard Model, the W boson, when it decays weakly, produces only the right-handed component for leptons and the left-handed component for antileptons. This happens due to the structure of the Lagrangian.\nYou’ve likely heard that the dominant interaction in the Standard Model is the V-A interaction. Here:\n\nV stands for vector (\\gamma^\\mu),\nA stands for axial (\\gamma^\\mu \\gamma^5).\n\nThe V-A structure means the Lagrangian contains terms like \\gamma^\\mu (1 - \\gamma^5), which can also be written as 2 \\gamma^\\mu P_L, where P_L = \\frac{1 - \\gamma^5}{2} is the left-handed projection operator.\nThis interaction vertex governs processes like W boson decay into an electron and antineutrino. The electron (e^-) is left-handed (spin opposite to motion at high energy), while the antineutrino is right-handed (spin aligned with motion).\n\nHave you heard about pion decay and spin suppression due to helicity mismatch? No? Good—I’m glad to be the first to explain it.\nOne must be careful when relating handedness to spin orientation because this one-to-one correspondence only holds in the high-energy limit or for massless particles. For neutrinos, this is always true:\n\nAll neutrinos are left-handed,\nAll antineutrinos are right-handed.\n\nThis is why, before neutrinos were well understood, right-handed neutrinos were proposed as new physics candidates—even as potential dark matter.\n\nDue to orthogonality, right-handed and left-handed particles in the massless case do not interact. They could even be entirely different particles with different masses. What connects them is relativity.\nFor massive particles, you can apply a boost \\Lambda^{-1} to bring them to the rest frame and then analyze their spin. But for massless particles, this is impossible—they have no rest frame, and their helicity cannot be flipped.\nRight-handed and left-handed states are called chiral states. The chiral limit refers to the case where m = 0, and particles are purely right- or left-handed.\nHelicity is tied to Lorentz transformations—it’s the spin’s alignment with motion. Chirality is closely related but only equivalent for massless particles. There’s no intuitive picture for chirality unless the particle is ultra-relativistic.\nIn the Lagrangian, chiral states are easier to work with because they are defined by projection operators (P_L, P_R) and don’t mix under boosts. Helicity states, while easier to visualize, do mix when you change reference frames. A left-handed chiral state remains left-handed no matter how you boost or rotate it.\n\n\n0.5 Flavor Symmetry and Chiral Limit in QCD Lagrangian\nHere is the Lagrangian of QCD. The Lagrangian of QCD is not the Standard Model. We don’t talk about electromagnetic interactions here. The dynamics of quarks and gluons is described by this Lagrangian:\n\n\\mathcal{L}_{\\text{QCD}} = \\bar{\\psi}_I^A (i \\gamma^\\mu D_\\mu - m_I) \\psi_I^A - \\frac{1}{4} G_{\\mu\\nu}^A G^{A,\\mu\\nu}\n\n\n\n\n\n\n\nKey components of the QCD Lagrangian:\n\n\\bar{\\psi}_I^A (i \\gamma^\\mu D_\\mu) \\psi_I^A: Kinetic term for quarks\n\\bar{\\psi}_I^A m_I \\psi_I^A: Mass term\nG_{\\mu\\nu}^A G^{A,\\mu\\nu}: Gluon field strength term\n\n\n\n\nThe G is the gluon field. This is the gauge part of the Lagrangian. There are many things here:\n\nThe \\bar{\\psi} \\partial \\psi is the kinetic term\n\\bar{\\psi} m \\psi is the mass term\nThe G inside the covariant derivative gives the interaction with the gauge field\n\n\nThe I is the index for the flavor. The main point is that I gives flavor, which is U, D, S, C, B, T—one of the six flavors. The A is the color index. The T is connected to the local gauge transformation.\nI hope you’re not afraid of these words. We discussed them before. “Gauge” means you can modify this; you can gauge. It means there is a symmetry of the Lagrangian, and there are many symmetries here. One of them, discussed in the first lecture, is the local gauge transformation, where you can readjust the phase of the color part of the fermion wave function.\n\nThe \\psi is a fermion wave function. It has many components, but the color components are in a three-state vector. This is something we can adjust with a local transformation. We do this using a matrix exponent with parameters that depend on the spacetime point x. This adjustment can be done at every point of spacetime—that’s what “local” means. This local transformation determines how gluons and quarks interact, and that’s how we construct the covariant derivative:\n\nD_\\mu \\psi = (\\partial_\\mu - i g T^A G_\\mu^A) \\psi\n\n\nWe don’t go deeper into this; instead, we look at the flavor symmetry. The masses of the light quarks—U, D, and S—are very small compared to the scale of quantum chromodynamics, which is about 1 GeV. That’s why we consider the chiral limit, setting these masses to zero. This is the basis for the low-energy expansion, where the small parameter is the ratio of the quark mass or momentum to \\Lambda_{\\text{QCD}}. This leads to our effective theory.\n\nWhat I wanted to discuss now is the flavor transformation. Flavor transformation means the flavor indices of the wave function can be rotated. We consider a global SU(3) flavor symmetry. The flavor part of the wave function has three components. We started with I as one of six flavors, but for simplicity, we now focus on the light quarks. The transformation is a global SU(3), represented by a 3 \\times 3 matrix. We can rotate or readjust this wave function freely.\nIt’s important that this is global—we don’t allow the transformation parameters \\alpha to depend on spacetime coordinates. The QCD Lagrangian is symmetric under this transformation. Wherever \\psi appears, the phases cancel each other. This is straightforward to see.\n\nWhat might be confusing is the T. T is a Gell-Mann matrix, and you might wonder how it interacts with the D_\\mu, since D_\\mu also contains T. But the T inside D_\\mu is the color Gell-Mann matrix, acting in a different space. In the flavor space, it’s just the identity—there are no matrices here. So the flavor transformation is simple.\nThe phase comes from the Hermitian conjugate—or rather, the Dirac adjoint \\bar{\\psi} = \\psi^\\dagger \\gamma^0. For clarity, let’s label the indices: \\alpha^a T^a, where a runs from 1 to 8 for SU(3).\n\nNow, we project the states into left- and right-handed components using the chirality operators:\n\n\\psi_L = P_L \\psi = \\frac{1 - \\gamma_5}{2} \\psi, \\quad \\psi_R = P_R \\psi = \\frac{1 + \\gamma_5}{2} \\psi\n\nThe vector part of the interaction is \\bar{\\psi}_L \\gamma^\\mu D_\\mu \\psi_L + \\bar{\\psi}_R \\gamma^\\mu D_\\mu \\psi_R. There is no mixing between left and right components. The covariant derivative D_\\mu doesn’t affect the flavor space, so it doesn’t change anything here. The \\gamma^\\mu could have been D_\\mu, but I didn’t say that.\n\n\n0.6 Chiral Symmetry and Its Spontaneous Breaking in QCD\nPossible symmetry. The chiral symmetry is the transformation of the right and left components independently. So we want to consider a symmetry.\nLet’s check if everything is fine. Here is a three-component vector in the flavor space, SU(3). And here is a 3 \\times 3 matrix. The parameter \\alpha does not depend on the coordinates—this is a global transformation.\nWe introduce parameters for the left-handed rotation and right-handed states rotation. We count eight numbers here and eight numbers there, giving 16 random numbers in total. We apply this transformation, and the Lagrangian doesn’t change unless it has mass.\n\n\n\n\n\n\nChiral Transformation of Quark Fields: \nq_L \\rightarrow e^{i \\theta_L^a T^a} q_L, \\quad q_R \\rightarrow e^{i \\theta_R^a T^a} q_R\n where q_L and q_R are the left- and right-handed quark fields, T^a are the generators of SU(3), and \\theta_L^a, \\theta_R^a are transformation parameters.\n\n\n\nHow do we see that it doesn’t change if the mass is zero? Because left-handed and right-handed states don’t talk to each other. An extra phase appears here and cancels with the one there—same for the other terms. The symmetry is only broken when mass terms appear. That’s chiral symmetry.\nThe mass term mixes the two components. Here is one combination of the \\alpha parameters, and here is the other one. The phases no longer cancel, so the mass term breaks chiral symmetry explicitly.\n\n\n\n\n\n\nMass Term Breaking Chiral Symmetry: \n\\mathcal{L}_{\\text{mass}} = -m \\bar{q} q = -m (\\bar{q}_L q_R + \\bar{q}_R q_L)\n This term mixes left- and right-handed components, explicitly breaking chiral symmetry.\n\n\n\nNow, something I won’t derive but you might have heard: chiral symmetry is also spontaneously broken. Even if you start with no mass term in QCD at low energy, the quarks generate a non-zero expectation value through their interactions. The quarks appear massive even without a mass term in the Lagrangian.\nHere is the vacuum. QCD, left side, right side—it’s quite spectacular that the entire symmetry is broken.\n\n\n\n\n\n\nSpontaneous Symmetry Breaking via Quark Condensate: \n\\langle \\bar{q} q \\rangle \\neq 0\n The non-zero vacuum expectation value of the quark condensate signals spontaneous chiral symmetry breaking.\n\n\n\nImagine a plane of field excitations. If you’re at zero, chiral symmetry is exact in the Lagrangian, and you can rotate left and right states independently. But the vacuum we live in is not at zero—it’s at a shifted location in field space.\nThis happens because the potential of the theory has a shape like the Higgs potential. For QCD, quark pairs acquire a non-zero expectation value by interacting, so the vacuum resides at a non-zero point in the field space.\n\n\n\n\n\n\nMexican Hat Potential (for Illustration): \nV(\\phi) = -\\mu^2 |\\phi|^2 + \\lambda |\\phi|^4\n Analogous to the Higgs mechanism, this potential shape describes spontaneous symmetry breaking, where \\phi represents the order parameter (e.g., quark condensate in QCD).\n\n\n\nThis continuous symmetry-breaking effect appears in many areas of physics: superconductors (Cooper pairs), the Higgs mechanism, and others. The mechanism is always the same. You have an exact theory at zero in the coordinate space, but once you move away, symmetry breaks spontaneously.\nThink of a Mexican hat potential. The interaction integrates over gluons, and the resulting potential has this shape. We could live in a vacuum with perfect chiral symmetry, but energetically, it’s favorable to be where the expectation value is non-zero. That’s spontaneous symmetry breaking.\nIn addition to spontaneous breaking, QCD has explicit symmetry breaking from non-zero quark masses. Spontaneous breaking is the major effect, captured by the theory. Explicit breaking is smaller since quark masses are much less than \\Lambda_{\\text{QCD}}.\nThis is where chiral perturbation theory comes in. It builds on chiral symmetry, with spontaneous breaking as an outcome. The breaking is treated as a perturbation.\n\n\n\n\n\n\nChiral Lagrangian (Low-Energy QCD): \n\\mathcal{L}_{\\text{chiral}} = \\frac{f_\\pi^2}{4} \\text{Tr}[\\partial_\\mu U \\partial^\\mu U^\\dagger] + \\cdots\n Here, U = e^{i \\pi^a T^a / f_\\pi} encodes the pseudo-Goldstone bosons (pions), and f_\\pi is the pion decay constant. The ellipsis represents higher-order terms in chiral perturbation theory.\n\n\n\nWe won’t go into details of chiral perturbation theory. Next lecture, I’ll write the chiral Lagrangian, but we won’t have time to explore it deeply. For that, I’d recommend dedicated courses—we have experts in the theory department.\nI wanted to give you the general concepts and an experimentalist’s perspective.\n\nQuestion: What is the variable in this plane? Is it the vacuum expectation value? Answer: Yes, the axis represents the expectation value of the field. The potential is drawn as a function of this expectation value.\nQuestion: Is the vacuum state always the correct one? Answer: It’s the state with the lowest energy, where the expectation value is non-zero.\nQuestion: What’s on the axis? Answer: The expectation value of the field. Strictly speaking, the potential in quantum field theory is defined via the expectation value.\nI’m not entirely happy with this analogy because the field space should be complex, but let me think of a simpler model, like superconductors, where this is clearer.\nQuestion: You mentioned electron-positron pairs earlier, but Cooper pairs are two electrons. Answer: Correct, Cooper pairs are two electrons.\nQuestion: Did you cover the Higgs mechanism in the particle physics course? Answer: Some of it, but not deeply.\nQuestion: What about heavy quarks? Answer: For heavy quarks, you use a different expansion—potential non-relativistic QCD (pNRQCD).\n\nWe’ll have a project discussion on June 28th for bachelors, masters, and anyone interested in internships. I’ll send details later.\nThanks!"
  },
  {
    "objectID": "2024-Lecture-11.html",
    "href": "2024-Lecture-11.html",
    "title": "(2024) Lecture 11",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Exam Participation and Group Coordination Discussion\n\n“This form of cross section—exactly.”\n“The form of a cross section, you have to remember.”\n\n\n\n\n\n\n\nWhile discussing cross sections, recall that in nuclear physics: \n\\sigma = \\frac{\\text{Number of reactions}}{\\text{Incident flux} \\times \\text{Target density} \\times \\text{Thickness}}\n This formula defines how cross sections are calculated experimentally.\n\n\n\n\nWe might still adjust between the 23rd and 25th. Do you know who from our group would come to do the exam? We would need to have Max. Moritz, maybe Fabian.\nI’m also not sure if she needs the credit points for this, but I will ask them if they will also come. Because Paul—I know he said he’s finishing his bachelor.\nI asked both of them, and I would actually like other guys who also have time to participate in that. Do you want to participate in the exam? We certainly have to make Dhruvanshu participate in that.\n\nDhruvanshu is a PhD student in our group who was partly involved in the typing exercises. Every week he is at CERN doing his hardware work. You might have seen him in the first week he was here. We just made him take the exam. I think it’s fair. He was skipping the entire semester—he has to work now.\n\n\nDid you receive my email to look at the symmetry breaking?\nI saw it this morning. I went through it and made some notes. I started making some notes on the ideas for that.\n\nMoritz, I briefly saw it yesterday, but then I had a dentist appointment because I sent it on Friday. Yesterday was the first working day. I put a date for Hanin there on Thursday.\n\n\n\n\n\n\n\n\nYou’re very welcome, together with the information you obtained today.\nLook at it once again and bring it to Ilya for the meeting on Thursday.\nIt’s also important because it gives you kind of easy points.\n\n\n\n\nThe exercise is simple, and I think you might need to report—no, we don’t need to report, but for our number of points you got over the semester. For every counting, please collect them.\n\n\n0.2 Mapping Riemann Sheets and Analytic Structures in the S-Plane and K-Plane\nIt’s a pity we started a bit late because the lecture today is very exciting—I’m particularly excited about lattice QCD and the hadron spectroscopy that we do in lattice. We will get to that next. I tried to start at 11:15, but we should be discussing lattice QCD already. Before that, we have two more items in the program. Let’s start with the recap. Today is Lecture 11.\n\nSo far, we’ve discussed a lot about the amplitude, which is a function of S. S is the Mandelstam variable, calculated as:\n\nS = (P_1 + P_2)^2\n\nfor the scattering. This S is a very convenient variable to describe the scattering amplitude, as the partial wave is a function of a single variable in this sense. Equivalently, one can consider the breakup momentum, the variable K, which is essentially the momentum that each particle carries. So S is the squared energy of the system in the center of mass, and K is the momentum of each particle in the center of mass.\n\nThis K is related to S with the channeling function, and the analytic structure of the function is somewhat simpler for K in the vicinity of the threshold. What I would like to do now is to look, instead of the S-plane where the function is analytic except for a few singularities, at the K-plane and relate structures in the S-plane to the K-plane—to map these points 1, 2, 3, 4 into the other plane.\n\n\n\n\n\n\n\nKey mappings between S and K:\n\nPhysical region (S &gt; S_{\\text{th}}) maps to real K.\nBelow threshold (S &lt; S_{\\text{th}}):\n\nBound state (first sheet) maps to imaginary K (positive imaginary axis).\nVirtual state (second sheet) maps to negative imaginary K.\n\n\n\n\n\n\nPoint 1 and point 2 are the resonances sitting on the second Riemann sheet with respect to S. Point 3 is the pole on the real axis in the first Riemann sheet, and this is the bound state. Point 4 (0.3) is the virtual state, which sits on the unphysical sheet below the threshold. I would like to have these points 1, 2, 3, 4 mapped to the other plane, as well as the real axis.\n\nThe real axis above threshold is the physical region, and it gets mapped to the physical region in the K-plane. This zero corresponds to the threshold. Below the threshold, S is smaller than the threshold, and the square root of it is imaginary, so K is imaginary.\nBut what about another square root? This is far away from the square root and does not change the sign when you cross the threshold. This is multiplied to S - M_1^2, and nothing happens when you go through this point. This one is always negative.\n\nThe branch point should be somewhere between zero and the threshold on this plane. The area close to zero is more complicated due to this branch point, but I want to focus on the function near the threshold.\nIf you were to continue that, you would have another real axis—somehow it flips back to the real axis. The dashed line would just come back to real. Number four is straightforward: we just add the dashed line, and it ends up here.\n\nFor number three, the negative part of the—yes, it needs to be there because it’s on another Riemann sheet. This is a value of S which is real but below threshold, on the other sheet. The sheets are related to the plus/minus sign of the square root:\n\nK = \\pm \\sqrt{S - S_{\\text{th}}}\n\n\nFirst sheet: plus sign.\nSecond sheet: minus sign.\n\n\nOn the K-plane, the square root changes sign, but on the S-plane, S does not change sign. Both sheets get mapped to the same K-plane, just with plus or minus.\nPoints 3 and 4, 2 and 1 are easy to connect analytically. You can draw lines or half-circles to connect them. The shape doesn’t matter as long as it’s analytic.\n\nThe K-plane is just S mapped differently. For the momentum plane, you don’t have a square root because K is the square root itself. If you have a function with a square root, it has two Riemann sheets. But when you replace the square root with K, it has a simple single-sheet structure.\n\nThe two-sheet structure comes from the plus or minus sign in front of the square root, which is part of the K-plane. If you do another mapping, like:\n\nX = K^2\n\nthe squaring operation maps the K-plane to a single-sheeted X-plane. This is why the sheets are connected—there is no discontinuity.\n\n\n\n0.3 QCD Confinement, Chiral Perturbation Theory, and Spontaneous Symmetry Breaking\nNow let’s go to physics. I would like to start from the discussion we had at the last lecture and discuss QCD phenomena—QCD and quark confinement dependence.\n\nAs you remember, the fundamental fields and fundamental degrees of freedom in QCD are quarks and gluons. However, when we go to the regime, one of the important phenomena is confinement. People start realizing the dynamics of QCD by looking at protons and pions—or rather, looking at neutrons. Nuclear physics and the interactions of the nucleus have these features of confinement.\n\nOne compares electron-proton scattering or electron-electron scattering, where there is a coupling constant: \n\\alpha = \\frac{e^2}{4\\pi} \\approx \\frac{1}{137}.\n This electromagnetic coupling constant is small and allows one to expand and use Feynman diagrams. In QED, we have \\alpha = \\frac{1}{137}.\nThen, for calculations of pion and proton interactions, you can make an effective field theory, match it to the observed phenomena, and figure out that in the non-perturbative regime, the theory is not perturbative. There is confinement.\n\nWe also know that the degrees of freedom—all interactions of quarks and gluons—are confined to a small scale. This makes new degrees of freedom, forming composite fields like mesons and baryons, or simpler objects like mesons.\n\nWhat we discussed last time is that at low energy, there is an effective theory describing interactions: chiral perturbation theory. This is an expansion of the theory in the masses and momentum of particles. It’s not about interactions being small, but rather about masses and momentum being small. This small parameter is used for the expansion.\n\n\n\n\n\n\nThe expansion parameter in chiral perturbation theory is: \n\\mathcal{L}_{\\text{eff}} \\sim \\sum_n c_n \\left(\\frac{p}{\\Lambda_{\\text{QCD}}}\\right)^n + \\mathcal{O}(m_q),\n where p is momentum, \\Lambda_{\\text{QCD}} is the QCD scale, and m_q are quark masses.\n\n\n\n\nThe picture of this effective theory is that we have it. The feature is that if you work with these effective fields—fermion, anti-fermion, fermion—you realize the theory experiences spontaneous symmetry breaking. The potential of this theory has a double-well structure, and the vacuum ends up in one of the minima.\n\nIt’s important to make this more accurate and realize what the sigma field represents. Last time we touched on this, and I reminded myself how the discussion goes. The potential can be introduced only when you consider fields as classical fields, not quantum fields. For classical fields, it’s clear what the potential is.\n\nIf a particle moves in a potential and its energy is below the potential barrier, it turns back. If you hit a ball against a wall, it reflects. This is the classical picture.\nQuantum fields are different. For quantum fields, there is a probability to end up inside the wall, with an exponential tail. If a particle approaches the wall, the wave function has a tail inside the wall due to quantum effects.\n\nThe relation between particles, quantum fields, and classical fields comes via the quasi-classical expansion. In quantum field theory, there is always \\hbar, a small number set to one in natural units. But you can use \\hbar as a small parameter and expand. The first term in the expansion is the average value of the fields—the classical version of \\sigma.\nNow, \\sigma is no longer a function of x; it doesn’t fluctuate in spacetime but is a fixed value. We can look at the quasi-classical potential of the theory as a function of \\sigma, and that’s how we define the potential.\n\nThe potential has a ring of minima. When you solve the problem—like in the previous assignment—you find the relation for the complex \\sigma. On the x-axis is the real part of \\sigma, and on the y-axis is the imaginary part. You can replace \\sigma with two independent fields, \\sigma_1 and \\sigma_2. The curvature of the potential indicates the particle’s mass.\n\nIn QCD, the degrees of freedom that experience binding in the confinement regime are the \\sigma field, which is integrated over—an unimportant field. The important degree of freedom is the pion field, which doesn’t experience binding and has no curvature in the potential.\nThe vacuum is aligned at the center, but due to spontaneous symmetry breaking, space is filled with the quark condensate of the \\sigma field: \n\\langle \\bar{q}q \\rangle \\equiv \\langle \\sigma \\rangle = v \\neq 0.\n On top of it, there are small fluctuations of the massless pion field.\n\nIt’s a nice picture: everything is filled with the \\sigma field, which fluctuates around a vacuum value rather than zero. The pion fields are massless—they don’t see the curvature of the potential. This is the Goldstone theorem: spontaneous symmetry breaking leads to massless bosons.\nIn QCD, pions are massless. The light mesons are massless, and they only gain mass in real life due to the Higgs mechanism, which makes quarks massive. If quarks were massless, pions would be massless Goldstone bosons, and the \\eta meson would be too. The entire SU(2) multiplet would be massless particles.\nThat’s it.\n\n\n\n\n\n\n\nThe Mexican hat potential for sigma and pion fields is: \nV(\\sigma, \\pi) = \\frac{\\lambda}{4}\\left(\\sigma^2 + \\pi^2 - v^2\\right)^2,\n where \\sigma is the radial mode (massive) and \\pi is the angular mode (massless Goldstone boson).\n\n\n\n\nThe Goldstone boson (pion) masslessness (before Higgs mechanism) is given by: \nm_\\pi^2 \\propto m_q \\quad \\text{(if } m_q \\to 0, \\text{ then } m_\\pi \\to 0\\text{)}.\n\n\n\n0.4 Chiral Symmetry Breaking, Goldstone Bosons, and the η’ Mass in QCD\nLast time we looked at the chiral symmetry, checking if the Lagrangian has the symmetry that you can rotate left quarks and right quarks independently. The idea is we look at the chiral symmetry for sure.\n\n\n\n\n\n\nThe chiral symmetry in the QCD Lagrangian (massless case) is given by: \\mathcal{L}_{QCD} = \\bar{\\psi}_L i \\gamma^\\mu D_\\mu \\psi_L + \\bar{\\psi}_R i \\gamma^\\mu D_\\mu \\psi_R where \\psi_L and \\psi_R are left- and right-handed quark fields, and D_\\mu is the covariant derivative.\n\n\n\nThe Lagrangian of QCD has a global SU(2) symmetry when there is no mass term present. This symmetry is broken spontaneously when you move to the minimum of the theory.\nIf you introduce independent rotations of the right and left fields, it was related to the rotations one can think of. This is the rotation in the \\sigma field—the symmetry of the potential. Once you move to the spontaneously broken phase, once you move to the minimum, you don’t have this symmetry anymore. This symmetry is broken, and it is a global symmetry.\nThe breaking of this left-right symmetry leads to the appearance of Goldstone bosons, which are pions and they are massless.\n\n\n\n\n\n\nFor SU(2)_L \\times SU(2)_R \\to SU(2)_V, the number of Goldstone bosons is: \\text{dim}(G/H) = \\text{dim}(SU(2)_L \\times SU(2)_R) - \\text{dim}(SU(2)_V) = 3\n\n\n\nWhen they acquire some mass, if the symmetry is bigger, like SU(3), we have eight mesons now that are massive. These are two kaons, two or three pions, and eight eta mesons appear to be massless. Why are they not massless? Because quarks have mass.\n\n\n\n\n\n\nFor SU(3)_L \\times SU(3)_R \\to SU(3)_V, the number of pseudo-Goldstone bosons (pions, kaons, eta) is: 8 = \\text{dim}(SU(3)_L \\times SU(3)_R) - \\text{dim}(SU(3)_V)\n\n\n\nI just made a connection with the Higgs mechanism because it’s the Higgs interaction with quarks that gives them mass. These are Yukawa couplings. This discussion is kind of irrelevant for the current perturbation theory. We just put a mass term explicitly in the Lagrangian.\n\n\n\n\n\n\nThe Yukawa coupling for quark masses in the Standard Model is: \\mathcal{L}_{Yukawa} = -y_f \\bar{\\psi}_L \\phi \\psi_R + h.c. where \\phi is the Higgs field and y_f is the Yukawa coupling constant.\n\n\n\nNow, having discovered the Higgs, we know that something similar to this fundamental symmetry breaking happens with the Higgs field. The vacuum is filled with the Higgs, through which particles, when moving, obtain mass. This is something connected but different—a different phenomenon. For the chiral perturbation theory, the mass term explicitly appears in the Lagrangian and breaks the symmetry explicitly.\nThe Yukawa coupling in the Higgs mechanism introduces mass. The original mechanism Yukawa invented was the exchange of pions—the Yukawa potential. Now, in the Standard Model, the mass matrix is introduced through terms called Yukawa couplings.\nIt’s a bit tricky about the \\eta' because even if you consider QCD without mass terms, the \\eta' still appears massive. The \\eta' is the one related to the U(1) symmetry.\n\n\n\n\n\n\nThe axial U(1)_A anomaly leads to a mass for the \\eta' meson even in the chiral limit: m_{\\eta'}^2 \\sim \\frac{N_f}{F_\\pi^2} \\chi_{top} where \\chi_{top} is the topological susceptibility and N_f is the number of flavors.\n\n\n\nHere, S for both groups means “special,” meaning the determinant of the matrix is equal to 1. The group U(2) is equal to SU(2) \\times U(1).\n\n\n\n\n\n\nThe relation between U(N) and SU(N) \\times U(1) is: U(N) \\simeq SU(N) \\times U(1)/\\mathbb{Z}_N\n\n\n\nIf you allow the determinant to be a phase, like if U(1) is a unitary group where the determinant of the matrix squared is one, you can consider this being one but have an extra phase for the determinant. The Lagrangian is seemingly invariant under U(1), under this phase rotation, especially globally, so that all derivatives don’t touch this phase. If you look at the composition of the field, like \\bar{\\psi}\\psi, it’s invariant under the phase transformation.\nBut apparently, this symmetry is also broken dynamically. If you compute higher-order perturbations, you find out that this symmetry is spontaneously broken. That’s related to the mass of the \\eta'. Once you see the light meson octet, you remember there is an \\eta' there, which is special.\nThe number of generators in U(2) is 4, and in U(3) it’s 9. For SU(3), 8 generators belong to SU(3), and these 8 are spontaneously broken, giving 8 Goldstone bosons—pions and kaons. The U(1) is seemingly a symmetry, but it could also be broken. There are anomalies that make the \\eta' massive even when the masses of quarks are zero.\n\n\nChiral symmetry in QCD Lagrangian (massless case): \\mathcal{L}_{QCD} = \\bar{\\psi}_L i \\gamma^\\mu D_\\mu \\psi_L + \\bar{\\psi}_R i \\gamma^\\mu D_\\mu \\psi_R\nGoldstone bosons from spontaneous symmetry breaking: For SU(2)_L \\times SU(2)_R \\to SU(2)_V, the number of Goldstone bosons is: \\text{dim}(G/H) = \\text{dim}(SU(2)_L \\times SU(2)_R) - \\text{dim}(SU(2)_V) = 3\nSU(3) case with explicit symmetry breaking: For SU(3)_L \\times SU(3)_R \\to SU(3)_V, the pseudo-Goldstone bosons (pions, kaons, eta) number: 8 = \\text{dim}(SU(3)_L \\times SU(3)_R) - \\text{dim}(SU(3)_V)\nYukawa coupling for quark masses: \\mathcal{L}_{Yukawa} = -y_f \\bar{\\psi}_L \\phi \\psi_R + h.c.\nU(1) anomaly and \\eta' mass: m_{\\eta'}^2 \\sim \\frac{N_f}{F_\\pi^2} \\chi_{top}\nGroup decomposition: U(N) \\simeq SU(N) \\times U(1)/\\mathbb{Z}_N\n\n\n\n0.5 Proton Mass and Chiral Symmetry in the Context of Quark-Gluon Interactions\nWe discussed so far about pions and kaons, but the proton is also made of three quarks. There is nothing here that tells or that we haven’t discussed the mass of the proton yet. But what’s your guess? If you go to the chiral perturbation theory in the chiral limit, you put masses of quarks to zero:  m_u = m_d = 0  What happens with the proton?\n\nPion is massless.\nKaon is massless,\nso the proton should also be massless because all masses are zero.\n\nBut maybe the gluon mass makes the difference.\n\n\n\n\n\n\n\nPions and kaons are special—they are Goldstone bosons of the chiral symmetry, but the proton is actually massive. The proton has energy of the quark-gluon interaction stored into it. The mass of the proton is purely determined by the quark-gluon interactions:  m_p \\approx \\Lambda_{\\text{QCD}} \\sim \\mathcal{O}(1 \\text{ GeV}) \n\n\n\n\nDo we have quark-gluon interactions in mesons as well? We do, but we have a condensate of the quarks, and this is the sigma field.\n\nPions are little fluctuations on the background of this field.\nProtons are something different—they are sort of excitations of this condensate.\nRho mesons are excitations of the gluon condensate, but protons are a different type of object.\n\n\nPions are special—they are massless. The proton just experiences this heavy gluon condensate. The rest of the hadrons, especially except for those Goldstone bosons, have a proper mass related to the quark-gluon interaction, and everything sits on the background of this gluon condensate. This is tricky to wrap your mind around.\n\nOne thing that would help is to look at the mathematics of the mechanism to see how sigma obtains its vacuum non-trivial expectation value:  \\langle \\sigma \\rangle = v = \\sqrt{\\frac{\\mu^2}{\\lambda}}  and how pions appear as fluctuations in directions where there is no potential.\n\nThe mass of the particles is the curvature of the potential.\nPions move in directions where there is no curvature, which is why they don’t have masses.\nProtons, rho mesons, and other excitations are different because they have this background field excited—they have energy stored and thus have mass.\n\n\nOne important point is that the theory itself does not have values of the masses that are special. You can tune the masses—you can increase them or make them zero—and then you have a series. But once you introduce the masses, the theory doesn’t change very much. Especially because the masses of the light quarks u and d are rather small—they are smaller than the QCD scale of 1 GeV. There is no big change in the physics when you introduce small-mass quarks.\n\nWhat happens then? When you introduce quark masses, you effectively add a linear term to your potential, skewing it slightly. The picture remains almost the same—it’s just that the minimum on one side gets more pronounced. Now there is curvature in the pion direction as well. It’s a Mexican hat potential:  V(\\sigma, \\vec{\\pi}) = -\\frac{\\mu^2}{2}(\\sigma^2 + \\vec{\\pi}^2) + \\frac{\\lambda}{4}(\\sigma^2 + \\vec{\\pi}^2)^2 \nWith quark masses, the potential becomes:  V(\\sigma, \\vec{\\pi}) \\rightarrow V(\\sigma, \\vec{\\pi}) - c \\sigma \nThis gives pions a small mass:  m_\\pi^2 \\propto m_q \\Lambda_{\\text{QCD}} \n\nMoreover, one of the quantities often introduced is the mass of the proton over the mass of the rho meson, which is characterized by the QCD interaction:  \\frac{m_p}{m_\\rho} \\approx \\text{constant} \nThis doesn’t change much if you increase the quark mass.\n\nWe will see that computations of the properties of the chiral perturbation theory depend very much on the quark masses you put in. It appears easier to calculate when pions are slightly heavier, which is what you do by introducing slightly larger quark masses. Then you do computations for slightly heavier pions. But the properties of the theory—the meson properties—do not change much. The details change, like the mass and width of the particles, but the characteristics remain. You can still learn a lot from non-physical particle masses.\nYou shouldn’t be scared by seeing unphysical pion masses or quark masses in calculations. Quantitatively, this is a tool we use to learn about the properties of the theory.\n\n\n\n0.6 Discretization and Grid Setup in Lattice QCD Calculations\nNow let’s move on to discuss lattice QCD.\nThe fields depend on the four-dimensional coordinate vector x^\\mu = (t, \\vec{x}), which includes time and spatial dimensions.\n\n\n\n\n\n\nThe spacetime coordinate is explicitly represented as: x^\\mu = (x^0, x^1, x^2, x^3) = (t, \\vec{x})\n\n\n\nWhat we do is introduce a grid in both the spatial and time dimensions, then compute the Lagrangian action or particle correlations on this grid.\n\n\n\n\n\n\nThe lattice action (simplified form) is: S_{\\text{lattice}} = \\sum_{n,\\mu,\\nu} \\text{Re}\\, \\text{Tr}\\left[1 - U_{\\mu\\nu}(n)\\right] + \\sum_n \\bar{\\psi}_n D \\psi_n where U_{\\mu\\nu} is the plaquette variable and D is the Dirac operator on the lattice.\n\n\n\nTypically, lattice QCD setups use a time dimension of around 200 points, with a spatial lattice size of about 7 Fermi, discretized into 50 points in each dimension.\n\n\n\n\n\n\nThe lattice spacing a and temporal extent T are given by: a = \\frac{L}{N} T = N_t a_t where L = 7 Fermi, N = 50, and N_t = 200.\n\n\n\nImagine a box with 50 points on each side. We then observe how it evolves over time by performing 200 steps.\nThese values are determined experimentally by testing what works best for calculations.\n\n\n0.7 Finite Volume Effects and Discretization Errors in Particle Simulation\nThe computations are very numerically heavy. Therefore, the lower number of points you can take, the better. But there are two main challenges when you put a particle in the box:\n\nFinite volume effect: Finite box, finite volume—you more often hear “finite volume effect.”\nDiscretization error: Both of them can be addressed: the first by making the volume bigger, the second by making the steps smaller. This comes with the cost of computation.\n\n\n\n\n\n\n\nThe finite volume effect leads to an energy shift approximated by: \\Delta E(L) \\sim \\frac{C}{L^3} e^{-m_\\pi L} where L is the box size, m_\\pi is the pion mass, and C is a constant.\n\n\n\n\nIt’s intuitive—you would need an infinite and continuous space to avoid these issues. That’s all there is to it.\nHere’s an analogy to help visualize the lattice: think of Wi-Fi and the point-and-box aspect. This is important because we use cyclic or periodic boundary conditions. If you put a proton in the box and calculate its properties, due to the finite size and periodic boundary conditions, it sees its mirror image.\n\n\n\n\n\n\nPeriodic boundary conditions enforce: \\psi(x + L) = \\psi(x) This avoids forcing the wave function to vanish at the borders, unlike fixed boundary conditions.\n\n\n\n\nWhy use periodic boundary conditions? There are advantages. What are the alternatives? You could just say the box ends there. If you only calculate something inside the box, it shouldn’t make a difference. But if the box ends there, the boundary condition would still impose quantization and affect the wave function.\nFor example, if the wave function had to vanish at the borders, you’d have a lower probability there. That’s my explanation.\n\nPeriodic boundary conditions have advantages—the wave function, in a second quantization context, isn’t forced to vanish at the borders. The two approaches are similar, but one fixes the entire wave function while the other lets it move. One is more like a static configuration, the other more dynamic.\n\n\n\n\n\n\nQuantized momentum on a lattice with periodic boundary conditions is given by: p_i = \\frac{2\\pi n_i}{L}, \\quad n_i \\in \\mathbb{Z}\n\n\n\n\nI would have assumed it wouldn’t make a difference, but uncertainties introduced by a finite box are exponentially suppressed for certain good observables.\n\n\n\n\n\n\nThe finite-volume correction to a hadron’s mass follows: m(L) = m_\\infty + \\frac{A}{L} e^{-m_\\pi L} + \\cdots where m_\\infty is the infinite-volume mass and A is a constant.\n\n\n\n\n\n0.8 Finite Volume Effects, Discretization, and Observables in Lattice QCD\nI have to be vague since we just decided to spend one lecture on QCD. I should have started by saying that one can spend a semester or several semesters learning quantum chromodynamics on the lattice. It’s a rapidly moving field with many interesting techniques. Unfortunately, I can only give you a brief overview.\n\n\n\n\n\n\nThe lecturer suggests reaching out to John Gulava for a deeper dive into QCD next semester, as he is the ideal expert for detailed explanations.\n\n\n\nCan you convince John Gulava to hold a lecture on this topic next semester? I could talk to him. He would dream of giving these lectures, and he would be the right person to explain all the details. But let me just give you some vague arguments, a taste of the computations, and some common knowledge about it.\n\nWe discussed finite volume effects. My claim is that corrections due to the lattice size for good observables like particle masses or form factors are exponentially suppressed with the lightest degrees of freedom of the theory, which is the pion.\nThe reason for this is that if you put a proton on the lattice, pions are present as well. The proton is addressed, so pions—these beautiful particles—pop up as virtual particles. They don’t propagate; they exist in clouds around the proton, mediating interactions. Since they are virtual, they cannot sample the boundary of the box. Because pions are the lightest particles, they set the scale for this suppression.\nAnother way to see why it scales this way is to look at interactions in momentum space. The potential has a Yukawa form, proportional to:\n\nV(r) \\propto \\frac{e^{-m_\\pi r}}{r},\n\nwhere r is the distance. This also gives you an argument for why the effects are exponentially suppressed.\n\nThis exponential factor is key. It means that once your lattice is large compared to the inverse pion mass, your computations closely approximate infinite-volume quantities. However, they are still finite-volume quantities. It’s important to realize that physics in a box is… or rather, infinite-volume physics is the ideal case. Once you take L to infinity, you approach infinite-volume quantities, and then you have the physical situation.\nNow, discretization errors—I won’t discuss them in detail, but they are also important. The more steps you take, the better.\n\nRegarding finite volume effects: would assuming a much larger but mostly empty box make computations much more intensive? Empty in what sense? No particles? If the box were infinite, most of it would be empty—we wouldn’t see particles except those we put in. But it’s slightly different. The nodes of the lattice all have fields defined. For every node, we have \\psi(x) defined. We work with a box where all fields interact, and even if you don’t insert particles, the vacuum fluctuates. The box is never empty; it’s filled with quantum fields. There are beautiful animations of virtual particle bubbles appearing and disappearing in this vacuum.\n\nIt wouldn’t make sense to change discretization to have different spacings. Maybe varying grid spacing could be considered, but this is related to the Serber curve and confinement. The grid spacing determines the scale at which you cut off the theory. Small step sizes are crucial because if a is too large, QCD loses confinement.\nTo answer your question: changing a must be done carefully. Lattice researchers ensure results aren’t strongly affected by spacing by performing computations at multiple discretization levels. For example, in lattice papers, you’ll see results for a = 0.1, 0.05, 0.02 fm, etc., to demonstrate discretization errors are under control. This is a tricky subject, often addressed numerically.\n\nNow, how are changes in states implemented? Is it some sort of hopping probability for particles? What we do is compute observables—specifically, correlation functions. These are expectation values of operators at time t and time 0, formally defined as:\n\n\\langle O(t)O(0) \\rangle = \\frac{\\int \\mathcal{D} \\psi \\mathcal{D} \\bar{\\psi} \\, O(t)O(0) e^{-S[\\psi, \\bar{\\psi}]}}{\\int \\mathcal{D} \\psi \\mathcal{D} \\bar{\\psi} \\, e^{-S[\\psi, \\bar{\\psi}]}},\n\nwhere S is the action. On the lattice, this involves (L/a)^3 \\times (T/a) \\times 4 (Lorentz) \\times 3 (color) dimensions. The \\psi field is a 4D Dirac spinor with color and flavor (SU(3)).\nThis integral is enormous—think 10^4 dimensions or more. Numerically evaluating it by direct discretization is impossible. Instead, we use Monte Carlo methods from statistical mechanics. We generate field configurations (samples) and evaluate observables statistically.\n\nThe key problem is the oscillatory phase factor in Minkowski space, which ruins convergence. To fix this, we Wick-rotate to Euclidean time, making the weights real and the integral tractable.\nIn Euclidean space, operator correlations decay exponentially with energy gaps. For example, a two-point function \\langle O(t)O(0) \\rangle behaves as:\n\n\\sum_n |\\langle 0|O|n \\rangle|^2 e^{-E_n t}.\n\nAt large t, the lightest state dominates.\nTo extract energies, we study the effective mass:\n\nm_{\\text{eff}}(t) = -\\log \\left( \\frac{C(t)}{C(t-1)} \\right),\n\nwhich plateaus at the ground-state energy. This is how lattice QCD computes particle masses (e.g., pions, rho mesons) and other spectral quantities.\n\n\n\n\n\n\n\nThe exponential suppression of finite-volume effects and the Yukawa potential are central to understanding lattice QCD’s accuracy. The Euclidean correlation functions and effective mass method are critical for extracting physical quantities numerically.\n\n\n\n\n\n0.9 Quantization of Momentum from Periodic Boundary Conditions\nLet me say one last thing. We are already over time, but it’s important to note that the homework exercise today is to realize that the particle in the box is slightly different from the particle in this space.\n\n\n\n\n\n\nDue to the periodic boundary condition, you have a quantization of the momentum. If your wave function is \\psi(x) = e^{i p x}, and you require that \\psi(x + L) = \\psi(x), you immediately realize that p L must be 2 \\pi n, where n is an integer.\n\n\n\nThe key relationships discussed are:\n\nWave function periodicity: \\psi(x + L) = \\psi(x)\nQuantized momentum condition: p = \\frac{2\\pi n}{L}\nPhase matching: e^{i p L} = e^{i 2\\pi n} = 1\n\n\nThe n comes in if you take more than one L. It’s a phase condition: for the wave function to match, the exponent must satisfy p L = 2 \\pi n. This ensures:\n\ne^{i p L} = e^{i 2 \\pi n} = 1\n\nmeaning the phase repeats every L.\n\nI thought L would be just 2 \\pi, and n L could be 2 \\pi n. But this is the same principle as a standard wave. Even if the phase is 4 \\pi, it still represents a single period.\n\nKey points to remember:\n\nThe integer n represents the quantum number for allowed momentum states\nThe quantization condition p L = 2\\pi n arises from periodic boundary requirements\nThe phase must return to its original value after distance L\n\n\n\n0.10 Quantized Momentum and Energy Shifts in a Periodic Two-Particle System\nWhat I mean is just that it shouldn’t be one if it’s anything less than L. It shouldn’t be one if it’s anything less than L. For l = \\frac{1}{2}, it could be one. Why not?\nSo it means that the momentum of particles is quantized and can only take values in steps as small as \\frac{1}{L}. The quantized momentum condition for particles in a periodic system is:\n\np_n = \\frac{2\\pi n}{L} \\quad \\text{where } n \\in \\mathbb{Z}.\n\nTherefore, if you consider a two-particle system, it has a discrete spectrum. Here, M = 1, n = 2, n = 3, and n = 4. This is the periodic boundary condition.\nYou can think of two particles moving in a circle of length L. The one-dimensional case of this box computation would be a circle, and the spectrum is discrete.\n\nIn that case, the energy of the system is equal to the sum of the energies of the particles. For a non-interacting system, the energy spectrum is:\n\nE_{\\text{non-int}} = \\sum_{i=1}^{2} \\frac{p_i^2}{2m} = \\sum_{i=1}^{2} \\frac{(2\\pi n_i)^2}{2mL^2}.\n\nThe particles are freely moving and do not interact. However, once interactions are introduced, the energy of the system becomes slightly different. We can still write the total energy, but we must account for the interaction energy.\n\nIt is natural that the spectrum of an interacting system differs from the non-interacting case. There is a shift due to interaction:\n\n\\Delta E = E_{\\text{int}} - E_{\\text{non-int}}}.\n\nThis is the idea behind computing interactions on the lattice and performing spectroscopy. First, compute the spectrum without interactions. Then, compute it with interactions and observe how the energy levels shift.\n\n\n\n\n\n\nThe difference between these levels gives information about the phase shift of the interaction. The relation between the energy shift and the scattering phase is given by Lüscher’s formula for periodic boundary conditions: \n\\Delta E = -\\frac{4\\pi a}{mL^3} \\left[1 + c_1 \\frac{a}{L} + c_2 \\left(\\frac{a}{L}\\right)^2 \\right] + \\mathcal{O}(L^{-6}),\n where a is the scattering length and c_i are numerical constants.\n\n\n\n\nTo measure this, you compute the two-point correlator:\n\nC(t) = \\langle \\mathcal{O}(t) \\mathcal{O}^\\dagger(0) \\rangle \\propto e^{-Et}.\n\nFor example, compute this quantity for two pions. Observe where it saturates at high T. The saturation point gives a measurement of the energy level.\nRepeat the same computation for the interacting system. Compare the two results to determine the shift. Relate the shift to the scattering phase.\nThe way collision data connects to the answer is through another technique, typically analyzed in configuration space with real-valued wavefunctions.\n\n\n0.11 Exploring Hadron Structure and Scattering Amplitudes in Lattice QCD\nThis is my latest QCD lecture. I invite you to the full lectures if someone offers them—it’s a fascinating subject. In recent years, we have learned so much about the hadron spectrum from lattice QCD.\n\nThe lattice provides an isolated setup free from experimental effects. You can perform calculations that are impossible in experiments. For example, you can scatter two pions. While pion scattering is possible experimentally, processes like a photon scattering off a sigma meson are unattainable in experiments—but not on the lattice.\n\n\n\n\n\n\nThe sigma meson, a pion-pion S-wave state, is one of the low-energy tetraquark candidates observed as a “blob” in the spectrum.\n\n\n\nBy studying pion-pion scattering and inserting a photon current, you can compute correlation functions to access form factors. These form factors reveal the spatial distribution of quarks and gluons inside mesons.\n\nOne exciting frontier of lattice QCD is probing hadron structure by inserting currents. This allows you to visualize how quarks and gluons are distributed spatially. Such studies are impossible in QED, where you only have pure electronic states.\nIn QCD, you can compute form factors, but fixing low-energy constants is challenging. Some researchers working on LPT might disagree, but I’m unaware of a reliable method.\nFor higher-energy experiments involving low-lying resonances, you produce them via stable particles. You only access the production amplitude, such as:  \\pi + \\text{proton} \\to \\text{target} \n\nUnitarity constraints, which we discussed earlier, are much stricter for scattering amplitudes. In the last lecture, I wrote a matrix element on the board, and Henrik pointed out: “Shouldn’t the imaginary part be positive due to unitarity?”\nThis refers to the scattering amplitude T(s), where:  \\text{Im}\\,T(s) \\geq 0 \\quad \\text{and} \\quad |T(s)| \\leq 1  For purely elastic scattering, |T_{el}(s)| = 1—it must lie on the unitary circle.\n\nOn the lattice, you can access amplitudes like:\n\n\\pi\\pi \\to \\pi\\pi\n\\pi\\pi \\to KK\n\\pi K \\to \\pi K\n\n…all while controlling quark masses. You can make pions massless or massive, drastically altering the elastic region.\n\nWith massless pions, thresholds for 2\\pi, 3\\pi, and 5\\pi states coincide.\nWith massive pions, a large elastic region opens where the 2\\pi threshold is accessible but higher thresholds (e.g., 3\\pi) remain closed.\n\n\nThis allows nearly parameter-free studies using unitarity constraints. By varying quark masses, you can make particles transition in the complex K-plane, where:  K(s) = \\sqrt{s/4 - m^2} \nResonances appear as poles in the second Riemann sheet. You can smoothly connect:\n\nBound states\nResonances\nVirtual states\n\n…all manifestations of QCD.\n\nBound states, virtual states, and resonances are deeply interconnected. Depending on quark masses m_q, a bound state can become a resonance or vice versa. This reveals the unified nature of these phenomena in QCD.\n\n\n0.12 Preparing Exercise Sheets and Closing Remarks\n\nOn exercise sheets: I hope you’re going to like the exercise sheet that Ilya kindly prepared and printed for you. I prepared it in the night, and Ilya printed it.\n\n\n\n\n\n\n\n\nAcknowledgments: Thank you very much for coming today, and sorry for going a little bit overtime. Well done."
  },
  {
    "objectID": "2024-Lecture-12.html",
    "href": "2024-Lecture-12.html",
    "title": "(2024) Lecture 12",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Ilya Segal\n\n0.1 Pion Decay Dynamics, Chirality Suppression, and the Muon Magnetic Moment\nWe are finishing the lecture cycle. Two more to go. Today is the month before last. However, content-wise, today will be more on physics, and next week I would like to discuss meta-issues and how the hadron community functions. I will give you a little bit of background on how research is structured in the field, how collaborations work, and what scientists actually do in the hadron physics domain. There will be less on physics next time. Perhaps I will spend the first 20 minutes reviewing recent discoveries, what experiments have brought up, and what the hadron community is currently working on.\n\nBut today I would like to talk more about physics and still have to write formulas and discuss how particle physics is used outside of particle physics, how it is an important part of many other studies. I considered dedicating one lecture to CP violation, where particle physics plays a key role. Another subject I was considering is the muon magnetic moment and its hadronic contributions. That’s what we will discuss. I decided on this fundamental topic because it touches on several subjects we’ve already covered. The exercise sheet is on the R ratio, something we postponed earlier. Now we enhance it with more problems and discuss it at a more advanced level.\n\n\n\n\n\n\n\nKey formulas discussed in this lecture:\n\nWeak interaction vertex: \\mathcal{L}_{\\text{weak}} \\propto \\bar{\\psi}\\gamma^\\mu (1 - \\gamma_5)\\psi\nPion decay amplitude suppression: \\mathcal{M} \\propto m_\\ell / E_\\ell\nMuon magnetic moment: \\vec{\\mu} = g \\left( \\frac{e}{2m} \\right) \\vec{S}\n\n\n\n\n\nI would like to start today’s lecture with a short recap. Let’s revisit our discussion of helicity and chirality and examine pion decay: \\pi^+ \\to \\mu^+ \\nu_\\mu. I would like you to compare this decay. Start by drawing this two-body decay in the rest frame of the pion, indicating momenta and chirality. The vertex is a weak interaction, so the produced particles have definite chiral properties—left or right. On this diagram, chirality is related to spin orientation in the high-mass limit. Given these chiral configurations, what are the ratios of the matrix elements? Which phase space is bigger? And which decay is more likely for the pion? Let’s take three minutes.\n\nLet’s discuss the first question. Who found this obvious? No one? That’s fine—we have something to discuss. Draw the center-of-momentum decay kinematics and indicate chirality. Note: it’s a neutrino, not an antineutrino, because the electron comes with an antineutrino. The weak interaction vertex has a V-A structure, \\gamma^\\mu (1 - \\gamma_5), which projects the quark current to left-handedness. Here, the vertex produces a left-handed positron and a left-handed neutrino.\nFor the right-handed positron, the left-handed component is suppressed by its mass. The neutrino is massless, so it’s purely left-handed. For the right-handed particle, the spin aligned with momentum dominates. For the left-handed particle, it’s the opposite. Both particles have spin \\frac{1}{2}, and their spins add to one, which is incompatible with the pion’s spin-zero nature. Other configurations are possible, but flipping the spin of the right-handed particle is suppressed by \\frac{m}{E}. The neutrino, being massless, has no opposite component, but the positron does. The v_R^+ component is suppressed by \\frac{m}{E}.\n\nThis leads to the second question: The matrix elements for the electron and muon are suppressed differently because their masses differ. The lighter the particle, the stronger the suppression—for the electron, it’s severe, but for the muon, especially since the pion mass is close to the muon mass, the chirality suppression is less pronounced. The correct answer is that the ratio is much less than one, explaining why the pion decays predominantly to muons, not electrons.\nFor phase space, the number of angular configurations is the same, but the energy release differs. The bigger the energy release, the larger the phase space. For the muon, most of the pion mass goes into kinetic energy because the neutrino is massless and the electron nearly so. The phase space is larger for the muon, but this doesn’t compensate for the suppression. The phase space integral is \\frac{s - m_e^2}{s - m_\\mu^2}, but this doesn’t change the conclusion.\n\nThe dominant decay of the pion is \\pi^+ \\to \\mu^+ \\nu_\\mu. This is important physics—it’s how parity violation was discovered. The muon is produced polarized because the pion has no spin, and the neutrino escapes undetected. The muon’s polarization can be measured because its decay \\mu^+ \\to e^+ \\nu_e \\bar{\\nu}_\\mu is self-analyzing. The electron’s angular distribution reveals the muon’s polarization. If the muon is polarized, the electron prefers to align with the spin direction. This setup was used in 1967 to confirm parity violation.\n\nLater, the same technique measured the muon’s magnetic moment. Recall from the second lecture that the magnetic moment of a particle describes its interaction with a magnetic field. For a point-like fermion, it’s given by:\n\n\\vec{\\mu} = g \\left( \\frac{e}{2m} \\right) \\vec{S}, \\quad \\text{where} \\quad g \\approx 2\n\nFor the proton, we found g \\approx 2.8, indicating it’s composite. But the muon is elementary, so we expect g = 2. However, quantum corrections make g slightly larger. The deviation is quantified by a_\\mu = \\frac{g - 2}{2}, one of the most precisely known numbers in physics.\n\nThe leading QED correction, calculated by Schwinger, is:\n\na_\\mu = \\frac{\\alpha}{2\\pi}\n\nHigher-order terms (\\alpha^2, \\alpha^3, etc.) have been computed up to \\alpha^5. Modern calculations use computers due to the complexity. The experimental setup involves trapping muons in a magnetic field ring. The muon’s spin precesses at \\omega_s = \\omega_c (1 + a_\\mu \\gamma), where \\omega_c is the cyclotron frequency. The difference \\omega_s - \\omega_c isolates a_\\mu, allowing precise measurements. This technique, pioneered decades ago, remains critical for testing QED and probing new physics.\n\n\n0.2 Proton Beam to Muon Spin Precession: Measuring Anomalous Magnetic Moment\nThe experiment is as follows. You start with the proton beam—that’s a hydrogen beam, the easiest beam we can get. You hit the target and you get a bunch of pions flying forward. You collect the pions and direct them towards your ring.\n\nHere is the tunnel through which the pions from the secondary vertex come. In this tunnel, they decay. Some of the pions convert to muons, and the muons are trapped. In the ring, they start moving.\n\n\n\n\n\n\nThe muon’s anomalous magnetic moment (a_\\mu) is central to this experiment: \na_\\mu = \\frac{g_\\mu - 2}{2}\n where g_\\mu is the g-factor of the muon.\n\n\n\n\nSince pions produce polarized muons, they already have exactly the configuration we want. There is a muon, and there is a spin. By applying a magnetic field, this setup starts working. The muon moves along the circle, and the spin begins precessing.\n\nWe measure the spin orientation by looking at the decay electrons. Muons go along the circle and decay. There are millions of particles in the bunch, and as they move, some muons decay.\n\nInside the circle, there are calorimeters—lead glass calorimeters, as I checked this morning. It’s a big block, a typical setup used in experiments like COMPASS. Lead glass is a material doped with lead to increase its atomic number (Z). When a particle enters, it deposits all its energy inside. Because it’s glass, it’s transparent to light. You place this heavy block of glass with a photosensor at the end. A particle comes in, leaves energy, and the sensor collects it.\n\nThese calorimeters measure the flux of electrons.\n\nIf the spin points outwards, you detect fewer electrons.\nIf the spin points inwards, you detect more electrons. Along the circle, you see fluctuations in the energy deposit.\n\n\nIn g-2 measurements, the plot shows time on the x-axis and energy deposit—or something similar. People cut this piece and start over.\nThe frequency of rotation stays the same. To be precise, the frequency doesn’t change. It is determined by the charge of the muon and the magnetic field.\n\nOut of the muons in the batch, some decay. The overall energy deposited decreases exponentially as: \nN(t) = N_0 e^{-t/\\tau_\\mu}\n where \\tau_\\mu is the muon lifetime.\nThis decay is modulated by the spin precession, fluctuating up and down. The signal follows: \nS(t) = N_0 e^{-t/\\tau_\\mu} [1 + A \\cos(\\omega_a t + \\phi)]\n where \\omega_a is the anomalous precession frequency.\n\nYou collect these statistics for three years, and then you have the number.\nThe analysis is blinded. Two years ago, there was a big press release for the new g-2 measurements. Before starting, the analysts wrote the real frequency on a piece of paper in an envelope. At the press conference, they unblinded the analysis. They opened the envelope, entered the frequency, and revealed the measured numbers.\n\n\n\n\n\n\n\nThe relationship between the anomalous magnetic moment and precession frequency is: \na_\\mu = \\frac{\\omega_a}{\\omega_c} = \\frac{\\omega_a m_\\mu}{eB}\n This is the key equation for extracting a_\\mu from the measured \\omega_a.\n\n\n\n\n\n0.3 Muon Polarization, Anomalous Magnetic Moment, and Hadronic Contributions in g-2 Experiments\nYou’re measuring \\omega_S—it’s actually correct. What enters the field is really this variable.\nThese fluctuations—are they related to \\omega_C or not? As far as you explained, you would measure \\omega_S and plot as many data points as possible.\nIf the two velocities were exactly the same, the spin configurations would propagate identically, so nothing would change. Therefore, up and down fluctuations—no, this is \\omega_C.\nThe key point is that it’s a ratio, a difference that enters. This plot is sensitive to the difference, but I don’t see it now.\nFor some reason, I thought you saw a difference between them, but no. I’m not sure.\n\n\n\n\n\n\n\nKey Physics Context: The discussion involves precision measurements of spin dynamics, where \\omega_S and \\omega_C represent frequencies related to spin and cyclotron motion, respectively. The difference between these frequencies is critical for observing effects like the anomalous magnetic moment (g-2).\n\n\n\n\nThe key question is: Are the other muons polarized when you shoot them into the ring? Does it matter? Muons? Yes, when they come up. Exactly.\nIn the vector boson binding case, this vertex is a weak interaction vertex, and it produces a certain chirality of the state. Muons are always polarized in the binding case. That’s really neat.\nThis spectacular experiment—the next generation is happening now. The most important concern in the setup is the homogeneity of the field. If the field is not exactly B but slightly different, the integral over the magnetic field that the muon acquires while traveling changes, introducing uncertainty.\nTo achieve such precision—quantified in parts per billion (ppb)—is remarkable. Currently, the uncertainty is 63 ppb in the experimental value, and the theoretical value is around 100 ppb.\n\nYou might ask, “What does hadronic physics have to do with this?” Hadronic corrections appear prominently in the theoretical calculation of the anomalous magnetic moment g-2.\nThere are two major hadronic contributions to g-2:\n\nHadronic Vacuum Polarization (HVP):\n\n\nA photon produces hadrons and then turns back into a photon.\nThis is running vacuum polarization.\n\n\nHadronic Light-by-Light (HLbL) Scattering:\n\n\nLight interacts with hadrons and re-emits light.\nMesons like \\pi^0 and \\eta (which decay into two photons) contribute to HLbL.\n\n\nThe photon has quantum numbers J^{PC} = 1^{--}. Which mesons share these quantum numbers?\n\nSpin one means vector mesons. Examples include:\n\\rho^0 (u\\bar{u})\n\\phi (s\\bar{s})\nJ/\\psi (c\\bar{c})\n\\Upsilon (b\\bar{b})\n\nIn the quark model, these mesons belong to the same multiplet. For instance, replacing the u-quark with a c-quark gives J/\\psi.\nIn the strange sector, the vector meson is \\phi (s\\bar{s}).\n\n\n0.4 Dispersion Relations and Analyticity in Complex Analysis\nThe Cauchy theorem tells you that in the complex plane you can integrate in the manner of analyticity. The integral is equal to the integral of the function over S'. This S' is equal to the sum of the 2\\pi i residues of the function at the poles. Does it look familiar? We discussed this already—that you can use this to have an integral representation of the function for any function in the domain of its analyticity.\nYou can write this in the following way, and what you do, actually, is manually introduce a pole by hand such that this integral would be equal. In this integral, it will be equal to the residue of the integrand. This function F that is here is this ratio; it has one pole. Therefore, you have to take the function right at the pole, and that would make you substitute S for S', and then it would make the derivation valid. This is known as the Cauchy theorem, and this is kind of a little consequence of it.\n\nNow, there is a trick, especially for real analytic functions. Imagine that you have a function that is real analytic in the domain below the cut, and then the cut starts at that threshold. We would like to use something like that to evaluate the value. So what I’m going to do is write this representation for this contour: my function G(S) is equal to \\frac{1}{2\\pi i} integral over contour C of \\frac{G(S_1)}{S_1 - S} dS_1. This is my contour C.\n\n\n\n\n\n\nKey Property of Real Analytic Functions (cut discontinuity): \nG(S + i\\epsilon) = G^*(S - i\\epsilon) \\quad \\text{(for real } S \\text{ below threshold)}\n implying \\text{Re}\\, G(S + i\\epsilon) = \\text{Re}\\, G(S - i\\epsilon) and \\text{Im}\\, G(S + i\\epsilon) = -\\text{Im}\\, G(S - i\\epsilon).\n\n\n\nI’m going to blow up the contour. Changing the contour does not change anything. The only singularity that I encounter is the one introduced by hand—it’s a pole in the center or somewhere inside of it. So I’m transforming my plane into the following contour. It has two contributions: the one that comes from the circle and the one from the cut. The one from the cut is actually evaluated along the real axis.\nI’m going to use the property that G(S + i\\epsilon) (with \\epsilon infinitesimal) is related to G(S - i\\epsilon). This is due to the real analyticity. It’s rather intuitively clear, but not necessarily so.\n\nLet’s consider an example just on the side. Any function with such a cut will be real analytic. We had an exercise on that. I want to have an x domain, and then the function has a cut starting at one, going to the right, and the function should be real here. My favorite example is \\sqrt{1 - x}. But \\sqrt{x} could have a cut in the other direction. So I think I should do \\sqrt{1 - x}.\nThis function has a cut to the right; it starts at 1. If I put x = 0, I can evaluate this, no problem. It’s real analytic because for negative x, say x = -8, then I have \\sqrt{9} = 3.\nMy claim is that G(x + i0) has a real and imaginary part. If I calculate this for negative, it has the same real part and negative imaginary part. Let’s check it for some value. At x = 5, G(5 + i0) = \\sqrt{-4 - i0} = 2i. Indeed, it’s a real analytic function. The value above and below the cut has the same real part and opposite imaginary parts.\nThat lets me write this integral as a principal value integral over the imaginary part. Plus, another thing I use is that the integral in one direction is equal to minus the integral in the other direction. That’s what gave me the minus sign here.\n\nIf the function drops fast enough when S goes to infinity, I can blow up my circle even more and drop the contribution from the circle. I forgot a 2i here—so here in the numerator should be 2i. This is a beautiful formula that we derived, known as a dispersion relation.\n\n\n\n\n\n\nDispersion Relation (recovering a function from its imaginary part): \nG(S) = \\frac{1}{\\pi} \\int_{\\text{cut}} \\frac{\\text{Im}\\, G(S')}{S' - S} \\, dS'\n (Assumes G vanishes sufficiently fast at infinity.)\n\n\n\nThis function allows us to recover the full function from its imaginary part. It’s quite amazing. It just tells you what the function is equal to here, and using this relation, I can give it to you anywhere in the complex plane.\nIt’s an advanced subject, the dispersion relation, but it’s part of the classwork. It’s a little exercise that helps you understand better how dispersion works.\n\nThere is also the subtraction. People say we do this dispersion relation with one subtraction. It’s not just one, but two poles are introduced. Then it’s under the integral with the prime. This is the variable I integrate over, and then S - a. Once you do that, you have to multiply this without the prime outside the integral. Then obviously this term is going to vanish once S = a. Therefore, to have the correct relation, you need G(a) here.\nThis is called a once-subtracted dispersion relation. The easiest subtraction is at zero. Then a = 0, and you have just G(S) = G(0) + \\frac{S}{\\pi} (and then an extra S for the integration). That’s what you have to do when your condition for the high circle is not met—when your function does not decrease fast enough. You can still use this dispersion relation; you just have to do one subtraction or two subtractions.\nThe twice-subtracted version would be putting this to power two and then having it as well. Quite amazing—I can’t believe it works.\n\nJust look here. I can use this formulation for my \\sqrt{1 - x} function.\n\n\n\n\n\n\nExample with \\sqrt{1-x} (applied dispersion relation): \n\\sqrt{1 - x} = 1 + \\frac{x}{\\pi} \\int_{1}^{\\infty} \\frac{\\text{Im}\\, \\sqrt{1 - x'}}{x'(x' - x)} \\, dx'\n where \\text{Im}\\, \\sqrt{1 - x'} is non-zero only for x' &gt; 1.\n\n\n\n(Maybe just a technical question: By real analyticity, you mean that the function is real for real inputs up to the threshold?)\nExactly. And this implies the relation.\nSo I think we can also understand this in a way that suddenly—where is my drawing?—suddenly my function was real, and then an imaginary part appeared. The fact that it’s real in the vicinity when I go towards it means that the imaginary part will appear on one side with one sign and on the other side with the opposite sign. So that’s how to understand it.\nThe cut can only—analyticity can only introduce an imaginary part consistently from both sides of the cut, given that it was real here. Right? Would you believe that?\nSo \\sqrt{1 - x} is equal to the integral of \\frac{\\text{Im}\\sqrt{1 - x'}}{\\pi(x' - x)}. And then I have to add here the value at zero, which is 1. Quite nontrivial—like, would you believe that if you just saw it? But mathematically, it works.\nThis is because we started with an analytic function. So this is analytic. We do the circle integral, we use… This can be evaluated explicitly. I could actually tell you what this imaginary part of the square root is—it’s just a Heaviside step function. So we could just have a real expression.\nMathematically surprising, but it works.\n\n\n0.5 Dispersion Relations, Cross Sections, and the R Ratio in Hadronic Processes\nNow we come back to the expression in the last minutes, and I’ll wrap up this sketch of how it works. We relate the imaginary part of this block, through the optical theorem, to the cross section:\n\n\\sigma \\propto \\text{Im}\\,\\mathcal{M}\n\nThe observable is given by the cross section. The cross section is the imaginary part.\nYou put the imaginary part in the numerator, perform the dispersion integral, and we obtain this contribution. It should now make more sense why we start at the threshold and why we extend to this global function.\n\nThe polarization is an analytic function and only has the right-hand cut. It is real and smooth before the threshold. Once we cross the threshold, it develops a branch cut. We compute this polarization using the dispersion relation:\n\n\\Pi(q^2) = \\frac{1}{\\pi} \\int_{s_{\\text{threshold}}}^\\infty \\frac{\\text{Im}\\,\\Pi(s)}{s - q^2 - i\\epsilon} ds\n\nand relate it to the cross section.\n\nThe quantity R is defined as:\n\nR(s) = \\frac{\\sigma(e^+e^- \\to \\text{hadrons})}{\\sigma(e^+e^- \\to \\mu^+\\mu^-)}\n\nThe denominator is the \\mu^+\\mu^- cross section calculated theoretically.\n\n\n\n\n\n\nThe R ratio compares hadronic production to muon pair production in e^+e^- collisions, serving as a probe for QCD dynamics and resonances.\n\n\n\nThis is not directly from experimental measurements, but the R ratio we use here is consistent with experimental data. The theoretical calculations for this process are very close to what is measured in experiments. The exercise sheet provides the minor differences that arise.\n\nThe R ratio reveals all the resonances that can appear. Here is a plot showing the experimental measurement of R. It is quite spectacular, as expected. It includes contributions from resonances like the \\rho, \\phi, and \\Upsilon.\nThere is also a background contribution. All resonances sit on top of this background. The background comes from hadronic contributions—quarks that hadronize not just into single pions or two pions, but into many particles.\n\nThe current status of the problem is a five-sigma mismatch between theoretical calculations and experimental measurements. This discrepancy has persisted for the past two to three years.\nEarlier, there were issues with QED calculations, but those were resolved. Now, the gap suggests potential new physics.\nTheoretical predictions reach a precision of 60 parts per billion, while experimental measurements also achieve 60 parts per billion. Yet, when comparing independent theoretical methods, a five-sigma discrepancy remains:\n\n\\Delta a_\\mu = a_\\mu^{\\text{exp}} - a_\\mu^{\\text{theory}} \\approx (251 \\pm 59) \\times 10^{-11}\n\n\n\n0.6 Challenges in Hadronic Contributions and the Weakening New Physics Signal\nIn later years, two works threatened this new physics discovery. The first is the lattice calculation on hadronic vacuum polarization. The lattice actually computes this quantity, but only part of it, because they work in different domains. They don’t work in the space domain with Q^2, but in the time domain. They interrelate the two and calculate the part of this quantity that’s also possible to evaluate theoretically. And they disagree.\n\n\n\n\n\n\nKey Formula: The Hadronic Vacuum Polarization (HVP) contribution to muon g-2 is given by: \na_\\mu^{\\text{HVP}} = \\left(\\frac{\\alpha}{\\pi}\\right)^2 \\int_{m_\\pi^2}^\\infty \\frac{ds}{s} K(s) R(s)\n where K(s) is a kernel function and R(s) is the hadronic R-ratio from e^+e^- \\to \\text{hadrons}.\n\n\n\nLattice disagrees with phenomenological calculations using perturbative methods, which calls part of this technique into question. This isn’t purely theoretical—there’s experimental input, specifically the R-ratio, which has uncertainties. These uncertainties are still large, and they dominate the error. Currently, the blame falls on the region between the \\rho and \\phi resonances, where experimental data is poor. This mismatch between lattice and experiment likely comes from there.\nIf you take the new lattice data, the discrepancy shrinks from 5\\sigma to 1.8\\sigma. Later last year, the CMD experiment in Novosibirsk published new measurements of this region, specifically around the \\rho resonance. They claim these measurements are inconsistent with past experiments but consistent with the experimental value of g-2 if their values are used.\nThis indication for new physics is weakening slowly, but it remains a puzzle. There’s an experimental effort to build new facilities for more precise measurements. Theoretically, there’s a major effort to properly calculate hadronic effects. Hadronic physics remains the main obstacle in determining whether there’s a true mismatch.\n\nI think we’re done. Questions? We have time. One thing I’d like to clarify: When fitting these oscillations, do you extract W_C or the difference between them? I’ve seen papers with expressions for vacuum polarization that seem to have typos, so I’ll check with colleagues. But roughly, now you have a basic idea—when you hear HLBL or HVP, you’ll know what they mean.\nQuestion: Why can’t we be sure the main contributions are hadronic? Even if we evaluate them correctly, other contributions might just be smaller. Answer: That’s a good point. I misspoke earlier—it’s not that they’re the largest contributions, but they have the leading uncertainty. They’re small, but we don’t know them precisely enough to reduce the uncertainty further. The hadronic parts are the most difficult to calculate.\nThe uncertainty was 600 ppb (parts per billion), but new work has reduced it to 200 or even 100 ppb. Still, it remains the dominant source of uncertainty—we know other contributions much better.\n\n\n\n\n\n\nLight-by-light (HLBL) scattering involves 64 terms and is even more challenging due to virtual photons: \na_\\mu^{\\text{HLBL}} \\propto \\int d^4q_1 \\, d^4q_2 \\, \\mathcal{K}(q_1, q_2) \\Pi^{\\mu\\nu\\rho\\sigma}(q_1, q_2)\n where \\Pi^{\\mu\\nu\\rho\\sigma} is the hadronic light-by-light scattering amplitude.\n\n\n\nLogistics:\n\nThursday: Obstacle course (canceled if weather doesn’t improve).\nFriday at 2:00: Project meeting (everyone invited, Swiss sweets provided).\n\nSee you at Linde Park.\n\n\n0.7 Seeking Sources on SU(N) Representation Theory via Young Tableaux\nI was wondering—maybe a question for a problem of mine. Maybe you can answer it.\nI’m currently looking for a book explaining the representation theory of SU(N). I read in a textbook how you get representations from Young tableaux using representations of S_n. Very exciting.\n\n\n\n\n\n\nThe connection between SU(N) representations and symmetric groups S_n via Young tableaux involves several key formulas:\n\nDimension of SU(N) reps: \\text{dim}(λ) = \\prod_{1 ≤ i &lt; j ≤ N} \\frac{λ_i - λ_j + j - i}{j - i}\nHook length formula for S_n: \\text{dim}(λ) = \\frac{n!}{\\prod_{(i,j) ∈ λ} h_{i,j}}\nBranching rules and tensor products follow Littlewood-Richardson coefficients.\n\n\n\n\nAnd it’s impossible to find a source for that. Z? That’s a book? What do you mean, Z? That’s why I went to [Yugo?].\nThis is rather good level."
  },
  {
    "objectID": "2025-Lecture-01.html",
    "href": "2025-Lecture-01.html",
    "title": "(2025) Lecture 1",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Anna Zimmer\n\n0.1 Introduction to Hadron Physics and Course Logistics\nThe first lecture will be dedicated to the introduction of hadron physics.\nI would like to start by explaining:\n\nWhat hadron physics is\nIts origin\nHow it appeared in the early stages of the universe, where it played a role in the evolution of our universe\n\n\nThen we’ll discuss:\n\nMatter composition and how quark physics plays its own part.\nA brief overview of the Standard Model.\nThe equations that describe the motion of fields.\nThe SU(3) group and the important property of the strong interaction key to matter formation, known as confinement.\n\n\n\n\n0.2 Scaling the Universe’s Timeline from Big Bang to Hadronization\nAs last year, it’s good to start the lecture course by starting at the same time the timeline for the universe and the numbers that we’ll put here for these different epochs and stages of the universe. They are really hard to imagine—either very small or very large. That’s why we need this on a human scale first.\n\nI can put here the number that is where we are now, and this is 14 billion years. That’s what we believe is the age of the universe:\n\nt_{\\text{universe}} \\approx 14 \\times 10^9 \\, \\text{years}.\n\nStarting from the Big Bang, there will be many points.\n\nBut to have a human scale, I would like to do one exercise. Now I will clap my hands here, starting the universe. You watch the distance between my clapping. You see me clapping, and then the sound comes to you. The sound comes to the back row first, and then probably to the last row last.\nWe will put this scale over there. Here was the clap—Big Bang. Here is the point where it reaches the first row, one meter. Since the speed of sound is about 300 meters per second:\n\nt = \\frac{1 \\, \\text{m}}{300 \\, \\text{m/s}} \\approx 0.003 \\, \\text{s}.\n\nIf my clap was the Big Bang, from that moment the universe started evolving. By the time the sound reached you, most of the stages had passed. We will be talking about hadron physics—that’s actually around this time.\n\nI need 10^{-6}, 10^{-12}, 10^{-12}, 10^{-6}. At about one second at the beginning, matter is produced. We don’t know what kind of matter is there, but we assume that the structures started forming already. The seeds of the structure were already produced during inflation—inflation of space and time. We have little idea about what’s going on here.\nWe will advance the time to 10^{-12} seconds, where already the electroweak scale is passed:\n\nE_{\\text{EW}} \\sim 100 \\, \\text{GeV}.\n\nThe Planck scale is passed:\n\nt_{\\text{Planck}} \\sim 10^{-43} \\, \\text{s}.\n\nThe Higgs potential developed its minimum:\n\nV(\\phi) = \\mu^2 |\\phi|^2 + \\lambda |\\phi|^4.\n\nThe universe collapsed to the lower minimum of the Higgs potential, and we arrive at 10^{-12} seconds, where the matter itself is in the quark-gluon plasma (QGP).\nThe dots here are quarks—they are constituents of the matter. The fields that operate in this space are gluons. With this abbreviation, QGP, we refer to quark-gluon plasma. There is no matter as we know it—it’s just a soup in which fields are acting.\n\nThe equation of motion at high temperature around 10^{-6} seconds, this soup starts evolving into structure. In the interval somewhere here, you’ve got the hadronization process:\n\nt_{\\text{hadronization}} \\sim 10^{-6} \\, \\text{s}.\n\nWhen the sound reached you, what happened with our universe is that it already has elementary matter blocks, which are mesons and baryons. Quarks are now confined, and gluon fields are almost entirely sitting inside these objects. Essentially, that’s all we need from this picture.\n\nWhat happens further is another 14 billion years of evolution. Somewhere here I mark one second, which is a good mark, where nuclear synthesis starts:\n\nt_{\\text{BBN}} \\sim 1 \\, \\text{s}.\n\nAnother visually pleasant indication is when radiation separates from the matter. Big Bang nucleosynthesis, and then radiation separates from the matter around—doesn’t have a scale. Does anyone remember the scale here? Probably it’s in this picture. I think it’s quite high.\nYou believe that happens about 400,000 years after the Big Bang:\n\nt_{\\text{recombination}} \\sim 400,\\!000 \\, \\text{years} \\approx 1.2 \\times 10^{13} \\, \\text{s}.\n\nWe’ve got pions and baryons.\n\nFor the rest of the semester, what will happen with our universe, which we just created? There will be some nucleosynthesis starting, maybe. We won’t get to the forming of atoms as we know them, with electron shells—it only happens 400,000 years after.\n\n\n\n\n\n\nKey Scales Recap:\n\nElectroweak symmetry breaking: E_{\\text{EW}} \\sim 100 \\, \\text{GeV}\nPlanck time: t_{\\text{Planck}} \\sim 10^{-43} \\, \\text{s}\nHadronization: t_{\\text{hadronization}} \\sim 10^{-6} \\, \\text{s}\nRecombination: t_{\\text{recombination}} \\sim 400,\\!000 \\, \\text{years}\n\n\n\n\n\n\n0.3 Structure of the Atom: Nucleus, Electrons, and Atomic Scale\nWhat I have here is the electron. So what is an atom? Let’s now go to the second item. What is matter? What’s the most abundant element in the crust of the Earth? By mass, the most abundant element is iron, but in the crust, it’s oxygen.\n\n\n\n\n\n\nOxygen is an element with eight protons and eight neutrons, forming a compact object at the center of the atom—the atomic nucleus. These nucleons are packed together very tightly.\n\n\n\nThe size of a single proton or neutron is roughly one fermi. Imagine 16 balls of one fermi packed together. This gives a nucleus with a diameter of about six fermi. A naive calculation suggests a radius of three fermi, so doubling that gives the six fermi diameter.\n\nElectrons are the other part of the atom. How many do we expect? They occupy shells: \n1s^2 2s^2 2p^4\n The s orbital is circular, while the p orbital has a dumbbell shape.\nThe energy of electrons versus angular momentum shows the lower energy levels: 1s, then 2s, and so on. The wave function of the electron in the 1s orbital is: \n\\psi_{1s}(r) \\propto e^{-r/a_0}\n where a_0 is the Bohr radius. This gives the scale of the atom.\n\nThe Bohr radius a_0 is about 50 picometers (or 59,000 femtometers). Solving the Schrödinger equation for an electron bound to the nucleus gives this scale.\nDoes the nuclear charge affect the radius? Yes. For a nucleus with charge Z = 8, the radius scales inversely. However, outer electrons are screened by inner ones, feeling less charge. This is why outer shells extend to several hundred picometers.\nTo visualize this, compare it to the solar system. If the nucleus is scaled down by a factor of 150 relative to the sun, the distances match roughly. This helps imagine the atomic scale.\n\nThe average radius for the 1s orbital is: \n\\langle r \\rangle_{1s} = \\frac{3}{2}a_0\n For the 2s orbital: \n\\langle r \\rangle_{2s} = 6a_0\n Dividing by the effective charge (e.g., Z_{\\text{eff}} \\approx 4 due to screening) gives the outer radius. This is why a_0 sets the scale, leading to the estimate of around 100 picometers.\n\n\nNuclear size: R_{\\text{nucleus}} \\approx 3 \\text{ fermi (fm)}, Diameter = 6 \\text{ fm}\nEffective charge scaling: r_{\\text{eff}} \\approx \\frac{a_0}{Z_{\\text{eff}}}\n\n\nThe text now uses bold for key terms, italics for less critical emphasis, block math for equations, and a callout for contextual notes. Horizontal rules separate logical sections, and a recap box highlights formulas. All original sentences are preserved.\n\n\n0.4 The Standard Model: Structure, Challenges, and the Strong Interaction\nParticle physics has the most accurate and precise theory that describes everything we have observed so far, which is the Standard Model. It is so accurate that for 10 years now we have been trying to find any single deviation by colliding particles at CERN and elsewhere from its predictions. So far, we haven’t found any—it’s very nice and accurate.\nThere are some problems with the Standard Model, which we will probably touch on at large scales. When we think of the evolution of the universe, there are also questions of naturalness of certain couplings—certain parameters have been measured, and we know them accurately in the Standard Model. But it’s unclear where they come from or why they are like this.\n\n\n\n\n\n\nThe Standard Model’s mathematical structure is captured by:  \\mathcal{L}_{\\text{SM}} = \\mathcal{L}_{\\text{EW}} + \\mathcal{L}_{\\text{QCD}}  where \\mathcal{L}_{\\text{EW}} describes electroweak interactions and \\mathcal{L}_{\\text{QCD}} describes strong interactions.\n\n\n\nRoughly, the Standard Model can be thought of as the electroweak sector times QCD. The fact that the Standard Model is such a good theory and works so well doesn’t mean we understand it fully. This particularly refers to quantum chromodynamics, the interaction that describes the strong force. Everything we talk about in hadron physics is governed by the strong interaction.\nQuantum chromodynamics is the theory of the strong interaction, the theory of the color charge. It works very well for phenomena in physics, but it’s so complicated that until recently, direct predictions from its basic equations to real-world observables were not possible. The reason, which we will discuss later, is that the fundamental interaction describes interactions between its building blocks—quarks and gluons. This is where they were present last time, even before that.\nAfter that, the theory turns into something different, where there are particles with non-trivial structure, not fundamental particles, and the theory starts interacting. Essentially, hadron physics is not so much about the level of quarks in drawings, but what this fundamental theory implies for the form in which it manifests itself. Hadrons manifest as color-neutral objects, where the charge carried by the theory is confined. In this course, we will try to understand the relation of the fundamental part to the effective interaction between hadrons.\n\nTo match this picture of the Standard Model being two parts, it’s worth expanding further. The electroweak sector and the leptonic sector contain three parts:\n\nElectromagnetic interaction\nWeak interaction\nThe mass-energy sector\n\nLet’s quickly list the particles. I’ll write down the particle symbol, and you tell me to which sector it belongs.\n\nHiggs? Very good—it belongs to the Higgs sector.\nW boson? Yes, from the weak sector.\nZ boson? Also weak. These are the carriers of the weak charge.\nPhoton? That belongs to the electromagnetic sector.\n\nNow, quarks:\n\nTop quark?\nCharm quark?\n\nThese belong to the strong interaction—QCD. These are the elementary particles that make strong interactions. This classification is a bit vague because we mostly talk about quarks. The electroweak sector deals with leptons, electromagnetic with photons, and weak interaction with W and Z bosons and Higgs as a separate field.\nHowever, if a particle has a charge of a certain type, it can interact with the carriers of that charge. So, let’s describe the charges. The quarks are organized in generations: first, second, third. They are also organized in two rows—top and bottom.\nThe charges are:\n\nElectric charge:\nTop row: +\\frac{2}{3}\nBottom row: -\\frac{1}{3}\nWeak charge:\nUpper row: +\\frac{1}{2}\nLower row: -\\frac{1}{2}\n\nA quark with +\\frac{2}{3} electric charge and +\\frac{1}{2} weak charge can couple to carriers of those charges. Quarks are the most diverse particles in the Standard Model because they couple to all charges: electromagnetic, weak, and strong. For example, a quark can interact with:\n\nLight (electromagnetic)\nZ or W bosons (weak)\nGluons (strong)\n\nThe strong charge is also called color charge.\n\nColor charge and strong charge are the same thing. Just as an electron can be positive or negative, color charge can be:\n\nRed\nGreen\nBlue\n\nOr their antiparticles:\n\nAnti-red\nAnti-green\nAnti-blue\n\nYou’ll quickly get used to this language—strong charge as color charge. In hadron physics, we’ll be talking about the strong interaction, which is also the color interaction.\n\n\n0.5 Interaction Fields and the Lagrangian Formulation\nWhen we say interaction, it’s interaction of the object with the field. The fields are represented by these carriers of the force, which are $W $ and $Z $ for the weak interaction, photons for the electromagnetic interaction, and gluons for the strong interaction.\n\nWe discussed the Standard Model composition, and we’re reaching unification. The standard framework in field theory to describe the fields and the interaction between particles and carriers is field theory, which starts with the Lagrangian—an expression that describes interaction in a very condensed line.\n\n\n\n\n\n\nThe Standard Model Lagrangian includes terms for kinetic energy, gauge interactions, the Higgs mechanism, and Yukawa couplings: \n\\mathcal{L}_{\\text{SM}} = \\mathcal{L}_{\\text{kinetic}} + \\mathcal{L}_{\\text{gauge}} + \\mathcal{L}_{\\text{Higgs}} + \\mathcal{L}_{\\text{Yukawa}}\n\n\n\n\n\nHere is an example. You might remember Lagrangian mechanics from your first or second semester, where the entire motion of the system was condensed down to a single equation. This was the Lagrange equation:\n\n\\mathcal{L} = T - V\n\nwhere $T $ is the kinetic term and $V $ is the potential term. Once you subtract them, you get an expression representing the energy of the system.\n\nIn that case, kinetic minus potential gives the equation of motion. What happens with the system in the next moment, starting from the initial state, is described by an equation derived from the Lagrangian.\nTo find it, you differentiate the Lagrangian by the velocity $ $, then subtract the term that differentiates the Lagrangian by the coordinate $q $:\n\n\\frac{d}{dt}\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}}\\right) - \\frac{\\partial \\mathcal{L}}{\\partial q} = 0\n\nThis describes a system where a point can move without friction, or a pendulum with another mass attached. The equation of motion is a differential equation, found by applying the classical equivalent of this formula.\n\n\n0.6 Lagrangian Structure in QED and QCD: Fields, Indices, and Equations of Motion\nI will clarify this equation now because this is the one we will use for the fields. Let me first start writing.\nLagrange—remember this or less. I start with QED (quantum electrodynamics). It describes how light interacts with anything that has a charge. It’s relevant for us for two reasons:\n\nOur quarks have charge—that’s why they interact with photons.\nThe Lagrangian for quantum electrodynamics is simpler than quantum chromodynamics.\n\nLet’s use this chance to understand all the symbols before we proceed to QCD later on.\n\nSo, who sees the equation for the first time? It’s complicated, but once you understand the general structure, you don’t need to look it up to write it down.\nThe Lagrangian is a function of two fundamental fields:\n\nOne is \\psi (psi), the field of an electron, muon, or quark—something with charge.\nThe other is A, the photon field.\n\n\\psi is a fermion field, and A is a vector field.\n\nThe Lagrangian is a scalar quantity—not a vector, not a matrix, just a number when evaluated at any point. A scalar is achieved by dealing with indices: every index introduces a dimension, and you only get a scalar when all indices match. We use Einstein notation, where repeated indices imply summation—same as in quantum mechanics.\nHere, \\mu and \\nu are Lorentz indices. \\mu lives in four dimensions—three spatial (x, y, z) and one time. The \\mu here and \\mu here must be contracted. I’m skipping the explicit summation over \\mu from 1 to 4, as well as the summation over \\nu, which also appears twice.\n\nYou can think of F_{\\mu\\nu} as a matrix—\\mu is 4-dimensional, and \\nu is 4-dimensional, so F_{\\mu\\nu} is a 4 \\times 4 matrix. When you multiply, you don’t multiply matrices the usual way—you multiply them component-wise. Every component is multiplied by itself, and then you take the trace (or sum all elements). In Python terms, you broadcast the matrix element-wise and then sum all elements—that’s what happens here.\nEach component of F_{\\mu\\nu} is computed as: \nF_{\\mu\\nu} = \\partial_\\mu A_\\nu - \\partial_\\nu A_\\mu\n where \\partial_\\nu is the derivative in time and space—essentially \\partial/\\partial x.\n\n\n\n\n\n\n\nThe QED Lagrangian is given by: \n\\mathcal{L}_{\\text{QED}} = \\bar{\\psi} (i \\gamma^\\mu D_\\mu - m) \\psi - \\frac{1}{4} F_{\\mu\\nu} F^{\\mu\\nu}\n This describes fermion-photon interactions, with \\psi as the fermion field and A_\\mu as the photon field.\n\n\n\n\nThis would be simple if it were just \\mu contracted with \\mu. If you think of this as a vector, it’s not a vector. They can contract them—scalar product, easy. However, the trick is that this is also a matrix. So it’s four matrices. How do we get a scalar quantity? There’s another set of indices I suppressed—they belong to this matrix.\nLet me make it explicit: what about \\tau and \\rho? In which space are they? While \\mu is the Lorentz index (four spacetime dimensions), \\tau and \\rho are related to spin. Particles are not scalar; they have spin. That’s why the fermion field \\psi has four components.\nAre these upper indices? No, they’re matrix indices, not Lorentz indices. For matrix indices, covariant and contravariant distinctions don’t apply. Only Lorentz indices distinguish between upper and lower.\n\nNow, something still seems off in this Lagrangian. We agree these are four matrices, and this is a vector, so we can contract them. We get a matrix here, but then we subtract a scalar—that doesn’t work. What’s missing is the diagonal matrix.\n\\psi is a four-component spinor, and \\bar{\\psi} is its adjoint—not a spinor but a row vector. We conjugate and transpose (\\dagger), then multiply by \\gamma^0 from the left to keep it as a row. Now we can contract this with the matrix here.\n\nOne of the exercises is to analyze the same structure for the QCD Lagrangian, which I’ll write next. Once you do it once, it becomes clear.\nLet’s do QCD now—it’s not too bad. The exercise is to recover the indices, their range, and the number of terms. For QCD, we introduce a color index G because every additional index expands the dimensionality.\nThe QCD Lagrangian is: \n\\mathcal{L}_{\\text{QCD}} = \\sum_f \\bar{\\psi}_f (i \\gamma^\\mu D_\\mu - m_f) \\psi_f - \\frac{1}{4} F^a_{\\mu\\nu} F^{a,\\mu\\nu}\n where:\n\n\\psi_f is the quark field for flavor f,\nD_\\mu = \\partial_\\mu - i g_s \\lambda^a A^a_\\mu is the QCD covariant derivative,\n\\lambda^a are the Gell-Mann matrices (SU(3) generators),\nF^a_{\\mu\\nu} = \\partial_\\mu A^a_\\nu - \\partial_\\nu A^a_\\mu + g_s f^{abc} A^b_\\mu A^c_\\nu is the gluon field strength tensor.\n\n\nNow, let’s check the dimensionality. There’s a new object \\lambda here—these are 3 \\times 3 matrices. The indices I, J correspond to color space. When you commute two matrices, you get another matrix. Overall, this is a matrix in \\mu, \\nu (4D spacetime) and I, J (3D color space).\nI’ll try to make more sense of this equation once all indices are introduced. I really want you to understand the mathematical structure. Everyone can trace this.\nThe index F traces flavors—quarks u, d, s, c, t, b. We have six flavors, so F runs over all six. The spinors here have four dimensions for spin and an extra three for color. The index I traces the color charge. The only thing I’m not tracking is the spinor indices.\n\nFor the QED Lagrangian, we agreed there are \\tau, \\rho indices for spin projection. We omit them here to avoid complexity. But if you think of this field, it has:\n\nA flavor (e.g., up quark),\nA color (e.g., red),\nAnd four spinor components for spin projections.\n\n\nFinally, once you know the Lagrangian, you can derive the equations of motion by applying: \n\\partial_\\mu \\left( \\frac{\\partial \\mathcal{L}}{\\partial (\\partial_\\mu \\phi)} \\right) - \\frac{\\partial \\mathcal{L}}{\\partial \\phi} = 0\n For \\psi, the derivative is non-zero only where \\partial_\\mu \\psi appears. Expanding D_\\mu \\psi, we have \\bar{\\psi} \\gamma^\\mu D_\\mu \\psi. Differentiating, \\bar{\\psi} remains, and \\partial \\mathcal{L} / \\partial \\psi comes from the mass term. This gives a differential equation for how the field evolves in time and space.\nIf we apply this to QCD, do you know what the equation is called that describes the motion of a fermion?\n\n\n\n\n\n\n\nThe Dirac equation describes fermion dynamics in QED/QCD: \n(i \\gamma^\\mu \\partial_\\mu - m) \\psi = 0\n In QCD, this generalizes to the covariant form with D_\\mu. The gluon field dynamics are governed by the Yang-Mills equations: \nD_\\mu F^{a,\\mu\\nu} = g_s \\bar{\\psi} \\gamma^\\nu \\lambda^a \\psi\n\n\n\n\n\n\n0.7 Gauge Symmetry, Phase Ambiguity, and Confinement in Field Theory\nWhile I’m cleaning the board, let me ask questions for this point. We discussed the Lagrangian equation of motion. Let’s discuss the gauge transformation and gauge. That’s an extremely important concept in field theory, and we will only touch on this briefly since we are not doing field theory, but it’s important—you know where Legendre transforms come from.\n\nSomething very familiar from quantum mechanics is the fact that we have a phase ambiguity for the wave function. We can update the phase, and the absolute square of the wave function will not change because you multiply \\psi by \\psi^*, and the phase drops out—and that’s fine. First, we must acknowledge that this should be a symmetry of our theory. This should be allowed—to make this gauge, to make this phase transformation. But it’s kind of by definition, so this overall phase you don’t even have to think of.\n\nThe problem appears when you demand your theory to be invariant under the change of the phase at all possible space-time points at the same time—different phases. What if I want to adjust my wave function at every space point? Why is this a problem? Because in our equation of motion, in our Lagrangian—let me just write the Dirac part—there is the \\overline{\\psi} D_\\mu \\gamma^\\mu \\psi.\nHere, I have a term D_\\mu \\psi, and this term is going to become D_\\mu \\psi', which is D_\\mu of e^{i \\alpha(x)} \\psi. Let me take the derivative:\n\nD_\\mu \\psi' = e^{i \\alpha(x)} (D_\\mu \\psi + i (\\partial_\\mu \\alpha) \\psi).\n\nThe way I apply the derivative is to apply it to the first term and then to the second term. When I apply it to the second term, I just have D_\\mu \\psi. When I apply it to the first term, I get D_\\mu acting on the exponent, yielding an extra term with \\partial_\\mu \\alpha.\nWhat happens? The same equation won’t hold any longer for \\psi'. It does not transform into the same equation because the derivative introduces an additional term with D_\\mu \\alpha. This means that local gauge transformation is not a symmetry of the free Lagrangian of the Dirac particle. You cannot adjust the phase independently at different points for the free particle.\n\n\n\n\n\n\nThe Dirac Lagrangian with gauge field interaction is: \n\\mathcal{L} = \\bar{\\psi} (i D_\\mu \\gamma^\\mu - m) \\psi\n where D_\\mu = \\partial_\\mu - i e A_\\mu is the covariant derivative.\n\n\n\nIn simple words, this is an incomplete theory. It only becomes complete if you consider radiation, photons, and charged particles together. When you look at the full Lagrangian, you update the phase of the field \\psi and the electromagnetic field simultaneously. The additional term in the covariant derivative D_\\mu cancels exactly the one from the phase, leaving the Lagrangian invariant.\nThis fact led us to the D_\\mu term. From the equation of motion, you can see how different fields are coupled. In our pendulum example, we saw how the pendulum affects the upper marble. Similarly, fermion fields affect photons, and photons affect fermions. Gauge symmetry dictates how they interact, enforcing a specific interaction strength g.\n\nBefore jumping to QCD, where we deal with three dimensions, let’s consider the weak interaction with two components: up and down. The weak charge for quarks is \\pm 1/2. The transformation that updates the phase is more general here. We want observables like \\psi^\\dagger \\psi to remain unchanged. The transformation is unitary, described by unitary matrices, forming the group SU(2).\nAny element of SU(2) can be written as:\n\nU = e^{i \\alpha_a \\sigma^a},\n\nwhere \\sigma^a are the Pauli matrices (generators of SU(2)). The condition \\det(U) = 1 comes from SU(2) being a standard group—we factor out the phase (U(1)) and work with SU(2).\nFor SU(3), we have three components (red, blue, green). The transformation is a 3x3 matrix:\n\nU = e^{i \\alpha_a \\lambda^a},\n\nwhere \\lambda^a are the Gell-Mann matrices. The number of generators (8 for SU(3)) corresponds to the number of gluons.\n\nConfinement is the property that the strong interaction grows with distance. Unlike electromagnetism, where attraction weakens with distance, quarks experience increasing force when pulled apart. This confines quarks within hadrons (mesons and baryons). If you try to separate them, new quark pairs form, maintaining color neutrality.\nLooking at the QCD Lagrangian, gluon self-interaction terms indicate confinement:\n\nG_{\\mu\\nu}^a = \\partial_\\mu A_\\nu^a - \\partial_\\nu A_\\mu^a + g f^{abc} A_\\mu^b A_\\nu^c.\n\nThese terms lead to vertices with three or four gluons, a signature of confinement.\nThe running coupling \\alpha_s(Q) describes how the interaction strength changes with momentum:\n\n\\alpha_s(Q) \\approx \\frac{1}{\\beta_0 \\ln(Q^2 / \\Lambda_{\\text{QCD}}^2)}.\n\nAt high Q (asymptotic freedom), \\alpha_s is small; at low Q (confinement), it grows.\n\n\n0.8 Exponential Decay and Half-Life Relationships\nWhen we examine radioactive decay, the number of nuclei decreases exponentially over time. The formula is:\nN(t) = N_0e^{-λt}\nThe half-life, when half the nuclei have decayed, is related to the decay constant by:\nt_{1/2} = \\frac{\\ln(2)}{λ}\n\n\n\n\n\n\nThis shows the fundamental relationship between the decay rate (λ) and the characteristic time (t_{1/2}) for radioactive processes. The decay constant λ represents the probability per unit time that a nucleus will decay.\n\n\n\nJust like a ticking clock measures regular intervals, the half-life measures probabilistic decay intervals.\n\n\nKey features of radioactive decay:\nExponential decrease in number of nuclei\nCharacterized by decay constant λ\nHalf-life t_{1/2} is a more intuitive measure of decay rate\nUniversal behavior across all radioactive isotopes\n\nThe mathematical relationship between λ and t_{1/2} applies to all exponential decay processes in nuclear physics."
  },
  {
    "objectID": "2025-Lecture-02.html",
    "href": "2025-Lecture-02.html",
    "title": "(2025) Lecture 2",
    "section": "",
    "text": "Presenter: Farah Afzal\nNote Taker: Anna Zimmer\n\n0.1 Classification of Hadrons and the Role of Isospin\nWelcome everyone to today’s lecture. We will mostly discuss the classification of hadrons. I will walk you through the history of how important discoveries were made, what we learned from them about hadrons, and how we can group them together.\n\nIn the 50s and 60s, there were a lot of new big accelerators built, like the Bevatron.\n\nIt came into operation in 1954.\nIt was a proton accelerator with energies of up to 13 GeV.\n\nThese high-energy protons were shot at a fixed target, and a lot of different particles were produced and detected—they found over 100 new particles. This was called the “particle zoo.” This term was first mentioned by Oppenheimer at a high-energy physics conference.\nThe physicists of the time had to think:\n\nHow can we organize these particles?\nIs there some pattern?\nAre these all fundamental particles?\n\nThis is what we will be talking about today.\n\nIf you think about the periodic table, for example, we have all these atoms grouped together according to their proton and neutron numbers. We also know how many electrons are in the outermost shells, which helped us arrange them in the periodic table.\nThis is similar to what we want to do now with hadrons. In the context of particle physics, the characteristics we choose to group them together are based on their quantum numbers. We will now discuss different quantum numbers—why they were introduced and how they helped us classify particles.\n\nIn 1932, the neutron was discovered by Chadwick, and experiments—including those at the Bevatron—showed that:\n\nPhoton-proton\nProton-neutron\nNeutron-neutron\n\ninteractions had very similar strengths. The interaction rates were basically the same. The masses of the proton and neutron are also almost identical at 939 MeV. This led to the suggestion that the proton and neutron could be considered the same particle in two different states, described by isospin.\n\n\n\n\n\n\nIsospin Symmetry: The strong interaction does not distinguish between a proton and a neutron. The only difference appears in an electromagnetic field—they are the same particle, the nucleon, in an isospin doublet.\n\n\n\nThis is analogous to electron spin, where we have spin up or spin down. At the quark level, protons and neutrons are composed of up and down quarks, so the strong interaction does not distinguish between up and down quarks.\nThe up quark has an isospin of I = \\frac{1}{2}, with the third component I_3 = +\\frac{1}{2}, and the down quark has I_3 = -\\frac{1}{2}.\n\nIsospin is not exactly like spin, but mathematically we can treat it similarly, following SU(2) algebra. The generators of SU(2) are the Pauli matrices. For isospin, we write the operators as I instead of J for spin.\nThe projections I_3 range from I to -I in integer steps, giving 2I + 1 states: \n2I + 1\n\n\nFor mesons, we combine a quark and an antiquark. If we combine up and down quarks (each with I = \\frac{1}{2}), the possible total isospins are 0 or 1. The dimensions are 2 \\times 2 = 3 + 1, corresponding to:\n\nA triplet (I = 1)\nA singlet (I = 0)\n\nFor I = 1, there are three projections (I_3 = +1, 0, -1), and for I = 0, just one (I_3 = 0).\nIn group theory terms, this is written as: \n\\mathbf{2} \\otimes \\mathbf{\\overline{2}} = \\mathbf{3} \\oplus \\mathbf{1}\n\nThe triplet consists of pions (\\pi^+, \\pi^0, \\pi^-) or rho mesons, while the singlet is the \\eta'.\n\nThe Clebsch-Gordan coefficients describe how these states combine. For example, combining two I = \\frac{1}{2} states gives: \n|1, +1\\rangle = |\\frac{1}{2}, +\\frac{1}{2}\\rangle |\\frac{1}{2}, +\\frac{1}{2}\\rangle\n \n|1, 0\\rangle = \\frac{1}{\\sqrt{2}} \\left( |\\frac{1}{2}, +\\frac{1}{2}\\rangle |\\frac{1}{2}, -\\frac{1}{2}\\rangle + |\\frac{1}{2}, -\\frac{1}{2}\\rangle |\\frac{1}{2}, +\\frac{1}{2}\\rangle \\right)\n \n|0, 0\\rangle = \\frac{1}{\\sqrt{2}} \\left( |\\frac{1}{2}, +\\frac{1}{2}\\rangle |\\frac{1}{2}, -\\frac{1}{2}\\rangle - |\\frac{1}{2}, -\\frac{1}{2}\\rangle |\\frac{1}{2}, +\\frac{1}{2}\\rangle \\right)\n\nFor antiquarks, the isospin projections are inverted:\n\nAnti-up has I_3 = -\\frac{1}{2}\nAnti-down has I_3 = +\\frac{1}{2}\n\n\nNow consider a pion beam hitting a proton target.\n\nThe pion has I = 1\nThe nucleon has I = \\frac{1}{2}\n\nCombining these gives possible total isospins of \\frac{3}{2} or \\frac{1}{2}: \n\\mathbf{3} \\otimes \\mathbf{2} = \\mathbf{4} \\oplus \\mathbf{2}\n\nThe \\frac{3}{2} states correspond to the delta resonances (\\Delta^{++}, \\Delta^+, \\Delta^0, \\Delta^-), while the \\frac{1}{2} states are nucleon-pion interactions.\n\nFor baryons, combining three quarks (\\mathbf{2} \\otimes \\mathbf{2} \\otimes \\mathbf{2}) gives: \n\\mathbf{2} \\otimes \\mathbf{2} \\otimes \\mathbf{2} = \\mathbf{4} \\oplus \\mathbf{2} \\oplus \\mathbf{2}\n\nThe quadruplet is the \\Delta resonances, and the doublets are spin-\\frac{1}{2} baryons.\n\nIsospin also explains differences in cross sections. For example, the cross section for \\pi^+ p scattering is three times larger than for \\pi^- p in the delta resonance region.\n\nThis concept originated in the 50s, before quarks were discovered. Physicists noticed that:\n\nProton-proton\nProton-neutron\nNeutron-neutron\n\ninteractions had similar strengths, suggesting the strong interaction doesn’t distinguish between them. Later, this was extended to quarks, where the strong interaction doesn’t distinguish between up and down quarks.\n\n\n0.2 Strangeness, Isospin, and the Eightfold Way: Patterns in Particle Multiplets\nFirst, they had this kind of an isospin doublet. Then they found all these other particles like the pions and K’s and so on. They grouped them together and looked for bigger patterns with also the strangeness included as the next quantum number. There are deltas. Do you remember when that was first found? Around the 60s. With the Bevatron experiment, one of the first large accelerators in Berkeley, they found over 100 particles and didn’t know what to do with them. Were they like atoms or not? Later they found out they could decompose them into smaller particles and understand how. We are still trying to understand.\n\nTake a three-minute break while I clean the blackboard. Then we will continue with strangeness.\n\nStrangeness. Some of the particles they detected behaved in a strange manner. For example, in cosmic rays, pions hit a carbon target. In a cloud chamber, they saw four tracks forming a V-shape: $ ^+ $, $ ^- $, proton, and $ ^- $. These particles always appeared in pairs and had a fairly long lifetime. They concluded by introducing a new quantum number, strangeness, which must be conserved.\n\n\n\n\n\n\nStrangeness conservation:\n\nStrong interactions: \\Delta S = 0\nWeak interactions: \\Delta S \\neq 0 (e.g., \\Lambda \\to p^+ + \\pi^-)\n\n\n\n\n\nI forgot to mention earlier that isospin is also a quantum number conserved in strong interactions. How do we assign this quantum number? Particles consisting of up and down quarks have strangeness 0, like pions, neutrons, and protons. For others, like lambda, strangeness is +1. These are called hyperons. There are different types: lambda, sigmas, cascades, and kaons. We can form isospin doublets with $ K^+ $ and $ K^0 $, and similarly $ K^- $ and $ $ in a doublet with strangeness -1.\nSince these particles always appear in pairs, strangeness must be conserved in strong interactions but not in weak interactions. For example, in the lambda decay $ p^+ + ^- $, neither the proton nor the pion has strangeness, but the decay happens. The long lifetime indicates weak decay. However, they are produced in strong interactions, like $ + p + K^0 $.\n\nWe can define hypercharge Y as: \nY = B + S\n where:\n\nB is baryon number\nS is strangeness.\n\nFor baryons, B = +1; for antibaryons, B = -1; for mesons, B = 0.\n\nTo clarify, certain reactions are forbidden. For example, if we replaced the lambda with a proton, the reaction wouldn’t happen. Similarly, a $ ^- $ in a proton beam cannot produce only two mesons—a baryon must also be present, conserving baryon number.\nFrom isospin, we saw a correlation with charge. For the isospin triplet $ ^+ $, $ ^0 $, $ ^- $, the charges are +1, 0, -1. This relationship is captured by the Gell-Mann–Nishijima formula: \nQ = I_3 + \\frac{Y}{2}\n where:\n\nQ is charge\nI_3 is the third component of isospin\nY is hypercharge.\n\n\nLet’s test this for the proton:\n\nI = \\frac{1}{2}, I_3 = +\\frac{1}{2}\nStrangeness S = 0, baryon number B = 1, so Y = 1.\n\nPlugging in: \nQ = \\frac{1}{2} + \\frac{1}{2} = +1\n which matches the proton’s charge. For the neutron, I_3 = -\\frac{1}{2}, giving Q = 0.\n\nUsing strangeness and hypercharge with isospin, Gell-Mann and Ne’eman found larger patterns, arranging particles into bigger multiplets. This became known as the eightfold way.\nFor these patterns, they included the strange quark alongside up and down, working in SU(3) flavor symmetry. At the time, quarks weren’t known, but we can describe it this way. Plotting the third component of isospin against strangeness for up, down, and strange quarks reveals the structure.\nHere are the quantum numbers for these quarks:\n\nBaryon number: B = \\frac{1}{3} for each quark (so three quarks give B = 1 for a baryon).\nCharge: Q_u = +\\frac{2}{3}, Q_d = -\\frac{1}{3}, Q_s = -\\frac{1}{3}.\nSpin: all are fermions with spin \\frac{1}{2}.\nStrangeness: S_s = -1 for the strange quark, S_{u,d} = 0.\n\n\nNow consider baryons, composed of three quarks (up, down, or strange). The combinations form a decuplet and octets: 3 \\otimes 3 \\otimes 3 = 10 \\oplus 8 \\oplus 8 \\oplus 1. We focus on the ground states: an octet and a decuplet.\nPlotting the third component of isospin against strangeness (or hypercharge), we see patterns. On the horizontal axis, isospin multiplets appear: a doublet for neutron and proton, a triplet for sigmas, and a doublet for cascades. The quark content varies: one strange quark for some, two for others.\n\nMass differences reveal symmetries:\n\nAlong the horizontal axis (same strangeness), masses are nearly equal (e.g., proton and neutron differ by ~1 MeV).\nVertically (changing strangeness), mass differences are larger (~250 MeV for sigmas, ~130 MeV for cascades).\n\nThis shows SU(2) isospin is a good symmetry, but SU(3) flavor symmetry is broken, as masses aren’t equal.\n\nThe octet baryons have spin-parity J^P = \\frac{1}{2}^+, while the decuplet has J^P = \\frac{3}{2}^+. At the time, the \\Omega^- was predicted by Gell-Mann and discovered two years later in bubble chamber experiments: K^- + p \\to \\Omega^- + K^+ + K^0. Strangeness conservation required additional kaons. The \\Omega^- decayed as \\Omega^- \\to \\Xi^0 + \\pi^-, then further to \\Lambda^0 + \\pi^0, and finally to p + \\pi^- and photons.\nGell-Mann also predicted the \\Omega^- mass. The mass spectrum shows spacing:\n\n\\Delta at 1232 MeV\n\\Sigma^* at 1385 MeV\n\\Xi^* at 1530 MeV\n\\Omega^- at 1680 MeV\n\nwith ~150 MeV between levels.\n\nThese patterns come from group theory, describing particles mathematically. The mass difference between octet and decuplet arises from spin-spin or spin-orbit interactions, reflecting the dynamics.\n\n\n0.3 Baryon and Meson Multiplets: Quantum Numbers and Decay Constraints\nThis is for the baryons, and we can also do a similar multiplet for the mesons. Let me exchange the blackboards.\nFor the mesons, if we think again of the up, down, and strange quarks, we have the SU(3) flavor symmetry composition: \n3 \\otimes \\overline{3} = 8 \\oplus 1.\n Here we have it.\nOn the diagonal, you always have the charge. For example:\n\nHere all of these particles on this diagonal have a charge of -1.\nIn the middle, the charges are 0, +1, and +2.\n\nThis octet—or nonet if we include the \\eta'—has the quantum numbers 0^-, 0^+. This means the quark-antiquark pair have antiparallel spins, resulting in parity P = -1. I will explain charge conjugation shortly—it is +1.\nFor the parallel spin alignment, we have:\n\nA spin of 1,\nParity P = -1,\nCharge conjugation C.\n\nI will explain it.\n\n\n\n\n\n\nKey Definitions:\n\nParity inverts the spatial coordinates.\nCharge conjugation converts a particle into its antiparticle.\n\n\n\n\nFor baryons, if you know the orbital angular momentum L, you can calculate parity using: \nP = (-1)^L.\n For the octet, these are ground states with L = 0, so parity is P = +1 for both the \\frac{1}{2}^+ and \\frac{3}{2}^+ states.\nFor mesons, parity is given by: \nP = (-1)^{L+1},\n which gives P = -1 for both cases here.\nCharge conjugation is only well-defined for neutral mesons. For example, applying it to \\pi^0 gives a +1 eigenvalue: \nC|\\pi^0\\rangle = +1|\\pi^0\\rangle.\n\nWhen analyzing particle decays, it is important to consider these quantum numbers:\n\nParity and C-parity are multiplicative.\nYou also need to know how to combine spins of different particles to get the total spin.\n\n\nExample: Can an \\omega meson decay into two pions?\n\nConsider C-parity: for \\pi^0, C = +1.\nMultiplying for two pions gives C = +1, but for \\omega, C = -1, so this decay is forbidden.\n\nThere are many more examples to explore in the exercises.\n\n\n0.4 Resolving the Omega Minus and Delta Plus Plus Symmetry Problem with Color Charge\nLastly, I want to discuss one thing. None of you complained when I wrote down the quark content for the omega minus or the delta plus plus. I told you all of these have spin aligned—all have spin up. You see their flavor content is also the same. Do you see a problem here? What are quarks? Yes, with Heisenberg’s exclusion principle. Exactly.\n\nWhen we have fermions, and they form a state, they should be distinguishable by at least one quantum number. Here, everything is the same—same spin, same flavor. Let’s look at the total wave function of these baryons.\nFirst, we have the spatial, spin, and flavor wave functions. For L = 0, the spatial wave function is symmetric. The spins are all aligned up, so the spin wave function is also symmetric. If you exchange any quark pair, the state remains the same—that’s what symmetric means. The flavor wave function is also symmetric since all quarks are identical.\n\n\n\n\n\n\n\nThe total wave function of baryons (fermions) must be antisymmetric under exchange: \\Psi_{\\text{total}} = \\psi_{\\text{spatial}} \\otimes \\psi_{\\text{spin}} \\otimes \\psi_{\\text{flavor}} \\otimes \\psi_{\\text{color}} For L=0 states like \\Delta^{++} or \\Omega^-, the spatial, spin, and flavor parts are symmetric, so the color wave function must be antisymmetric: \\psi_{\\text{color}} = \\frac{1}{\\sqrt{6}}(rgb - rbg + brg - bgr + gbr - grb)\n\n\n\n\nWhat do physicists do when we can’t solve this? We introduce a new quantum number. Which quantum number could it be? Yes, color charge. This was already discussed a little last time.\nThis part of the wave function must be antisymmetric to make the total wave function antisymmetric. We assign each quark a different color—red, green, blue. That’s what makes them distinct.\n\nKey points:\n\nQuarks are fermions, so their total wave function must obey the Pauli exclusion principle.\nFor baryons with identical flavor and spin, the color charge provides the necessary distinction.\nThe antisymmetric color wave function ensures the total wave function is antisymmetric.\n\n\n\n0.5 Evidence for Three Quark Colors from e^+ e^- Annihilation Cross-Section Ratios\nLet’s very briefly discuss one last experiment, and then we are done. Last time, I mentioned that we have three different colors. How do we actually know that there are three and not, say, six or something like that? I mean, we have six types of quarks.\n\nSo there were experiments done that actually showed us that it has to be three. What they did was look into e^+ e^- annihilation. Then you can have a virtual photon that is exchanged, and you can have different types of particles produced here:\n\nYou can have quark-antiquark pairs, or\nYou can also have leptons like \\mu^+ \\mu^-.\n\nFor the quarks, we have the different types that we know. This gives us basically the mass of the produced systems.\n\n\n\n\n\n\n\nThe key experimental observable was the cross-section ratio R, comparing hadronic production to lepton pair production: \nR = \\frac{\\sigma(e^+ e^- \\to \\text{hadrons})}{\\sigma(e^+ e^- \\to \\mu^+ \\mu^-)}\n At leading order, this simplifies to: \nR = N_c \\sum_q e_q^2\n where N_c is the number of colors and e_q is the electric charge of quark flavor q.\n\n\n\n\nWhy measure this ratio? Because for the leptons, we don’t have any color. So if we divide by this, we should get something proportional to the number of colors we have. That’s the idea behind it.\nHere, a photon is exchanged at this vertex, with proportionality to the charge squared. Then we have the different types of quarks. Initially, you have just up, down, and strange quarks, so you can measure, for example, the \\rho, \\omega, or \\pi mesons as peaks in the spectrum.\nWhen the energy is high enough, you can also produce charm-anticharm bound states—these are called J/\\psi states, for example. This is what happens starting here. Then, at some point, the energy becomes high enough to also have bottom-antibottom states.\n\nThe quark charge contributions change as more flavors become accessible:\n\nFor the light quarks (u, d, s): \n\\sum_q e_q^2 = \\left(\\frac{2}{3}\\right)^2 + \\left(-\\frac{1}{3}\\right)^2 + \\left(-\\frac{1}{3}\\right)^2 = \\frac{2}{3}\n\nWhen charm threshold is passed: \n\\sum_q e_q^2 = \\frac{2}{3} + \\left(\\frac{2}{3}\\right)^2 = \\frac{10}{9}\n\nAnd when bottom threshold is passed: \n\\sum_q e_q^2 = \\frac{10}{9} + \\left(-\\frac{1}{3}\\right)^2 = \\frac{11}{9}\n\n\n\nThis can be calculated exactly to determine how large these steps are, and what they found was that the number of colors must be three. We’ll discuss this more later.\n\n\n0.6 Historic Development of the Quark Model and Particle Discoveries\nI just want to leave a couple of minutes for questions. This was a little bit of historic background—how from the different particles that were detected, different patterns were seen using the eightfold way (S for strangeness).\n\n\n\n\n\n\nThe Gell-Mann–Nishijima formula relates these properties: Q = I_3 + \\frac{B + S}{2} where Q is charge, I_3 is isospin, B is baryon number, and S is strangeness.\n\n\n\nLater, looking into these multiplets, they formed a quark model where all these particles consist of smaller particles called quarks. We said they come in six different flavors and have three types of color. This is basically the historic background to this topic.\n\nWhat I always find difficult with histories like this is distinguishing what was known and what wasn’t. For example, when strangeness was introduced with cosmic rays, how did they know it was strangeness (\\Delta S \\neq 0) and not just invariant masses? They called these particles strange because they were confused about why they only appeared in pairs or had longer lifetimes.\n\nPractically, you can only look at the invariant mass. You detect the proton and the pions, then analyze the spectra. That’s how you determine the knowns and unknowns.\n\nKey particle discoveries:\nPions were discovered quickly\nProton, then neutron in 1932\n1950s: antiproton and many other particles (charged, measurable directly)\n\n\nYou mentioned CP symmetry. How was that introduced? Was it used to say a decay isn’t allowed, or was it the other way around? If you don’t observe a decay, there must be a violated quantum number.\n\n\n\n\n\n\nCP violation occurs when: \\frac{\\Gamma(P \\rightarrow f) - \\Gamma(\\bar{P} \\rightarrow \\bar{f})}{\\Gamma(P \\rightarrow f) + \\Gamma(\\bar{P} \\rightarrow \\bar{f})} \\neq 0\n\n\n\n\nI don’t know exactly when charge conjugation was introduced, but it was early—around the time of the positron, before the antiproton was discovered. Sometimes concepts come from theory, sometimes from experiments. For example:\n\nThe omega baryon (\\Omega^- = sss) was predicted\nQuantum numbers were introduced based on cross sections It’s difficult to track historically.\n\n\nThe quark model was mostly correct historically. When the multiplets were assembled, it was only postulated that there might be a smaller substructure. They were first called partons, not quarks.\nThe mass differences in the deltas, sigmas, and cascades suggested different quark content. They saw a mass difference and predicted the next particle would be 150 MeV heavier, which turned out to be correct. The original quark paper is just two pages—you can look it up. It was an exciting time: quarks were predicted in 1962, and the Omega minus was discovered in 1964."
  },
  {
    "objectID": "2025-Lecture-03.html",
    "href": "2025-Lecture-03.html",
    "title": "(2025) Lecture 3",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Anna Zimmer\n\n0.1 Symmetry, Hadron Structure, and Isospin in Meson and Baryon Systems\nThanks everyone who found the room, and apologies for changing it all the time. We are fixed now. From now on, this room is booked, and we will continue using it—hopefully for the next 10 lectures.\nToday we have lecture number three. We will follow the discussion of symmetry from the last lecture, but also introduce and look more carefully at what we know about the structure of hadrons: how they look inside and what the actual experimental evidence is that they are not point-like, that they’re made of quarks.\n\nWe will go through this list:\n\nKinematics and cross sections:\n\n\nCalculation of cross sections.\nMultibody kinematics.\nStart with classical scattering of an electron and a proton.\n\n\nForm factors in scattering:\n\n\nRelation to hadron properties.\nConnection to classical quantities like charge distributions and magnetic moments.\n\n\nQuark model:\n\n\nHadrons as quark composites.\nRelating experimental distributions to internal structure.\n\n\nSymmetry considerations:\n\n\nFlavor symmetry.\nSpin symmetry.\n\n\nProton and neutron spin-flavor wave functions:\n\n\nMagnetic moment computation in the quark model.\nRelation to observables.\n\n\n\n\n\n\n\nKey Formula: Magnetic moment in the quark model: \n\\mu = \\sum_i \\mu_i = \\sum_i q_i \\frac{e\\hbar}{2m_i} \\langle \\sigma_z \\rangle_i\n where q_i, m_i, and \\langle \\sigma_z \\rangle_i are the charge, mass, and spin projection of the i-th quark.\n\n\n\n\nBefore starting, I wanted to pose a question to recap. There are two questions:\n\nFirst question:\n\n\nWhat is the isospin of the D meson?\nSpecifically, the D^+ meson (c\\bar{u}) and the D^0 meson (c\\bar{d}).\n\n\nSecond question:\n\n\nWhat is the isospin of the \\Xi_{cc} baryon and its quark content?\n\n\n\n\n\n\n\nIsospin Assignments:\n\nFor D^+ (c\\bar{u}) and D^0 (c\\bar{d}): I = \\frac{1}{2}.\nFor \\Xi_{cc} (ccu or ccd): I = \\frac{1}{2}.\n\n\n\n\n\nIsospin symmetry is related to the symmetry of the wave functions with respect to the light quarks.\n\nYou have a u-quark or d-quark. These are two light quarks, and they get replaced by whatever they are.\nThe wave function has two components, \\psi_{up} and \\psi_{down}, corresponding to the up and down as a two-component spinor function.\nThis multiplicity-two wave function corresponds to the +\\frac{1}{2} or -\\frac{1}{2} isospin projections.\n\nKey Points:\n\nOne light quark \\implies isospin \\frac{1}{2}.\nTwo light quarks \\implies combined isospin 1 or 0.\n\n\nThe second question:\n\nJ/\\psi (J^P = 1^-) decays to two vector particles (1^- each).\nWhat is the orbital angular momentum L between them for parity conservation?\n\n\n\n\n\n\n\nParity Constraint: \nP_{final} = (-1)^L \\cdot P_{V_1} \\cdot P_{V_2} = (-1)^L \\cdot (-1) \\cdot (-1) = (-1)^L\n Since P_{J/\\psi} = -1, L must be odd (1, 3, \\dots).\n\n\n\nPossible Configurations:\n\nTotal spin combinations: 0, 1, or 2.\nOrbital angular momentum: L = 1 (P-wave), L = 3 (F-wave), etc.\n\n\nWhy don’t you have isospin for heavier quarks?\n\nThe Lagrangian doesn’t obey symmetry if quark masses differ significantly.\nExample: Changing s to c breaks symmetry because m_s \\neq m_c.\nFor u and d quarks: masses are nearly equal \\implies isospin symmetry holds.\n\nExtension to SU(3) Flavor Symmetry:\n\nIncludes u, d, s quarks: \n\\text{SU(3)}_f \\supset \\text{SU(2)}_I \\times \\text{U(1)}_Y\n\nCharm quark could be included if masses were equal (hypothetical).\n\n\n\nIsospin partners: Identify quark content and name (e.g., D^+ and D^0).\nLHCb convention: Charge-conjugated states are implicitly included in analyses.\n\nSpin-Orbit Combinations:\n\nLearn to fill the table of possible quantum numbers for decays.\nExample: For L = 2 (D-wave), total angular momentum can be 2^+, 1^+, or 3^+.\n\n\n\n0.2 Decay Analysis, Hadron Structure Probes, and Scattering Formalism\nThe next step is to look at what is decaying and identify within this table what decays. I’m looking at this list, and one that decays is 1^-. I have it here, here, here—so four combinations. The answer to this question is it decays into two.\nLet me remove identical particles because identical particles make it more difficult. Two identical particles cannot be in a certain configuration of S. We will come back to this identical particle point later.\n\n\n\n\n\n\nFor identical particles:\n\nThese two are forbidden for L=3\nThree combinations forbidden when considering L=3 and the cases shown\nOnly remains when L + S is even (since 3 + 2 = 5 is odd)\n\n\n\n\nBut identical particles will make these two forbidden. For L=3, these two are forbidden. For L=3 and the two you showed—three forbidden. I think I remember that L + S must be even for identical particles. So 3 + 2 is odd. This is why only one combination remains. For identical particles, only one decay is allowed, while for non-identical particles, all four combinations are possible.\n\nSecond question: You read this and realize you need to:\n\nDraw a table like this\nQuickly fill it—it’s super easy\nIdentify what is stable\n\nThat’s it. Questions? This is a skill you’ll learn so you never have trouble with combinatorics.\n\nTo understand the structure of hadrons, you need a well-understood probe—a standard current insertion into the hadron. To study strong interactions and how hadrons are organized, we use electromagnetic interactions. We scatter electrons off protons and analyze the reaction in kinematic phase space.\nThe electron side is simple: the electron is point-like and interacts with the proton’s electromagnetic field. This is well understood.\nThe proton side is more interesting:\n\nAt low momentum transfer, the electron sees the proton’s charge\nAt high momentum transfer, the electron starts probing the quarks inside the proton\n\nThis is like smashing a large cucumber with an electromagnetic current. At low momentum transfer, you see the proton’s characteristics. At high momentum transfer—visible in large electron scattering angles—you probe the proton’s internals. This is the idea behind deep inelastic scattering. Large accelerators are built to study this process.\n\nFor a 2 \\to 2 reaction, the observables are simple:\n\nTotal energy (set by beam energy)\nElectron scattering angle\n\nMore familiar quantities are the center-of-mass energy and scattering angle. But Mandelstam variables s and t are more natural:\n\ns = (p_1 + p_2)^2 \\quad \\text{(center-of-mass energy squared)}\n\n\nt = (p_1 - p_3)^2 \\quad \\text{(momentum transfer squared)}\n\nIn the center-of-mass frame:\n\n\\sqrt{s} = E_{\\text{total}} = E_e + E_p\n\nThe proton momentum p_p^* = -p_e^* (star denotes center-of-mass frame).\nFor t:\n\nt = m_e^2 + m_e^2 - 2E_e E_e' + 2|\\vec{p}_e||\\vec{p}_e'|\\cos\\theta\n\nAt high energy, this simplifies to:\n\nQ^2 = -t = 4E_e E_e' \\sin^2(\\theta/2)\n\n\nTo connect theory to experiment, we use cross sections. The differential cross section depends on the matrix element \\mathcal{M}:\n\n\\frac{d\\sigma}{d\\Omega} = \\frac{1}{64\\pi^2 s} \\frac{|\\vec{p}_f|}{|\\vec{p}_i|} |\\mathcal{M}|^2\n\nThe total cross section integrates over phase space:\n\n\\sigma = \\int |\\mathcal{M}|^2 \\, d\\Pi\n\nPhase space counts final-state configurations. For n particles:\n\nd\\Pi_n = \\prod_{i=1}^n \\frac{d^3p_i}{(2\\pi)^3 2E_i} (2\\pi)^4 \\delta^{(4)}\\left(p_{\\text{initial}} - \\sum p_i\\right)\n\nFor 2-body final states:\n\nd\\Pi_2 = \\frac{|\\vec{p}^*|}{8\\pi\\sqrt{s}} d\\Omega^*\n\nThe flux factor normalizes the cross section:\n\nF = 4E_1E_2|v_1 - v_2|\n\n\nIn elastic scattering, Q^2 (the spacelike momentum transfer) is key. The beams’ energy is fixed; the scattering angle determines Q^2. Electrons scatter at different angles:\n\nSmall Q^2 (small angles): probe proton charge distribution\nLarge Q^2 (large angles): probe quark structure\n\nFor small-angle scattering, we measure:\n\nProton charge radius (approximated by \\rho(r) \\sim e^{-r/R})\nMagnetic moment \\vec{\\mu}, which interacts with external fields:\n\n\nH_{\\text{int}} = -\\vec{\\mu} \\cdot \\vec{B}\n\n\n\n0.3 Phase Space: Counting Configurations and Two-Body Kinematics\nPhase space is nothing else but the continuous version of the counting of possible configurations. It’s very easy for the two-body phase space, a little trickier for the three-body phase space, and gets rather complicated for four-body and five-body. But it’s always a standard integral. We compute this numerically if we need to.\nFor the two-body case, there was a classwork to compute this. We will meet this expression very often, so I will give the expression for two-body phase space.\nLet’s first count the number of variables:\n\nEvery delta function gives the energy-momentum constraint, which removes one integration variable.\nSo you can count the number of differentials d that appear—this is the number of variables to integrate—and then subtract the number of constraints.\nEach constraint allows us to remove one integration variable.\n\nFor two-body phase space, how many differentials do we have initially? The answer is six. Now subtract four constraints (from energy-momentum conservation). We are left with two degrees of freedom.\nIf we proceed wisely, we can trade each constraint for a differential, leaving us with two remaining variables. The resulting expression (which Drawn should send you) is:\n\nd(\\text{phase space})_2 = \\frac{1}{8\\pi} \\frac{\\lambda^{1/2}(s, m_1^2, m_2^2)}{s} \\frac{d(\\cos\\theta) \\, d\\phi}{4\\pi}\n\n\n\n\n\n\n\nThe Källén function \\lambda(s, m_1^2, m_2^2) appearing in the phase space formula is defined as: \n\\lambda(s, m_1^2, m_2^2) = s^2 + m_1^4 + m_2^4 - 2sm_1^2 - 2sm_2^2 - 2m_1^2m_2^2\n This governs the energy dependence of the phase space volume.\n\n\n\nThis is a great exercise because it’s totally analytic and contains many physical insights. I recommend doing it three times: once, then again, and a third time—then you truly understand it.\nThis expression has two remaining differentials, corresponding to an integral over spherical angles. In 2-to-2 scattering, there is one variable (the scattering angle \\theta) that determines the process, but there is also an azimuthal angle \\phi. If the problem is unpolarized, the matrix element does not depend on \\phi, so we can ignore it.\nImagine a coordinate system:\n\nThe z-axis is the collision axis.\nThe x- and y-axes are transverse. The scattered electron has an angle \\theta relative to the z-axis and an azimuthal angle \\phi in the XZ plane.\n\nIf there is no spin or preferred direction, the physics is independent of \\phi. For the total cross section, we integrate over \\phi and replace it with 2\\pi:\n\n\\int d\\phi = 2\\pi\n\nThe nontrivial part is the \\cos\\theta dependence, which relates to Q^2 and probes the parton structure.\nThe kinematic factor \\lambda^{1/2}(s, m_1^2, m_2^2)/s tells us the number of configurations available at a given energy.\n\nAt low energy (close to threshold), this factor suppresses the phase space.\nIt vanishes when s = (m_1 + m_2)^2, meaning no phase space is available below threshold.\n\nThis makes sense:\n\nIf the system energy is barely enough to produce the particles, there are very few possible configurations.\nIn the non-relativistic limit, the phase space is simply given by the number of angular configurations (4\\pi).\nBut for relativistic particles, we get this suppression factor near threshold.\n\nQuestions?\n\nI’ve been thinking about how knowledge will be acquired in the future. How will we “upload” knowledge into students’ brains? I think it will involve interactive training, like a ChatGPT that forces you to apply concepts repeatedly.\n\nFirst, information is explained (like in this lecture).\nThen, immediate testing—perhaps a quiz or derivation exercise.\nNext, a gaming-like format where you solve problems interactively.\n\nFor example:\n\n“What is the high-energy behavior of phase space?”—and you figure it out through guided exploration.\n\nImagine “Duolingo for hadron physics.” After a few sessions, you master the material. This is what we try to do with homework—please work on it. This is your chance to learn deeply.\n\n\n0.4 Matrix Elements, Form Factors, and Proton Structure in Scattering Theory\nWe discussed this in scattering. Let me quickly tell you what the matrix element is. The matrix element you see there is a key input to the reactions.\nThis is something we can derive if we have a theory of interaction. If we don’t have a theory, as often happens in hadron physics, we can use general principles to come up with the matrix element. For electromagnetic processes, we do have a theory: QED plus QCD.\nWe know how to write an expression that, when squared, gives the cross-section. The sum over final states and the average over initial states yields a mathematical expression we can compare to experiment. This expression can only depend on two variables: s and t.\n\n\n\n\n\n\nThe matrix element for electromagnetic scattering in QED + QCD is given by: \n\\mathcal{M} = \\bar{u}(p_3) \\gamma^\\mu u(p_1) \\left( \\frac{-ig_{\\mu\\nu}}{q^2} \\right) \\bar{u}(p_4) \\left[ F_1(q^2) \\gamma^\\nu + F_2(q^2) \\frac{i\\sigma^{\\nu\\rho} q_\\rho}{2M} \\right] u(p_2)\n where F_1(q^2) and F_2(q^2) are form factors, and q = p_3 - p_1 is the momentum transfer.\n\n\n\n\nThe matrix element is defined in the domain of the phase space variables. The phase space, as we discussed, is a function of two variables. In the unpolarized case, there is no \\phi dependence, so only \\theta remains. Therefore, the matrix element in our case will be a function of the scattering angle. It also depends on the total energy, which is fixed by the colliding particles.\nThis mathematical expression is complex. In field theory, it is formally defined as part of the scattering matrix. You have initial states at t = -\\infty and final states at t = +\\infty, with non-interacting particles asymptotically. The matrix element is the expectation of the interacting operator between these asymptotic states.\nIt is a complex function, but its absolute value squared gives a real value. So it is a complex function of s and t. When s is fixed, it becomes a function of t alone.\n\nLet me write something you might have seen in field theory. In field theory, scattering is approached using perturbation theory, expanding order by order in the number of interactions. Each electron-current interaction introduces a factor of \\alpha = \\frac{1}{137}.\nA series expansion works because the first term already gives a good approximation of the total cross-section. For electromagnetic processes, we consider the first-order interaction. Each term in this expansion corresponds to a Feynman diagram.\nHere, I draw a diagram and write the corresponding mathematical expression. Something here might not be familiar: F_1 and F_2 are form factors. These functions appear because the proton is not point-like. If it were, F_2 would vanish, and F_1 would be trivial. But they are not, so we must measure them experimentally.\nu is a spinor, and \\Gamma is a gamma matrix. The dimensionality of the matrix element is important—you must ensure all indices match. For example, this \\mu contracts with this \\mu.\nThe spinors and matrices combine to form scalars or vectors depending on the contractions. F_1 and F_2 are scalar functions. To derive the cross-section, we square the matrix element and perform spin summations.\n\nThe final expression is the Rosenbluth formula:\n\n\\frac{d\\sigma}{d\\Omega} = \\left( \\frac{d\\sigma}{d\\Omega} \\right)_{\\text{Mott}} \\left[ G_E^2(q^2) + \\tau G_M^2(q^2) \\right] \\frac{1}{1 + \\tau} + 2\\tau G_M^2(q^2) \\tan^2 \\left( \\frac{\\theta}{2} \\right)\n\nwhere:\n\n\\left( \\frac{d\\sigma}{d\\Omega} \\right)_{\\text{Mott}} is the Mott cross-section,\nG_E(q^2) = F_1(q^2) + \\tau F_2(q^2) (electric form factor),\nG_M(q^2) = F_1(q^2) + F_2(q^2) (magnetic form factor),\n\\tau = \\frac{q^2}{4M^2}.\n\n\nIt includes a \\frac{1}{\\sin^4 \\theta} term from the \\frac{1}{Q^2} dependence, where Q is proportional to \\sin \\theta. This leads to a divergence at small angles, making forward scattering non-integrable without regularization.\nFor convenience, we replace F_1 and F_2 with G_E and G_M. These are what experiments measure. Their Q-dependence reveals properties of the proton.\nThe Q-dependence is dual to the spatial distribution via Fourier transform. Measuring G_E(Q^2) lets us infer the proton’s charge distribution. Exponential spatial distributions correspond to dipole parameterizations in momentum space.\nOne key question is the proton’s charge radius. Experiments and lattice QCD calculations extract this from the Q-dependence of form factors.\n\nThe magnetic form factor G_M at Q = 0 gives the hadron’s magnetic moment. The magnetic moment is:\n\n\\vec{\\mu} = g \\left( \\frac{e}{2m} \\right) \\vec{S}\n\nwhere g = 2 for point-like Dirac particles.\nFor the proton and neutron, experiments show deviations from this prediction. The proton’s magnetic moment is not 2, and the neutron’s is not zero. The ratio \\mu_p / \\mu_n \\approx -1.46 suggests a composite structure.\nIn the quark model, the proton’s magnetic moment arises from its quark constituents. The up and down quarks contribute differently, explaining the observed ratio. We can write the proton’s wave function in terms of quark flavors and spins.\nThe quark model assumes that gluonic degrees of freedom are integrated out, leaving massive constituent quarks. These quarks carry most of the proton’s properties, with up quarks at +\\frac{2}{3}e and down quarks at -\\frac{1}{3}e.\n\nToday, we discussed:\n\nElectron-proton scattering kinematics,\nForm factors and their relation to charge and magnetic properties.\n\nNext time, we will derive the quark model predictions for magnetic moments.\n\n\n0.5 Closing Remarks\nAll right, thanks for your attention.\n\n\n\n\n\n\n\nThis concludes the lecture segment. No technical content was present in this portion to analyze or enhance with nuclear physics formulas."
  },
  {
    "objectID": "2025-Lecture-04.html",
    "href": "2025-Lecture-04.html",
    "title": "(2025) Lecture 4",
    "section": "",
    "text": "Presenter: Farah Afzal\nNote Taker: Anna Zimmer\n\n0.1 Electron-Photon Scattering, Form Factors, and Baryon Magnetic Moments\nToday’s lecture will start by reviewing electron-photon scattering, then continue with the magnetic moment. Later, we will discuss different experiments and how they produce various hadrons.\n\nThe electron-photon spectrum was discussed last time. In this context, remember three cross sections:\n\nThe Rosenfeld cross section\nThe Mott cross section\nThe Rosenbluth cross section\n\nWhat’s important here are the charges. We neglected the spins of the particles initially. The logical extension is the Mott cross section, where \\beta is the velocity and the electron spin is considered. It is essentially the Rutherford cross section multiplied by a correction term:\n\n\\left(\\frac{d\\sigma}{d\\Omega}\\right)_{\\text{Mott}} = \\left(\\frac{d\\sigma}{d\\Omega}\\right)_{\\text{Rutherford}} \\cdot \\left(1 - \\beta^2 \\sin^2\\left(\\frac{\\theta}{2}\\right)\\right)\n\nThis term suppresses scattering at backward angles. At 180 degrees, the cross section is lower due to conservation laws for relativistic particles.\n\nExperiments show the cross section falls more steeply than predicted by the Mott formula. This discrepancy arises because the target is not point-like. The finite size is accounted for by a form factor.\nThe form factor is the Fourier transform of the charge density:\n\nF(Q^2) = \\int \\rho(r) e^{i \\vec{q} \\cdot \\vec{r}} d^3r\n\nFor a point-like particle (\\rho(r) = \\delta(r)), the form factor is a constant. For an exponential charge density (e.g., a proton), it takes a dipole form:\n\nG_E(Q^2) = \\left(1 + \\frac{Q^2}{\\Lambda^2}\\right)^{-2}, \\quad \\Lambda \\approx 0.71\\,\\text{GeV}^2\n\nFor a nucleus with a diffuse edge, the form factor exhibits oscillatory behavior.\n\nThe Rosenbluth cross section incorporates electric and magnetic form factors:\n\n\\frac{d\\sigma}{d\\Omega} = \\left(\\frac{d\\sigma}{d\\Omega}\\right)_{\\text{Mott}} \\left[ \\frac{G_E^2(Q^2) + \\tau G_M^2(Q^2)}{1 + \\tau} + 2\\tau G_M^2(Q^2) \\tan^2\\left(\\frac{\\theta}{2}\\right) \\right]\n\nwhere \\tau = Q^2/4m_p^2 and Q^2 is the momentum transfer.\n\n\n\n\n\n\nTo extract the proton radius, plot the measured cross section divided by the Mott cross section versus Q^2. The slope at Q^2 \\to 0 gives the radius:\n\n\\langle r_p^2 \\rangle = -6 \\hbar^2 \\left.\\frac{dG_E(Q^2)}{dQ^2}\\right|_{Q^2=0}\n\n\n\n\n\nThe electric form factor of the proton follows a dipole distribution, while the neutron’s magnetic form factor starts at -1.91 at Q^2 = 0. The values at Q^2 = 0 correspond to:\n\nThe proton’s charge (1)\nThe magnetic moments of the proton and neutron\n\nMeasuring at low Q^2 is challenging because it requires small scattering angles. Experiments like those at the Mainz accelerator use spectrometer arms to detect electrons and protons. New experiments, such as MUSE at MESA, aim to improve these measurements.\nHistorically, discrepancies arose between proton radii measured via:\n\nElectron scattering (\\sim 0.86 fm)\nAtomic spectroscopy (Lamb shift), which suggested a smaller radius\n\nNew experiments, like AMBER at CERN (using muons), seek to resolve this.\n\nThese experiments revealed the proton’s substructure. Now, let’s discuss spin, flavor symmetry, and calculating baryon magnetic moments.\nThe total baryon wave function must be antisymmetric. Color wave function antisymmetry implies the spin-flavor part must be symmetric. For a proton (spin-1/2), the symmetric spin-flavor wave function is:\n\n|p \\uparrow \\rangle = \\frac{1}{\\sqrt{2}} \\left( \\chi_S \\psi_S + \\chi_A \\psi_A \\right)\n\nwhere \\chi_S and \\psi_S are symmetric spin and flavor states, and \\chi_A, \\psi_A are antisymmetric.\n\nThe magnetic moment of a quark is:\n\n\\mu_q = \\frac{e_q \\hbar}{2m_q}\n\nFor up (e_u = +2/3) and down (e_d = -1/3) quarks:\n\n\\mu_u = +\\frac{2}{3} \\frac{e\\hbar}{2m_u}, \\quad \\mu_d = -\\frac{1}{3} \\frac{e\\hbar}{2m_d}\n\nThe proton’s magnetic moment (spin-up) sums contributions from its quarks:\n\n\\mu_p = \\langle p \\uparrow | \\sum_{i=1}^3 \\mu_i \\sigma_z^{(i)} | p \\uparrow \\rangle = \\frac{4}{3}\\mu_u - \\frac{1}{3}\\mu_d\n\nFor the neutron, swapping up and down quarks gives:\n\n\\frac{\\mu_n}{\\mu_p} \\approx -\\frac{2}{3} \\quad \\text{(if } m_u \\approx m_d \\text{)}\n\nExperimentally, this ratio is close to -1.91/2.79 \\approx -0.68, consistent with the quark model.\n\nThe constituent quark masses include:\n\nValence quarks\nSurrounding gluons/quark-antiquark pairs\n\nThis explains their larger values (\\sim 300 MeV) compared to bare quark masses.\nMagnetic moments are measured via:\n\nSpin splitting in magnetic fields (e.g., Stern-Gerlach)\nRadiative decays in photon-proton collisions for short-lived baryons (e.g., \\Delta^{+})\n\nFor the \\Omega^- baryon (three strange quarks), the magnetic moment is:\n\n\\mu_{\\Omega^-} = 3\\mu_s = 3\\left(-\\frac{1}{3}\\frac{e\\hbar}{2m_s}\\right) = -\\frac{e\\hbar}{2m_s}\n\n\nThis concludes the recap of electron-photon scattering and magnetic moments. Next, we’ll explore further applications of flavor symmetry.\n\n\n0.2 Light Baryon Spectroscopy and Hadron Production Methods\nWe already discussed the proton. Now let’s think about the light baryons. We have seen you can use SU(3) for flavor with the up, down, and strange quark, and combine it with SU(2) spin. This gives us SU(6) for spin-flavor symmetry:\n\n\\text{SU(6)} \\supset \\text{SU(3)}_{\\text{flavor}} \\otimes \\text{SU(2)}_{\\text{spin}}\n\nAs discussed earlier, for SU(3) flavor, we have the baryon decuplet and octet. Combining flavor and spin into SU(6), we get the 20, 70, and 56-plets. The 56-plet is antisymmetric, while the others are mixed symmetric. With these considerations, we can arrange the known light baryons into these multiplets. I won’t cover all of them, but let’s look at some mass differences as examples.\nFor instance, we have:\n\nThe radial excitation number\nOrbital angular momentum L\nThe spin, which can be 1/2 or 3/2\n\nFor spin 1/2, we get the nucleons, and for 3/2, we enter the decuplet, which includes the \\Delta. I’ll focus on the N^* and \\Delta^* baryons, excluding those with strange quarks. These can have excited states from radial or orbital angular momentum excitations. For L=1, the spin can still be 1/2 or 3/2.\n\nLet me give an example of radial excitation. If we arrange these states in patterns:\n\nS denotes L=0\nThe N=1 state corresponds to N=0, which is just the proton or nucleon\nThe next band contains five states, with masses ranging from 1500 to 1700 MeV\n\nOne state might be slightly lower, and others could include D-states, and so on.\nThe key point here is the mass difference of about 600 MeV, arising from spin-orbit and spin-spin interactions:\n\n\\Delta M \\approx 600 \\text{ MeV}\n\nThese states are well-documented up to certain energies in the PDG, with star ratings indicating their reliability. A goal of light baryon spectroscopy is to identify how many states exist and their patterns, based on quark model predictions. Are these correct, or is there different dynamics? We need to study the states’ properties, which leads me to discuss experimental methods for producing hadrons.\n\n\n\n\n\n\n\nQuestion about 1S and 2S: I’m surprised you labeled this as 3S. What’s the difference between 1S and 2S? The radial wave function changes, but no orbital angular momentum is added—just the hadron’s size increases. How does this introduce more states? I was checking the quark model’s predictions. Why three states? I’d expect one, like the \\rho resonance.\nThe answer lies in multiplets from different LS combinations. For n=1, l=0, there’s a spin combination. Some papers claim more states should exist, including \\Delta resonances, but these are distinct. Some studies don’t separate them clearly.\nFor radial excitations (increasing N), the number of states in the multiplet shouldn’t change because no extra degrees of freedom are added. The 2P multiplet has the same number of states as 1P. Higher excitations introduce L=2, altering the count.\nAgreed—for N=1, 2, 3, the state count remains the same. Experimentally, only the first band is well-known; the rest are uncertain. Some states match quark model predictions, others don’t, leaving gaps in light baryon spectroscopy.\n\n\n\n\nNext, let’s discuss hadron production. Electromagnetic probes like electron or photon beams can be used, with colliding or fixed-target setups. Hadron beams (e.g., pions or protons) are another option.\nStarting with electromagnetic probes, consider the BESIII facility in China. It’s a symmetric e^+e^- collider with beam energies of 1–2.47 GeV. The annihilation produces a virtual photon, which can form a J/\\psi, decaying into other hadrons.\nThe center-of-mass energy \\sqrt{s} = 2E_e determines the producible particles. For BESIII, this covers masses up to ~2 GeV. The cross-section shows peaks like J/\\psi and \\psi(2S), with decays like J/\\psi \\to \\gamma \\eta \\eta' or 3\\pi.\nAt higher energies (e.g., Belle, \\sqrt{s} \\approx 10 GeV), bottomonium states like \\Upsilon are produced.\n\nNow, photo-production experiments: fixed-target setups like:\n\nA2 (Mainz, 0.08–1.6 GeV)\nCBELSA/TAPS (Bonn)\nGlueX (US, 6–12 GeV)\n\nPhoton beams are generated via Bremsstrahlung from electron beams:\n\n\\frac{dN}{dE_\\gamma} \\propto \\frac{1}{E_\\gamma}\n\nFor \\gamma p collisions, the center-of-mass energy is:\n\ns = m_p^2 + 2E_\\gamma m_p\n\nWith E_\\gamma \\leq 3.2 GeV, resonances up to 2.7 GeV (e.g., \\Delta) are accessible.\n\n\nCan hyperons be produced here? No—flavor conservation forbids strangeness change in s-channel. However, t-channel processes (e.g., \\gamma p \\to K^+ \\Sigma) are possible, as planned for the INSIGHT experiment.\n\nQuantum number selection rules apply. For E1 photons (L=1, parity -):\n\nJ^P = \\frac{1}{2}^- \\text{ or } \\frac{3}{2}^-\n\nResonances are labeled like S_{11}, where:\n\nS: L=0 (meson orbital momentum)\nFirst subscript: 2 \\times isospin (1 for nucleons, 3 for \\Delta)\nSecond subscript: 2 \\times total angular momentum\n\n\nFinal note: Higher energies allow meson spectroscopy, but we’ll cover that next time.\n\n\n0.3 SU(2) Algebra and Delta Baryon Wave Functions\nWe’ve been talking to theorists yesterday. There will be advanced theoretical hadron physics next semester. Everyone is invited to take it, and it will be completely complementary to this course—it will go in-depth into group theory.\nOne thing we discussed is that in this course, we cover SU(2) well. In the theory course, this SU(2) is considered trivial, so it goes to SU(3) in depth, looking at the irreducible representations. But for this course, we focus on SU(2), the groups we discussed for the flavor, for the isospin, and for the spin, as Farah demonstrated today.\nIt’s indeed simple once you understand it, and understanding requires practice. I mentioned last time, Farah mentioned today, and we will mention yet another time how the SU(2) algebra works. But I wanted to invite you to engage with a little exercise.\n\n\n\n\n\n\nKey SU(2) Algebra Formulas:\n\nCommutation relations: [J_i, J_j] = i \\hbar \\epsilon_{ijk} J_k or [I_i, I_j] = i \\epsilon_{ijk} I_k (with \\hbar = 1).\nRaising/lowering operators: J_{\\pm} = J_x \\pm i J_y (similarly for I_{\\pm}). J_- |j, m\\rangle = \\sqrt{j(j+1) - m(m-1)} \\, |j, m-1\\rangle. J_+ |j, m\\rangle = \\sqrt{j(j+1) - m(m+1)} \\, |j, m+1\\rangle.\n\n\n\n\n\nI’ll quickly write a wave function—it goes nowhere—and I invite you to write down the answer, leave it on the table right here. You don’t even need to write your name on it. We just want to gauge how well you understand what’s going on.\nSo, I want to consider a delta that has three quarks in isospin states. There are four states: \\Delta^{++}, \\Delta^+, \\Delta^0, and \\Delta^-. I would like to build this spin-flavor wave function.\nFor example, the \\Delta^+ state would be |uud\\rangle with spin projection J_z = \\frac{3}{2}. On this little piece of paper, write:\n\nWhat is J_- acting on |\\Delta^+ (J=\\frac{3}{2}, J_z=\\frac{3}{2})\\rangle?\nWhat is J_+ acting on |\\Delta^+ (J=\\frac{3}{2}, J_z=\\frac{3}{2})\\rangle?\nWhat is I_+ acting on |\\Delta^+ (I=\\frac{3}{2}, I_z=\\frac{1}{2})\\rangle?\nWhat is I_- acting on |\\Delta^+ (I=\\frac{3}{2}, I_z=\\frac{1}{2})\\rangle?\n\nYou have two minutes. I have a meeting now, so I won’t provide solutions—it’s trivial once you understand the logic. Just give it a try.\n\nA reminder: the total isospin is the sum of the isospins of the three quarks, \\mathbf{I} = \\mathbf{I}_1 + \\mathbf{I}_2 + \\mathbf{I}_3. The same applies for \\mathbf{J}. You don’t need much derivation if you know the answer. We just want to check if you remember how the lowering operators act.\n\n\n\n\n\n\nExercise Solutions (for reference):\n\nJ_- |\\Delta^+ (J=\\frac{3}{2}, J_z=\\frac{3}{2})\\rangle = \\sqrt{3} \\, |\\Delta^+ (J=\\frac{3}{2}, J_z=\\frac{1}{2})\\rangle\nJ_+ |\\Delta^+ (J=\\frac{3}{2}, J_z=\\frac{3}{2})\\rangle = 0 (max J_z state)\nI_+ |\\Delta^+ (I=\\frac{3}{2}, I_z=\\frac{1}{2})\\rangle = \\sqrt{3} \\, |\\Delta^{++} (I=\\frac{3}{2}, I_z=\\frac{3}{2})\\rangle\nI_- |\\Delta^+ (I=\\frac{3}{2}, I_z=\\frac{1}{2})\\rangle = \\sqrt{2} \\, |\\Delta^0 (I=\\frac{3}{2}, I_z=-\\frac{1}{2})\\rangle"
  },
  {
    "objectID": "2025-Lecture-05.html",
    "href": "2025-Lecture-05.html",
    "title": "(2025) Lecture 5",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Anna Zimmer\n\n0.1 Production Mechanisms and Kinematic Analysis in Hadron Spectroscopy\nLet’s start this lecture.\nToday, lecture number five will be dedicated to the production mechanisms in different directions across the world to study hadron spectroscopy.\nWe will also go into depth on the kinematics of different reactions, discuss numerical algorithms to produce distributions of particles—particularly decays—and look at one of the most important kinematic setups: frames and the kinematic representation of the phase space for different reactions.\n\n\n\n\n\n\n\nKey formulas referenced in this lecture:\n\nInvariant Mass (Mandelstam variable s):  s = (p_1 + p_2)^2 = (E_1 + E_2)^2 - (\\vec{p}_1 + \\vec{p}_2)^2 \nDecay Width:  \\Gamma = \\frac{1}{\\tau} = \\frac{|\\mathcal{M}|^2}{2\\pi} \\rho_f \nLorentz Invariant Phase Space:  d\\Phi_n = (2\\pi)^4 \\delta^4\\left(P - \\sum_{i=1}^n p_i\\right) \\prod_{i=1}^n \\frac{d^3p_i}{(2\\pi)^3 2E_i} \n\nThese are foundational for understanding reaction kinematics and phase space representations in hadron spectroscopy.\n\n\n\n\n\n0.2 Flavor Multiplets, Isospin Symmetry, and Gauge Symmetry in Baryons\nBefore starting, we go through the recap and I ask you quickly to give me the answers. Who feels confident on question number one? Let’s break it down. So I would ask you guys in the first row to translate the words one by one.\n\n\nFlavor: What does “flavor” mean? It’s the charge of the strong interaction. In this context, what does “flavor” refer to here? It’s a quantum number for the multiplet. There are many quantum numbers. This is a quantum number—the flavor quantum number.\n\n\n\n\n\n\n\nFlavor quantum numbers include: up, down, strange, charm (not to be confused with color charges like red, green, blue).\n\n\n\n\nMultiplet: What is a multiplet? There are singlets and multiplets. It’s different possible combinations for one quantum number. Like if I have one total spin and multiple configurations, that’s called multiplets.\n\n\nAlexander, you’re up. What’s your name again? Benedict. Benedict, what remains for you? The whole question. What are the flavor multiplets of Σ_b?\n\nΣ_b: “Flavor” is clear. “Multiplet” is clear. Σ_b is the name of the baryon with this quark content. We are looking for flavor multiplets of this baryon.\nBaryon Composition: \n\\Sigma_b^+ = (u\\, s\\, b), \\quad \\Sigma_b^0 = (d\\, s\\, b), \\quad \\Sigma_b^- = (d\\, d\\, b)\n These represent the quark content of the Σ_b baryons in the isospin multiplet.\n\n\n\nIsospin SU(2): Replace u with d, and you get Σ_b^0. Same particle, different isospin projection—different charge. What is the charge of Σ_b^0? It’s neutral.\nSU(3) Flavor Multiplets: For baryons composed of three quarks: \n3 \\otimes 3 \\otimes 3 = 10 \\oplus 8 \\oplus 8 \\oplus 1\n (or alternatively mentioned as 6 \\oplus \\bar{3} for 3 ⊗ 3).\n\n\nWhat are possible spins for a two-pion system?\n\nPion: Quantum numbers 0^-.\nD-wave spin: \nJ^P = 2^+\n (from combining L = 2 with total intrinsic spin S = 0).\n\n\nIs flavor symmetry gauge? Is Lorentz symmetry gauge?\n\nFlavor Symmetry: Anna, flavor symmetry is global. \n\\mathcal{L} \\rightarrow \\mathcal{L}' = \\mathcal{L} \\quad \\text{under global } SU(3)_\\text{flavor}\n\nLorentz Symmetry: Can you boost different points of spacetime differently? No. In standard treatment, Lorentz is global: \nx^\\mu \\rightarrow \\Lambda^\\mu_\\nu x^\\nu \\quad \\text{(applied uniformly)}\n\n\n\n\n\n\n\n\nIn gravity, Lorentz symmetry can be promoted to local symmetry, but here it is treated as global.\n\n\n\n\n\n0.3 Classes of Particle Production Experiments: Fixed Target vs. Collider Setups\nToday is 8, 6, 6, and we start lecture 5 on particle production. I called the first section “A Dictionary and Slang of Particle Production,” but I essentially want to discuss a follow-up to what Farah started last time on different production mechanisms.\n\nWhen we talk about production kinematics, it makes sense to distinguish a few different classes. We will see this classification depending on:\n\nThe reaction kinematics\nThe practicalities of the experiments\n\n\nI want to start by separating the two different classes of experiments:\n\nFixed target experiments: where you install the target and shoot it with a beam.\nCollider experiments: where you collide two beams.\n\nThere are practical differences in these reactions:\n\nThe energy one can achieve is much higher when you collide two particles.\nHowever, the decay products are also more boosted in a fixed target experiment.\n\nDepending on what you want to achieve—either higher collision energy or more forward-boosted decay products—you should choose different reactions.\n\nMost of the time, fixed target experiments study reactions that were historically easier to perform.\n\nYou just need the target and the beam, and the setup is much simpler.\nEssentially, you only care about the beam production.\nThe two particles are directed to your experiment, and the target is a simple fixed setup.\n\n\n\n\n\n\n\nThe interaction rate in fixed target experiments can be expressed as: R = \\Phi \\cdot \\sigma \\cdot n \\cdot L where:\n\n\\Phi = beam flux\n\\sigma = interaction cross section\nn = target number density\nL = target length\n\n\n\n\n\nAs an example, consider experiments with fixed target particle production and hadron spectroscopy studies. I would name the GlueX experiment that Farah introduced last time, where we have:\n\nA photon beam on a hydrogen target\nPion or proton beams\nSometimes a mixed beam of K and N\n\nThere are different targets:\n\nLead\nFront targets\nAmmonia (for polarized targets) But mostly, it’s a hydrogen target.\n\n\nTo give you an idea, hydrogen targets are the most popular in hadron spectroscopy.\n\nThey look like a 1-meter pipe, 30 cm long, filled with liquid hydrogen—which is highly explosive.\nHandling this apparatus requires great care.\nThe hydrogen must be kept liquid under high pressure.\nNot every experiment can operate with such a setup.\n\n\n\n\n\n\n\nAt CERN, setting up COMPASS took years of effort to pass security inspections for a liquid hydrogen target.\n\nIf it exploded, it could destroy the entire building.\nWith such a volume of liquid hydrogen (e.g., 1 meter long and 30 cm in diameter), it would be a major disaster.\n\n\n\n\n\nThe beam interacts with protons in the hydrogen target somewhere inside it.\n\nBy reconstructing final-state particles and tracing them back, you can determine the exact interaction vertex.\nIf you collect data for days and plot the vertex distribution, it matches the target shape.\n\nOne funny observation is that the target isn’t always filled to 100%.\n\nIf you plot the XY coordinates, you see empty regions where there are no vertices.\nThis is a transverse cut of the target.\n\nThe beam has a Gaussian profile: \n\\frac{dN}{dr} \\propto \\exp\\left(-\\frac{r^2}{2\\sigma_r^2}\\right)\n and is slightly wider than the target in COMPASS, so the entire target is exposed.\n\nIdeally, the beam should be more focused to avoid wasting particles that miss the target.\n\n\nA beryllium target is simpler—just a 3 cm disk, a few centimeters thick.\n\nIt’s the cheapest and easiest target to use.\nAt this thickness, it provides a significant particle production rate.\n\nLead targets have even higher Z, so the cross section is larger.\n\nThe cross section scales with Z (the number of protons in the target): \\sigma \\propto Z^\\alpha where \\alpha is typically 1-2 depending on the process.\nFor some physics programs, heavy-Z targets are preferred.\n\n\nEarlier, we discussed how the Buicks setup differs slightly. To produce photons for experiments, you use bremsstrahlung from electron scattering:\n\nWhen electrons pass through a thin foil (a few hundred micrometers), photons are emitted.\nThe electron beam is converted into a photon beam.\nElectrons that emit bremsstrahlung lose momentum and are deflected by a magnet, while the photons continue forward.\nA magnetic field placed after the foil removes all electrons, leaving only the photon beam to hit the target.\n\n\n\n\n\n\n\nThe bremsstrahlung cross section for photon production is: \\frac{d\\sigma}{dE_\\gamma} \\propto \\frac{Z^2}{E_\\gamma} where Z is the atomic number of the target and E_\\gamma is the photon energy.\n\n\n\n\n\n0.4 Exclusive vs. Inclusive Processes and Kinematic Frames in Particle Production\nTo discuss the physics of the production mechanism, we draw cartoons like this, where the incoming particles enter the blob on the left side and outgoing particles appear on the right side. When we account for all the particles and none are missing, we call the production “exclusive,” which should be contrasted with “inclusive” processes.\n\nIn inclusive processes, we characterize the class of reactions as statistical ensembles. We say we don’t care about each individual state—the 2 pions, 3 pions, 2 kaons—they are produced in addition to the one. We only care that the 1 kaon is produced and nothing else.\n\nExclusive and inclusive reactions are complementary to each other. If one wants to understand the exact details of the production mechanism—where this kaon comes from, which particular resonance—one has to study exclusive processes. Inclusive processes average over different production mechanisms and in turn provide access to average quantities that are sometimes not accessible otherwise.\n\n\n\n\n\n\n\nKey distinction:\n\nExclusive: All particles accounted for.\nInclusive: Only specific particles of interest are tracked, others are averaged over.\n\n\n\n\n\nIf you want to see how this strangeness is produced—well, depending on the case, it would probably be better to use muons here. That’s what is studied in COMPASS: a muon gets scattered, and a kaon is detected. To make these computations more physical—if you want to study the strangeness inside the proton—we would study reactions like this: the electromagnetic current from the muon interacts with the proton, and somehow a kaon gets produced. This gives us access to the gluon probability distribution function inside the proton—how strange quarks are produced from the sea of gluons inside the proton. That’s a typical example of an inclusive process.\n\nFor now, let’s focus on the kinematics of exclusive reactions. I want to introduce two important kinematic frames used for studying these reactions. In particle physics, we deal with four-vectors—quantities that, under boost transformations, follow Lorentz group representations. We know how to boost and rotate four-vectors, and the frame in which you perform computations does not matter in principle. However, for practical considerations, some frames are better suited. When discussing quantities like angles or specific energy values, it’s crucial to specify the frame.\n\nTranslating between frames is mathematically straightforward—a conceptually simple procedure. You are in one frame, you boost, and your energy and angles change. However, it still requires practice, which is why it’s worth discussing.\n\nLet’s start with the center-of-momentum frame. In this frame, the four-vectors of the beam and target sum to zero—they are back-to-back. Here, I have a photon, and here I have a proton. In the center-of-momentum frame, the magnitudes of these two vectors are equal. Then we have scattered particles: here, a recoiling proton, and here, a combination of two pions. Clearly, these two vectors are equal and back-to-back, as are these two. Thus, we have a reaction plane.\n\nThe total energy is given by the square of the sum of the four-vectors of the target and beam—this is our Mandelstam variable S, the center-of-mass energy squared:\n\nS = (P_{\\text{beam}} + P_{\\text{target}})^2\n\n\nThere’s a trick to evaluate the magnitudes of these vectors and momenta using this relation: for the target four-vector, we write it as the total minus the beam, square it, and obtain S + m_{\\text{beam}}^2 (which is zero for a photon) minus m_{\\text{target}}^2. This formula holds in any rest frame. If you have two particles at rest, the energy of each particle is computed as:\n\nE_1 = \\frac{S + m_1^2 - m_2^2}{2 \\sqrt{S}}\n\n\nAnother useful relation comes from squaring the energy expression and subtracting the mass: E^2 - m^2 = p^2, leading to the breakup momentum formula:\n\np = \\frac{\\sqrt{\\lambda(S, m_1^2, m_2^2)}}{2 \\sqrt{S}}\n\nwhere \\lambda(x, y, z) is the Källén (or triangle) function:\n\n\\lambda(x, y, z) = x^2 + y^2 + z^2 - 2xy - 2xz - 2yz\n\nThis function is symmetric in its arguments, so the order doesn’t matter.\n\nNow, let’s compute momenta for final-state particles. The expression remains the same—just replace the masses with those of the decay products. For example, for the two-pion system, we use m_{\\pi\\pi}^2, m_{\\text{recoil}}^2, and m_{\\text{proton}}^2.\n\nA question arises: This only works if we treat the two pions as a single particle moving in the same direction. Why can we do this? The answer is kinematics, not physics. We write momentum conservation as:\n\nP_{\\text{target}} + P_{\\text{beam}} = (P_{\\pi^+} + P_{\\pi^-}) + P_{\\text{recoil}}\n\nand replace P_{\\pi^+} + P_{\\pi^-} with P_{\\pi\\pi}. This composite four-vector has its own energy and momentum, but the pions need not be collinear—they can decay at an angle.\n\nNow, let’s expand on the \\pi\\pi production. We can go to the rest frame of the \\pi\\pi system—the Gottfried-Jackson frame—where the two pions are back-to-back. This frame is common in spectroscopy studies. It’s obtained by boosting and rotating along the resonance direction, aligning the beam with the z-axis and placing the target and recoil in the xz-plane.\n\nIn this frame, \\pi^+ and \\pi^- are back-to-back, and the beam is back-to-back with the difference between the target and recoil.\nAbsolutely. The same trick that I applied here now can work for these two vectors. We know that in this frame, the four-vector of the \\pi^+ and the four-vector of the \\pi^- end up as a vector that has no spatial component—it’s in the rest frame. Therefore, it’s just (m_{\\pi\\pi}, \\mathbf{0}). So let’s just write, for completeness:\n\nP_{\\pi^+}^{\\text{GJ}} + P_{\\pi^-}^{\\text{GJ}} = (m_{\\pi\\pi}, \\mathbf{0})\n\n\n\n0.5 Kinematics, Collider Experiments, and Particle Decay Analysis\nI will probably try to add a few more exercises on kinematics because that’s something worth practicing—how you derive these particular expressions—and it’s all over the place. It’s a very important skill to be able to operate with the four-vectors.\nBut we continue with discussing another type of the class of experiments, particularly collider experiments. It’s the Beijing Spectrometer. There are a few collider experiments in the world, and depending on the colliding particles, you have different physics.\n\nFermilab: Antiproton beams were collided here.\nLHC (Large Hadron Collider): The biggest running machine where two protons collide.\nLHCb: The most dedicated experiment to study strong interactions.\nElectron-positron machines: Currently running are the BESIII and Belle II experiments.\n\n\nTwo important concepts to introduce are production and decays:\n\nPrompt production: Hadrons produced directly from the colliding beams (primary vertex).\nDelayed production: Long-lived particles fly away from the primary vertex and decay later.\n\n\n\n\n\n\n\nThe strong interaction is called “strong” because it makes the cross section large and processes happen quickly. The weak interaction is “weak” because it has a smaller cross section, leading to longer lifetimes for particles decaying weakly.\n\n\n\nFor example:\n\nB meson, D meson, Lambda baryon, Omega baryons all decay weakly because the strong interaction cannot change flavor or color.\n\n\nWhen a particle decays weakly, its lifetime is large, allowing it to fly away from the primary vertex before decaying.\n\nB meson: Lifetime \\sim 10^{-9} seconds, but due to relativistic boost (\\gamma = \\frac{E}{m}), it lives longer in the lab frame.\nFor E = 500\\,\\text{GeV} and m = 5\\,\\text{GeV}, \\gamma = 100.\nLab-frame lifetime: 10^{-7} seconds.\nTravel distance: \\sim 20\\,\\text{mm}.\nCharm particles: Shorter travel distance (\\sim 5\\,\\text{mm}).\n\n\n\n\n\n\n\nThe distance a particle travels is not fixed—it follows an exponential distribution due to quantum decay probabilities:  N(t) = N_0 e^{-\\lambda t} \n\n\n\n\nIn experiments, we reconstruct decays by tracking final-state particles (e.g., pions). Key steps:\n\nTrack reconstruction: Fit trajectories (straight or curved in a magnetic field).\nVertex fitting: Identify secondary vertices displaced from the primary vertex.\nImpact parameter (IP): Closest distance of a track to the primary vertex, normalized by uncertainty:  \\text{IP} \\chi^2 = \\left( \\frac{d}{\\sigma_d} \\right)^2 \n\n\nLambda decays: Can fly meters (e.g., 5\\,\\text{m} at LHCb due to high boost).\n\n\nWhen analyzing decays (e.g., B^0 \\to 3\\pi), we compute the invariant mass:  M = \\sqrt{\\left( \\sum_i E_i \\right)^2 - \\left( \\sum_i \\vec{p}_i \\right)^2 } \nThe spectrum shows:\n\nPeak: Centered at M_B, smeared by experimental resolution (Gaussian shape).\nBackground: Combinatorial noise from random track associations.\nSecondary peaks: Misidentified particles.\n\n\n\n\n\n\n\nThe multiplicity of proton-proton collisions at the LHC (7\\,\\text{TeV}) is a few thousand particles per event, leading to high combinatorics in track reconstruction.\n\n\n\n\nTheoretically, the spectrum should be a delta function at M_B, but experimental uncertainties widen it into a Gaussian.\n\nKey factors: Tracking precision, hit resolution, and energy/momentum measurements.\nBackground: Arises from random combinations of pions passing selection cuts.\n\n\n\n0.6 Particle Identification and Phase Space in Decay Dynamics\nThe last feature I want to discuss here is PID (Particle Identification). It’s important for all experiments.\nWhen we track particles, we don’t measure their mass directly. The momentum is determined by the curvature of the track in the magnetic field. By solving the differential equations of charged particle motion in a magnetic field, we can fit the momentum to the measured points.\nThe four-vector has energy and momentum components. We measure momentum, but energy is computed under a mass assumption. For these calculations, we must assume the particle is a pion.\nAmong thousands of charged particles, they all look similar without extra identification. Without PID detectors, we can’t distinguish pions, protons, or kaons. This assumption introduces background.\nFor example, in an event like B \\to K^- \\pi^+ \\pi^-, the tracks look identical to our signal. If we misidentify the kaon as a pion, we compute the wrong energy. The momentum is correct, but the energy is not. This leads to an incorrect invariant mass.\nIf we assume a pion mass for a kaon, the reconstructed mass shifts. Since the kaon mass is larger, using a smaller pion mass distorts the result. Without PID, we’d be in trouble.\nThat’s why experiments need good particle identification detectors, like ring-imaging Cherenkov or time-of-flight systems. These suppress misidentification.\n\n\n\n\n\n\n\nKey Challenge in PID: Misidentifying particles (e.g., kaons as pions) leads to incorrect energy calculations and distorted invariant mass reconstructions. Proper PID detectors are essential to suppress this background.\n\n\n\n\nIn experiments, simulation is crucial. We model collisions, decays, and detector interactions using computer programs.\nWe must know cross-sections and available phase space—the configuration space for particles.\nFor an N-particle phase space, the differential is:\n d\\Pi_N \nwith 3N - 4 integrals, reflecting four-momentum constraints.\nFor two-body phase space:\n d\\Pi_2 = \\frac{1}{8\\pi} \\frac{2p}{\\sqrt{s}} d\\Omega \nAfter resolving energy-momentum conservation, only angular integrals remain.\nFor three-body phase space, it’s more complex:\n d\\Pi_3(1,2,3) = d\\Pi_2(1,2) \\times d\\Pi_2((12),3) \\times \\frac{dM_{12}^2}{2\\pi} \nWe split phase space into pairs, introducing intermediate masses.\nThe Kibble function defines the Dalitz plot boundary:\n \\Phi = \\lambda_1 \\lambda_2 \\lambda_3 - \\lambda_1 \\lambda_{23}^2 - \\lambda_2 \\lambda_{13}^2 - \\lambda_3 \\lambda_{12}^2 \\leq 0 \nwhere \\lambda is the Källén function:\n \\lambda(x,y,z) = x^2 + y^2 + z^2 - 2xy - 2xz - 2yz \nThe differential decay width is:\n d\\Gamma = \\frac{|\\mathcal{M}|^2}{2M_0} d\\Pi \nOn the Dalitz plot, phase space is flat—only the matrix element introduces structure.\n\nLet’s map momenta to a Dalitz plot. Given three momenta summing to zero, we find their coordinates.\nHere’s a COMPASS measurement example. The mass distribution in a 2D plot reveals dynamics—phase space is uniform, but the matrix element shapes the data.\n\n\n\n\n\n\n\nPhase Space & Dalitz Plots:\n\nTwo-body phase space reduces to angular integrals after momentum conservation.\nThree-body phase space decomposes into sequential two-body systems with intermediate masses.\nThe Kibble function \\Phi \\leq 0 defines the physical boundary of the Dalitz plot."
  },
  {
    "objectID": "2025-Lecture-07.html",
    "href": "2025-Lecture-07.html",
    "title": "(2025) Lecture 7",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Anna Zimmer\n\n0.1 Helicity Formalism, Weak Decays, and Isospin Violation in \\Lambda^0 Decay\nWelcome everyone. Today we have a seventh meeting on hadron physics. Today’s lecture will expand on the previous lecture on helicity formalism. We will discuss partial wave expansion, which you have seen already in homework, and move toward general principles discussing the Mandelstam plane and analyticity.\nToward the end of the lecture, as always, I invite you to look at the problems to recap the material from the previous lecture. I will give you five minutes to think about them, and then we will discuss together.\n\nWhich interaction is responsible for the \\Lambda^0 decay to the proton and pion?\n\nIs isospin conserved?\nIs parity conserved?\nIs angular momentum conserved?\n\nDemonstrate that the rotations we discussed last time are not different from isospin rotations—they are both represented by an SU(2) group.\n\nWhat is the 30-degree isospin rotation about the Y-axis?\nHow does it act on the \\Delta^+ state?\n\nGiven a Dalitz plot, determine which decay case takes more time.\nLet’s go through the problems quickly, starting with 1, then 3, and then 2.\n\nThe weak interaction is responsible for the \\Lambda^0 \\to p + \\pi^- decay. It must be weak because there is a flavor change—the \\Lambda^0 contains a strange quark, while the final state does not. Flavor change always indicates weak interaction.\n\n\n\n\n\n\nWe had a seminar yesterday discussing proton-proton to \\Lambda-proton transitions in neutron stars. If you compress neutrons in a neutron star, a neutron-neutron interaction could produce a \\Lambda and other particles. This involves a flavor change, so it must be weak. But weak interactions are too slow for production processes in dense matter—likely a typo or missing quarks in the description.\n\n\n\nFor ground-state baryons, most decays are weak.\nThe initial state \\Lambda^0 has J^P = \\frac{1}{2}^+, the proton has \\frac{1}{2}^+, and the pion has 0^-. Parity conservation would require orbital angular momentum L = 1: \nP_{\\text{final}} = P(p)P(\\pi^-)(-1)^L = (+1)(-1)(-1)^1 = +1\n However, the decay also occurs via L = 0 (parity-violating): \nP_{\\text{final}} = (+1)(-1)(-1)^0 = -1\n In weak decays, parity is not conserved—there is always a parity-violating component.\nYes, angular momentum is always conserved in particle physics due to Lorentz symmetry. For \\Lambda^0 \\to p + \\pi^-: \nJ(\\Lambda^0) = \\frac{1}{2}, \\quad J(p) = \\frac{1}{2}\n The balance is achieved by L = 0 or L = 1.\nNo. The \\Lambda^0 has isospin I = 0, the proton has I = \\frac{1}{2}, and the pion has I = 1. The transition 0 \\to \\frac{1}{2} + 1 violates isospin. Weak interactions generally break isospin, similar to parity.\n\n\nIsospin Conservation (broken in weak decays): \n\\Delta I \\neq 0\n Example for \\Lambda^0 \\to p + \\pi^-: \nI(\\Lambda^0) = 0, \\quad I(p) = \\frac{1}{2}, \\quad I(\\pi) = 1\n\nParity Conservation (violated in weak decays): \nP_{\\text{initial}} \\neq P_{\\text{final}}\n\nAngular Momentum Conservation: \n\\vec{J}_{\\text{initial}} = \\vec{J}_{\\text{final}} + \\vec{L}\n\nSU(2) Rotation (isospin space): \nR_y(\\theta) = e^{-i\\theta I_y}, \\quad \\theta = 30^\\circ\n Acting on \\Delta^+ (I = \\frac{3}{2}, I_3 = +\\frac{1}{2}).\nDalitz Plot (decay kinematics): \ndm_{12}^2 \\cdot dm_{23}^2\n\n\n\n\n0.2 Identifying the Decaying Particle via Dalitz Plot Analysis and Resonance Bands\nAll right, question number three. We agreed. Any guesses: who decays? How?\n\nFirst, how many particles are in the final state? Three.\nHow many particles are in the initial state? One.\nSo it’s a 1-to-3 decay.\nThat’s why we represent this on the Dalitz plot.\n\n\nWhat is the final state? On the first row—Alex, what is the final state? Benjamin? Benedict? Ah, we are not in France. Benedict. What are the particles in the final state? Right, very good.\nHow did you guess? Because those are the three particles mentioned on the axes.\n\nOn the X-axis, you have one pair: proton and K-minus.\nThe other pair is on the Y-axis.\nThat is always the mass of one subsystem versus the mass of the second subsystem.\nThat’s how you figure out the Dalitz plot.\n\n\nWhat are the subsystems of the decay? This is the X-axis, this is the Y-axis. Something like that.\n\n\n\n\n\n\nDalitz plot variables: \nX = (m_{pK^-})^2 \\quad \\text{vs} \\quad Y = (m_{K^-\\pi^+})^2\n\n\n\n\nNow, what remains to figure out is who decays, and what are the clues?\n\nOn the Dalitz plot, there’s a horizontal band—the strongest resonance in the plot.\nThe particle that decays into the proton and K-minus is that resonance.\nSo there will be some proton K-minus.\nYou’re talking about which line? This one.\nThat line gets projected into the peak in this dimension.\nSo that line would be a resonance in the pion K.\nThese are K-stars.\n\n\nDo we have bands in this dimension? Something that will project to the peaking structure here?\n\nThese are lambda states, and these are K-star states.\n\nHow do we figure out who decays?\n\nOne clue is the internal structure.\nHow does the decay proceed? What are the intermediate states?\nBut if the question is: who decays? What particle decays into this final state?\nWhat are the clues? Perhaps masses somehow.\nThe initial state should probably be charged.\n\n\nCan we compute the mass of this particle?\n\nThe phase space will be limited by the mass.\nWe can take the upper border of the X-axis, and the square root of that, minus the lower border on the Y-axis, and the square root of that lower.\n\n\n\n\n\n\n\nPhase space boundaries: \n(m_{p} + m_{K^-})^2 \\leq X \\leq (M_0 - m_{\\pi^+})^2\n \n(m_{K^-} + m_{\\pi^+})^2 \\leq Y \\leq (M_0 - m_{p})^2\n\n\n\n\nWhat do you get?\n\nThe minimum value on the Y is the masses of the two particles.\nThe upper border here is (M_0 - M_\\pi)^2.\nHere the value is (M_0 - M_p)^2.\nThe minimum value is (M_K + M_\\pi)^2.\nHere the minimal value is (M_p + M_K)^2.\n\nSo indeed, you find M_0 by looking at the highest value.\n\nTake the square root, as Henry said, and add the mass of the pion.\nOr take the highest value here, square root, and add the mass of the proton.\n\nWhat do you get? 2.24.\n\nAdd the mass of the proton, 1 GeV, something like that.\nThen we open the PDG and look for a particle with a mass of 2.6 or similar.\nThere will be few in this domain.\n\n\nOne has to figure out whether the decay happens weakly or strongly.\n\nIf it happens strongly, what should the quark content be?\nThe proton is uud, the K-minus is s\\bar{u}, and the pion is u\\bar{d}.\nThese could come from the vacuum.\nThe rest you cannot cancel with the vacuum: u d s \\bar{u} \\bar{d}.\n\nIs it a pentaquark that decays strongly? Or a big decay of something?\n\nIn fact, it’s a weak decay of the particle called \\Xi_c^+.\n\nWait, why can’t you cancel d\\bar{u}?\n\nOh, it’s true. Thanks, Ilia.\nThen who is this? \\Xi.\nThis is the \\Xi^+ without C.\n\nWhat we find is that \\Xi^+ can decay to this combination \\Sigma^+ \\Sigma^+.\n\nThanks. \\Sigma or \\Lambda? \\Sigma^+, because \\Lambda is only zero.\n\n\nNow, almost full view.\n\nIt could be a strong decay of the \\Sigma^+.\nMaybe isospin. Can you make it from isospin?\nThe \\Sigma^+ has isospin 1, the proton has isospin \\frac{1}{2}, the K-minus has isospin \\frac{1}{2}, and the pion has isospin 1.\nFrom \\frac{1}{2}, \\frac{1}{2}, and 1, you can make 1.\nSo isospin is conserved. Could be conserved.\n\n\n\n\n\n\n\nIsospin conservation check: \nI(\\Sigma^+) = 1 \\quad \\text{vs} \\quad I(p) = \\frac{1}{2}, \\quad I(K^-) = \\frac{1}{2}, \\quad I(\\pi^+) = 1\n\n\n\n\nBut in fact, what I did—I looked at the \\Xi_c angular analysis and printed that plot of the \\Xi_c decay.\n\nThe last question here is about kinematics.\n\nIt’s good that you know these resonances, and we will discuss them more in the lecture.\nBut when you just need to know the mother particle, you check the conservation laws.\nYou can figure out that the mass of the decaying particle is on the borders of the phase space.\n\n\n\n\n\n\n\nQuark content conservation (weak decay): \n\\Xi_c^+ (usc) \\to p(uud) + K^-(\\bar{u}s) + \\pi^+(u\\bar{d})\n\n\n\n\n\n\n0.3 Isospin Operators, Rotations in Isospin Space, and QCD Evolution in Two-Body Decays\nQuestion number two. Isospin operators are given. Is it true that isospin acting on \\Delta^+ gives \\frac{15}{4}? \\Delta^+ is the first state. Who thinks it’s true? Who thinks it’s wrong?\nThe J^2 operator acting on the state gives J(J+1). That’s what I meant here. This is \\frac{3}{2} \\times \\frac{5}{2}.\n\n\n\n\n\n\nFor \\Delta with isospin I = \\frac{3}{2}: \nI^2 |\\Delta^+\\rangle = I(I+1) |\\Delta^+\\rangle = \\frac{15}{4} |\\Delta^+\\rangle\n\n\n\n\n\\Delta has isospin \\frac{3}{2}, so the first expression is true.\n\nThe second expression: the projection to the z-axis of \\Delta^+ is \\frac{1}{2}. Who thinks it’s true? Who thinks it’s wrong?\n\\Delta^+ sits in the multiplet with four states. The upper one always has the highest isospin projection:\n\n\\left|\\frac{3}{2}, \\frac{3}{2}\\right\\rangle,\n\\left|\\frac{3}{2}, \\frac{1}{2}\\right\\rangle,\n\\left|\\frac{3}{2}, -\\frac{1}{2}\\right\\rangle,\n\\left|\\frac{3}{2}, -\\frac{3}{2}\\right\\rangle.\n\nThe operator I_z gives the z-projection of the isospin vector. It maps states in the multiplet.\n\n\n\n\n\n\nThe isospin projection for \\Delta^+: \nI_z |\\Delta^+\\rangle = \\frac{1}{2} |\\Delta^+\\rangle\n\n\n\n\nIf I act with I_z on \\Delta^+, I get \\frac{1}{2}. These states are eigenstates of I_z, so this is also true.\n\nRotations in isospin space: If you apply a rotation, as Farah explained last time, states transform under SU(2). For a rotation acting on |JM\\rangle, you get a mixed state with coefficients.\nApplying a y-rotation of 30 degrees to \\Delta^+ gives a mixed state of different \\Delta states. The D coefficients are known functions, found in the PDG table with Clebsch-Gordan coefficients. These are trigonometric functions.\nLet’s check D_{3/2,1/2}(30^\\circ): \nD_{3/2,1/2}(30^\\circ) = -\\frac{\\sqrt{3}}{2} (1 + \\cos\\theta) \\cdot \\frac{\\sin\\theta}{2}.\n For 30 degrees, you can express \\sin\\theta as \\sqrt{1 - \\cos^2\\theta}.\nWhen you apply a rotation in spin space to an eigenstate, you get an expansion into all states. The coefficients give the probabilities: \n|D_{3/2,3/2}|^2 + |D_{3/2,1/2}|^2 + |D_{3/2,-1/2}|^2 + |D_{3/2,-3/2}|^2 = 1.\n Probability is conserved.\nThis shows rotations in isospin space are no different from regular rotations—both are driven by SU(2).\n\nQuestions: This was part of the homework. The last coefficient D_{3/2,-3/2,1/2} isn’t listed. Is it the same as D_{3/2,1/2}?\nThe parity relations are listed. If you switch the bottom indices, both get a minus sign: \nD_{-M_1,-M_2} = (-1)^{M_1 - M_2} D_{M_2,M_1}.\n But D_{-3/2,-1/2} is also not given.\nYou can swap indices or add a minus sign. For unitary matrices, there’s another relation: For a negative rotation, you can complex conjugate and transpose. Transposing swaps the indices.\nThis comes from the unitary nature of SU(2). The two Wigner functions are the same: swapping and adding a minus sign gives the same result.\n\nToday’s lecture: QCD evolution. We’ll apply helicity formalism to express amplitudes. The biggest danger is confusing models with general statements. We’ll make approximations—ask me to clarify what’s general vs. model-dependent.\n\nTwo-body decay reminder: In the rest frame of the decaying particle, the matrix element depends on:\n\nKinematic variables,\nSpin configurations (M_0, \\lambda_1, \\lambda_2).\n\nThe total spin controls the angular dependence: \n\\mathcal{M}_{M_0,\\lambda_1,\\lambda_2} = H_{\\lambda_1,\\lambda_2} \\cdot D^{J_0 *}_{M_0,\\lambda_1-\\lambda_2}(0,\\theta,\\phi).\n The D function comes from active rotations:\n\nRotate about Y by \\theta,\nRotate about Z by \\phi.\n\nThe reduced matrix element H_{\\lambda_1,\\lambda_2} depends only on mass, not angles. It projects the helicity states onto the initial state: \nM_0 = \\lambda_1 - \\lambda_2.\n\n\nCascade decays: For decays with intermediate states, we use the same construction. The general structure includes:\n\nD functions for each decay vertex,\nReduced matrix elements H,\nPropagators for intermediate resonances.\n\nThe amplitude is: \n\\mathcal{M} = \\sum \\left( D^{J_0 *}_{M_0,\\lambda_1-\\lambda_2} \\cdot H^{(1)}_{\\lambda_1,\\lambda_2} \\cdot \\text{Propagator} \\cdot H^{(2)}_{\\lambda_3,\\lambda_4} \\cdot D^{J_1 *}_{\\lambda_1-\\lambda_2,\\lambda_3-\\lambda_4} \\right).\n\nThis gives a recipe for any cascade decay with spin.\n\n\n\n\n\n\n\nKey takeaway: The helicity formalism provides a systematic way to handle spin-dependent decays, with Wigner D-functions capturing the angular dependence and reduced matrix elements H encoding the dynamics.\n\n\n\n\n\n0.4 Factorization, Helicity Amplitudes, and Angular Parameterization in Cascade Decays\nThis is the model, and it is a model not because of the angular functions—they are precise and exact—but because of the factorization. We are assuming that the matrix element can be factorized into the initial first decay and the second decay. The fact that we split the interaction matrix element into two parts—here is the reduced matrix element of the first transition, and here is the second—is our modeling assumption: factorization.\n\n\n\n\n\n\nThe factorization of the matrix element is expressed as: \n\\mathcal{M} = \\mathcal{M}_1 \\cdot \\mathcal{M}_2\n where \\mathcal{M}_1 and \\mathcal{M}_2 are reduced matrix elements for the first and second transitions, respectively.\n\n\n\nWhat is missing here is that we didn’t introduce \\lambda. You see that the intermediate state appearing here also has a spin projection \\lambda, and that’s the one carried by one of the particles. This is something up to a sum over the angles that appear as arguments. They are not the same but are specific to particular transitions here and there, so we better label them differently. Keeping consistent labeling is always tricky.\nWhen you apply this cascade decay parameterization, you have a million different angles, and labeling them requires some rule to start. For example, for the pair of particles that go back-to-back, say IJ, I can use the IJ index here. The angles will have indices \\theta_{12}, \\phi_{12}. Here it’s 1, 2, 3, and now I would like to invite you to do some 3D imaging. I enjoyed this very much, and I hope you seeing this once can also enjoy.\n\nI want to sketch on the board in 2D how it looks in 3D—the definition of the axes. We need axes, and that’s how it starts. Here is the representation that I think is easy to visualize.\nWe start with particle 0 in the lab frame. It flies in this frame, and the particle has the helicity \\lambda_0. This particle decays into a combination of particles 1, 2, and 3. We can boost the system to its rest frame following this axis. In the rest frame, \\lambda_0, which was the helicity, becomes the canonical spin projection. If you think of this—here is the z-axis, here is m_0 (the rest frame of particle 0)—then the spin is quantized along the z-axis by our construction, since we boost in this direction.\nIn this frame, particles 1, 2, and 3 go back-to-back. To apply the second part, you follow the combination of particles 1 and 2. It’s the same as we started from: in this frame, it flies in a certain direction and decays into two particles. We need to boost to its rest frame, where it decays into particle 1 going back-to-back.\n\nLet’s define the angles. Angles are taken as spherical angles of one of the decay products. If you boost directly, the spherical angles of one decay product would be \\theta and \\phi. Here is \\theta_{123}, and here is \\phi_{12}.\nFor the second transition, \\theta is the angle with respect to the z-axis in the rest frame of the particle. Here is \\theta_{12}, and here is \\phi. When you deal with this practically, you follow the chain and introduce angles while applying constraints.\nIn experimental physics, we are given four-vectors. You’re literally given three four-vectors for particles 1, 2, and 3 in the lab frame. The convention is the active transformation: first, find the spherical angles of the (1, 2, 3) combination, then rotate to align the 1-2-3 vector with the z-direction. Next, boost in this frame, take \\theta (the angles of the 1-2 pair), align it with the z-axis, boost, and apply rotations.\n\nThe topology we draw here is not the only one possible. You could imagine the same transition happening through the combination of particles 2 and 3 or 3 and 1, and you would write the same expression. Before discussing resonances in other channels, let’s note some caveats. The main caveat is that the helicities in the expressions are taken from different rest frames. To apply the helicity formalism and replace the expectation value with the reduced matrix element (helicity coupling), we must be in the particle’s rest frame. Thus, the \\lambda values here are taken in the rest frame of the pairs.\nWhat does it mean for the quantization axis to be taken from the z-axis of the appropriate frame? From the last homework, we saw that helicity is affected if you boost off-axis. I prefer to write the amplitude where the \\lambda are defined in the same frame: \\lambda_1, \\lambda_2, \\lambda_3, \\lambda_0. I get a linear combination of amplitudes. It’s bizarre, but we don’t use this literally because we don’t care about the \\lambda themselves—we sum them up later. Consistency of the quantization axis only matters for coherent processes using different axes.\n\nWhat’s important is the axis in this matrix element where spins are quantized. \\lambda_0 comes from the lab frame (helicity in the lab frame). \\lambda_3 appears in… and these two are… It’s totally okay. Once I square the matrix element, the sum over helicities doesn’t depend on the quantization axis. But if I add this amplitude to another (e.g., an expansion in a different topology), I must be careful about the helicity quantization. First important note: watch your helicity quantization axis.\nSecond important note: we implicitly assume factorized vertices. This is a good assumption, especially for narrow resonances. Even for broad resonances, if the dynamics are driven by the 1-2 combination, factorization is reasonable. The Wigner d-functions here are properties of rotations—not model assumptions. Lorentz symmetry requires them. If we take the expression without factorization, join the matrix elements with the propagator, and compute the largest product of the three, this is model-independent. It’s safe to pull out rotational functions but not to break them.\n\nAnother note: we’ve explicitly found the angular dependence for a spin-J particle in the N2 combination. The heuristic formalism identifies the matrix element’s dependence on angles. All I care about is the spin of particles 1 and 2. This form doesn’t give the answer—it’s simply D^J with the appropriate angles. Dealing with spin is complicated, as you’ve seen.\nFor further discussion, let’s reduce to scalar particles. If a scalar decays into three scalars, the matrix element has no indices but still depends on dynamic variables.\nLook at this expression: the only possible \\lambda value is zero. For spin zero, there’s a single state in the multiplet, transformed under rotations. Only the phase can change, and \\lambda must be zero. This term gives \\delta_{\\lambda 0}, which propagates further.\nThe expression simplifies: the matrix element depends only on one polar angle. In the rest frame of two particles, all angles drop out except one. The convenient way to write this is…\n\nI’m moving toward matching this expression to the partial wave expansion in the homework. That’s why I pulled helicity couplings and reduced matrix elements into X and introduced the 2J+1 coefficient.\n\n\n\n\n\n\nThe partial wave expansion for scalar decays is given by: \n\\mathcal{M} = \\sum_J (2J+1) X_J(s) d^J_{00}(\\theta)\n where d^J_{00} is the reduced Wigner function.\n\n\n\n\nKey formulas integrated:\n\nFactorization: \\mathcal{M} = \\mathcal{M}_1 \\cdot \\mathcal{M}_2\nHelicity amplitude: \\mathcal{M}_{\\lambda_0 \\to \\lambda_1 \\lambda_2 \\lambda_3} = \\sum_\\lambda \\mathcal{M}_{\\lambda_0 \\to \\lambda} \\cdot \\mathcal{M}_{\\lambda \\to \\lambda_1 \\lambda_2 \\lambda_3}\nWigner D-functions: \\mathcal{M} \\propto D^{J_0}_{m_0,\\lambda_0}(\\theta,\\phi) \\cdot D^{J}_{m,\\lambda}(\\theta',\\phi')\nScalar case: \\mathcal{M}_{\\text{scalar}} = X(s_{12}, s_{23}) \\cdot \\sqrt{2J+1} \\cdot \\delta_{\\lambda_0,0}\nPartial wave expansion: \\mathcal{M} = \\sum_J (2J+1) X_J(s) d^J_{00}(\\theta)\n\n\n\n0.5 Partial Wave Expansion and Resonance Modeling in Dalitz Plots\nWe started by writing the matrix element for the decay that proceeds through the resonance with spin J. We found that the angular dependence of this matrix element is given by d^J_{00}, which are Legendre polynomials P_J(\\cos\\theta).\n\n\n\n\n\n\nThe matrix element for resonance decay with spin J is: \n\\mathcal{M} \\propto d^J_{00}(\\theta) = P_J(\\cos\\theta)\n where P_J are Legendre polynomials.\n\n\n\n\nI would like to match this to the general technique for analyzing the matrix element for any expression with angular dependence. We can expand this in a series of Legendre polynomials and analyze the coefficients, leaving only the mass dependence. The coefficients in this expansion represent dynamics corresponding to the specific spin J of the resonance.\n\nWe started from a modeling approach, but it provides interpretation and intuition. What are the coefficients in the partial wave expansion? The total matrix element in this scalar transition depends on only two variables: the scattering angle \\theta and the mass s. This is also what we see in the Dalitz plot, where the two variables are the masses of the two subsystems. The dependence can be explored by analyzing the coefficients in the partial wave expansion.\n\n\n\n\n\n\nThe partial wave expansion of the matrix element is: \n\\mathcal{M}(s, \\theta) = \\sum_{J=0}^\\infty (2J+1) a_J(s) P_J(\\cos\\theta)\n where a_J(s) are the dynamical coefficients for each spin J.\n\n\n\n\nThe expansion is an exact representation if the series is infinite. Truncating the series makes it an approximation. In experiments, we often truncate the partial wave series because high spins are rare. Looking at the PDG, particles with large J are not abundant, and amplitudes for high J are suppressed. This provides a natural limit to the expansion.\n\nThe highest observed spin is J = 6. While quark models predict higher spins like J = 7 or 10, these have not been observed experimentally. They are likely too high in the excitation spectrum and too broad to couple effectively. Practically, we truncate the series at J = 6.\n\nFor channels where resonances appear in one subsystem, the partial wave expansion is a good model. When resonances appear in multiple channels, we use a sum of truncated series instead of a single infinite expansion. For example, in Dalitz plots, resonances can appear in pairs (1,2), (2,3), or (3,1). The model, when projected onto any channel, still contains an infinite number of partial waves.\n\nThis is key: the angles in the expansion for (2,3) are defined in the (2,3) rest frame, different from the (1,2) channel. Each term in the full expansion projects onto an infinite number of Legendre polynomials in any single channel.\n\nThis approximated amplitude is called the Isobar model in hadron spectroscopy, where Breit-Wigner amplitudes parameterize the peaks phenomenologically. It works well but is simplistic. A more general approach is the cascade decay model, where three truncated series are used without assuming factorization.\n\n\n\n\n\n\nThe Breit-Wigner propagator for resonances is: \n\\mathcal{BW}(m) = \\frac{1}{m_0^2 - m^2 - i m_0 \\Gamma(m)}\n where m_0 is the resonance mass and \\Gamma(m) is the energy-dependent width.\n\n\n\n\nThe Chew-Mandelstam Ansatz starts similarly but includes final-state interactions and rescattering. It introduces theoretical constraints, leading to more sophisticated dynamical functions. The amplitudes correct each other through rescattering, accounting for triangle-like processes. While derived from dispersion relations, it still involves modeling assumptions.\n\nFinally, let’s return to the Dalitz plot and discuss modeling it using the helicity formalism. The inhomogeneities in the plot reflect resonances (bumps) and spin-angle distributions (structures). To apply the helicity formalism, we need:\n\nThe spins of the particles.\nThe reduced matrix elements.\nThe propagator, parameterized via the Breit-Wigner form with mass, width, and spin J.\n\n\n\n\n\n\n\nThe Dalitz plot density distribution is: \n\\frac{d^2\\Gamma}{dm_{12}^2 dm_{23}^2} \\propto |\\mathcal{M}(m_{12}, m_{23}, \\theta)|^2\n where m_{ij} are invariant masses of particle pairs.\n\n\n\n\nLooking at the plot, we identify resonances:\n\nFor K\\pi, the horizontal band corresponds to K^*(892) with J^P = 1^-.\nFor Kp, the vertical bands include \\Lambda(1520) (J^P = 3/2^-) and \\Lambda(1690).\nThe diagonal line is \\Delta(1232) (J^P = 3/2^+).\n\nWith these inputs, we can construct the Dalitz plot. However, quantization axes complicate the helicity formalism for spin-\\frac{1}{2} particles like the proton. For scalar particles, the approach would be straightforward.\n\nAny questions? Next time, we’ll cover resonance dynamics, advanced Breit-Wigner parameterizations, and constraints from unitarity and analyticity."
  },
  {
    "objectID": "2025-Lecture-08.html",
    "href": "2025-Lecture-08.html",
    "title": "(2025) Lecture 8",
    "section": "",
    "text": "Presenter: Farah Afzal\nNote Taker: Anna Zimmer\n\n0.1 Scattering Matrix, Unitarity, and the K-Matrix Formalism\nWe start with recap problems. Here all three are positive. So:\n\nG is positive,\nS is positive,\nand also U is positive.\n\nThis would happen if we have a constellation where one of the particles decays into the other three, and we then have a Dalitz plot here.\nFor the S channel, we can see that:\n\nU is negative\nT is negative\n\nDepending on what type of reaction you have—for example, if it’s an elastic scattering or non-elastic scattering reaction—we can also have mass constraints here.\n\n\n\n\n\n\nElastic vs. Non-Elastic Scattering:\n\nIf it’s elastic, then this would be just the masses here four times, and similarly here as well.\nFor non-elastic, the constraints differ.\n\n\n\n\nIn general, there’s a function called the Kibble function that gives us the constraints and how exactly these areas are constrained. We just put some variation here.\nWhat I have sketched is the more general case where we don’t have elastic scattering. For elastic scattering, we would have a different kind of border where it would go a little bit over here.\nYou can look this Kibble function up, for example, in the book by Spearman and Martin. There, different cases are discussed on how these borders change when you have inelastic or elastic scattering.\n\nNow let’s move on to the scattering matrix. You have two terms:\n\nThe identity matrix (\\mathbb{I})\nThe transition matrix (T)\n\nThis means when we have scattering where basically nothing happens, we use the identity matrix. Here is where the interaction is then included.\nBecause of the way it’s written—it’s not clear—let me write it again:\n\nS = \\mathbb{I} + 2\\pi i \\, T\n\nThis means no interaction.\nAn important condition we have is that the probability of finding a system in a certain state must sum to one.\n\nYou can have a sum of different possibilities for the probability, and the total should always be one.\nWe can’t have more than one, so the total conservation of probability is given for n different states for a certain initial state.\n\nThis is just the identity. If you take any two different orthogonal states, this conservation of probability means that S has to be a unitary matrix. This condition is called unitarity:\n\nS^\\dagger S = \\mathbb{I}\n\nIf you now use what we have written down for the scattering matrix—the identity matrix plus 2\\pi i times the transition matrix T—let’s put this into the unitarity condition and see what comes out.\nThis has to satisfy the unitarity condition. What we can do is cross this out and move these two terms, for example, to the other side. You can also divide by 2\\pi i.\n\nWhen we want to calculate, for example, a cross section or transition probabilities, we do it like this:\n\nWe have an initial and final state\nThen the transition between them\n\nIf you put this condition here, you sum over different states. In principle, you can also write this as an integral over the phase space.\nIf we look at an initial and final state, we can:\n\nSum over all the different spin projections\nWrite it either as a sum or as an integral over the phase space\n\nThese are basically the phase space vectors that come in.\nWhen you insert identity, it’s written as the integral over possible states. Here’s the insertion of that identity, and it comes within the phase space.\nSo identity then—if you have two particles with two momenta, the identity can be expressed as the integral over the phase space of these states:\n\n\\mathbb{I} = \\int d^3p_1 \\, d^3p_2 \\, (2\\pi)^4 \\delta^4(p_0 - p_1 - p_2)\n\nWhen you insert identity, you integrate over all momentum. You don’t constrain particles to be on-shell in energy, only in mass. So it’s not a \\delta^5 but rather d^3p_1 \\, d^3p_2.\nThe phase space comes once you add the delta function constraint:\n\n(2\\pi)\\delta(E_2 - E_1 - E_2)\n\nand then for each particle. The difference between phase space and this integral is the \\delta^4(p_0 - p_1 - p_2), the energy-momentum conservation that comes from the matrix element.\nFrom any of that, it comes with a \\delta^4 for energy-momentum conservation, like this:\n\n\\langle f | T | i \\rangle \\propto \\delta^4(p_f - p_i) \\cdot \\mathcal{M}\n\n\nNow let’s think about the scattering amplitude. Here we have two particles, 1 and 2. We can look, for example, in the center-of-mass system where we define the z-axis in the direction of motion of p_1 and p_2.\nThey are just 180 degrees to each other. Here we have the scattering angle \\theta. In the center-of-mass frame, the momenta are equal and opposite to each other.\nThis we can then express as the scattering amplitude part and then a part where we have momentum conservation. This scattering amplitude depends on variables like s and t.\nIn practice, we usually fix, for example, t and then look at the s-dependence. We can write the scattering amplitude and decompose it into different partial waves:\n\nA(s,t) = \\sum_j (2j + 1) \\, A_j(s) \\, P_j(\\cos \\theta)\n\nA partial wave expansion means we decompose the scattering amplitude into an infinite series of these partial waves, where we sum over the spin j.\nHere we have the Legendre polynomials describing the angle dependence. Then you have a term that depends on s minus some variable—this is where the mass dependence goes in, describing the resonance, its mass, width, and so on.\nThis is practical because what we measure in experiments are angular distributions. We know that if you’re looking for certain states or resonances, they have a definite spin j.\nWhen we do this partial wave decomposition, we can divide up the different contributions we see in the data, making it easier to recognize new states.\nThese Legendre polynomials can be related to the Wigner D-functions. I also want to write this down in the notation we had previously, with helicity states.\nFor example, with two particles, we have the angle \\theta and an azimuthal angle \\phi. This is another way to write this, where we now have the partial wave amplitudes as a reminder:\n\nA(s, \\theta, \\phi) = \\sum_j A_j(s) \\, D^j_{\\lambda \\lambda'}(\\theta, \\phi)\n\nThe big Wigner D-functions depend on \\theta.\n\nBefore we discuss how to parameterize the mass dependence here, let’s first look at what the unitarity constraints mean for the scattering amplitude and for A_j.\nYou can see up there the unitarity constraint. If you apply this to the scattering amplitude—and for the partial waves, you can also write this down—\\rho is the phase space factor.\nWhat does it mean? These are complex numbers, so you can write this as the real part A plus i times the imaginary part of A, and this with a minus in between.\nWhat remains here is 2i times the imaginary part if we subtract this.\nThis is what the unitarity constraints mean for the scattering amplitude and the partial waves. What I want to discuss next is how we can describe the energy dependence for this.\nWe have already discussed a couple of times that what we can see, for example in the cross section, is a peak which we then call a resonance.\nThe question is: how can we describe this? If we have, for example, two particles where we added the four-momenta and then looked at their mass, and we see a peak-like structure here.\nOne of the easiest ways to describe this shape is with a relativistic Breit-Wigner amplitude:\n\nA(s) = \\frac{g^2 \\, \\rho(s)}{M^2 - s - i M \\Gamma(s)}\n\nwhere:\n\nM = mass of the resonance\n\\Gamma = coupling of the resonance to the initial and final state\n\\rho(s) = phase space\n\nWe are not just interested in the mass, but also what the width here is. This is, in this case, related to this part. So this is roughly the width divided by two.\nThis type of concept works well when we have an isolated resonance. One really good example for this is the \\Delta(1232) resonance, where the mass is at 1232 MeV.\nThe next \\Delta resonance that we have in the spectrum comes at around 1600 MeV. We can safely assume that it is isolated enough so we can use this kind of amplitude to describe it.\nWe also talked briefly about what kind of width to expect. It’s very different, depending on what kind of states you’re looking at.\n\nIf you are in the low-mass quark region, like we have the \\Delta or the nucleon resonances, there we have widths of over 100 MeV.\nIf we are in the heavy quark sector, then most of them are so narrow that the width is just given by the detector resolution.\n\nSince we have been talking about unitarity, one important thing here is that for a single isolated resonance, the Breit-Wigner description fulfills unitarity.\nUnfortunately, in experiments and in reality, we often have the situation where we have overlapping resonances, and simply summing up two Breit-Wigners, for example, would not give us unitarity. The unitarity condition would then not be fulfilled.\nWhat we do then instead is to basically start from unitarity and construct a matrix called the K-matrix, which fulfills unitarity:\n\nT = K (\\mathbb{I} - i \\rho K)^{-1}\n\nThis is what we want to do next.\n\n\n\n\n\n\nCorrection: The denominator of the amplitude has g^2 times \\rho, which is mass times width, not just \\Gamma. So it’s not \\Gamma over two. It’s M \\Gamma.\n\n\n\nLet’s start with our unitarity condition. We can now bring one of the T’s to the other side and also split this up. So we have here 2i times the identity matrix.\nWhat we get is we bring this here. This part here is what we call the K-matrix N^{-1}. Since there’s an equal sign between this means that K has to be Hermitian.\nIf we want symmetry between going from an initial state to a final state and the other way around, having the final state as the initial state going to it, then we know that K has real values and is symmetric.\nInstead of writing here the transition matrix, we can also write it as the phase space matrix times the invariant matrix and then square root of again phase space matrix.\nWe can also write K in terms of T. In principle, with this kind of definition, either with T or with this invariant matrix M, we can expand this into an infinite series with this K-matrix.\nThis K-matrix describes here this interaction. Here we have also terms where we have some intermediate state here, or more than one, and so on. This can be expanded into this infinite series.\nNow I want to discuss two cases that we have, for example, one single resonance in one channel and how this K-matrix can be then modeled.\nIf you have one resonance and it can only decay into one single channel, this is how we can parameterize it:\n\nK = \\frac{g^2}{M^2 - s}\n\nWe have again coupling constants, the mass of the resonance, and s.\nIf we now plug this into the invariant matrix, the invariant amplitudes—so what we get out if we do this—is we get again the Breit-Wigner amplitude out.\nFor the single resonance case, this Breit-Wigner parametrization gives us also unitarity. This is what we also get out using this K-matrix approach.\nIn general, the width can also depend on s. So that’s not a constant width.\n\n\n0.2 Two-Resonance Dynamics and Unitary Constraints in Scattering Theory\nNow let’s think about single-channel two resonances. Imagine we have this plot with two peaks. What we can see here is that this does not correspond to the sum of two \\pi thresholds. We also don’t want this because it doesn’t fulfill unitarity.\n\nIf the masses of the two resonances are far apart, this part would vanish, and we would have just the sum of two Breit-Wigner peaks:\n\nf(E) = \\sum_{j=1}^2 \\frac{g_j^2}{E - E_j + i\\Gamma_j/2}.\n\nIn this case, they are so far apart that we can consider them isolated. If the masses were the same, we would get just one single peak.\n\n\n\n\n\n\n\nThe Breit-Wigner formula describes isolated resonances: f(E) = \\frac{\\Gamma/2}{(E - E_0) + i\\Gamma/2} For two resonances, we sum their individual contributions when they’re far apart.\n\n\n\n\nWe have only a couple of minutes left. One important thing to remember is that we are looking at:\n\nThe imaginary part of the partial wave\nThe phase shifts\n\nWe expect to see:\n\nA peak in the imaginary part\nIn the real part, we see a phase motion going through zero\n\n\nThe best way to intuitively understand this is to think back to mechanics. In a thought experiment with an external frequency scan, you would observe:\n\nA resonance at a certain eigenfrequency\nA phase motion going through 90 degrees\n\nThis is similar here: for a resonance state, we also see this phase motion.\n\nThis ideal behavior only occurs for an isolated resonance. With overlapping resonances, it becomes much more complicated to determine:\n\nHow many resonances there are\nHow to parameterize them\n\n\nProbably about 5. We already have this from the nuclear top machine. The Argand diagram plots:\n\nThe imaginary part against the real part of F\nForms a circle\n\nWe can examine specific values on this circle.\n\nFor elastic scattering, the energy dependence describes the unitary circle:\n\n(\\text{Re}\\,f)^2 + \\left(\\text{Im}\\,f - \\frac{1}{2}\\right)^2 = \\left(\\frac{1}{2}\\right)^2.\n\nDifferent points on the circle satisfy this equation. For example, at 45 degrees, we would get 1/2 + i/2.\n\nThe circle is centered around (x_0, y_0) with radius R. For non-elastic scattering:\n\nThere is a deviation from this circle, moving inward\nIf unitarity is not fulfilled (as in the case of two-body weakness), the trajectory would go outside the unitary circle"
  },
  {
    "objectID": "2025-Lecture-09.html",
    "href": "2025-Lecture-09.html",
    "title": "(2025) Lecture 9",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Anna Zimmer\n\n0.1 Kinematics of Decay and Scattering: The Källén Function and Dalitz Plot\nLecture number 9. We start with a recap. Let’s discuss the problems:\n\nWho is finished with the first?\nWho is finished with the second? Good.\n\n\nWhat’s the name of the area of the decay that we have here? The usual representation of the decay in the coordinates of the invariants is called the Dalitz plot. But you can use the same type of coordinates for the crossed process, for the scattering process.\n\n\n\n\n\n\nThe Källén function (or triangle function) defines the kinematic boundary for both decay and scattering processes: \n\\lambda(S, T, U) = S^2 + T^2 + U^2 - 2ST - 2SU - 2TU = 0\n\n\n\n\nI don’t want you to know the equation by heart, but rather I want to stress that the same equation would be relevant not just for the Dalitz plot and the decay region, but exactly the same equation would determine the border of the scattering process.\n\nCan anyone tell me what equation that is? It’s the Källén function. The Källén function of S, T, U is equal to zero: \n\\lambda(S, T, U) = 0\n\nThe Källén function is the determinant of the Chew-Low functions. The Chew-Low function has the mass of the decaying particle, the pair mass of the two particles, and then the other one.\n\nIf you want to plot the area, you just look for the solutions in the plane. Is this just common knowledge? It’s easy to derive—either for scattering or for the decay.\nThe fact that it describes both appears once you look at the equations. Essentially, changing one particle to the other side does not change the kinematics. This comes from considering the scattering angle and requiring that the scattering angle is either +1 or -1, which determines the border: \n\\cos \\theta = \\pm 1\n\n\nThe meaning of this scattering angle changes slightly when you go from the scattering domain to the decay domain. But it’s always seen in the rest frame of two particles.\nFor the S channel:\n\nHere’s particle 1,\nHere’s particle 2,\n1, 2, 3.\n\nThis is the cosine theta. The condition for the border is determined by the equations where \\cos \\theta = \\pm 1. It’s the same for the decay and for the scattering.\n\nThe only difference is the direction of the arrow.\n\nFor the decay, two particles collide, and the system you consider is the center of mass for the whole system—that’s for the scattering.\nFor the decay, it’s just a subsystem of the final state.\n\nThe way you derive it: you write the cosine in terms of the invariants, set \\cos \\theta = 1, and arrive at the same equation.\n\n\n\n\n\n\n\nThe scattering angle \\theta in the center-of-mass frame relates to invariants via: \n\\cos \\theta = \\frac{t - u}{\\sqrt{\\lambda(s, m_1^2, m_2^2)}}\n where s, t, u are Mandelstam variables and \\lambda is the Källén function.\n\n\n\n\n\n0.2 Scattering Kinematics, Unitarity, and the Optical Theorem\n\n\n\n\n\n\nKey formulas discussed in this section:\n\nBreakup momentum (phase space factor): \n\\rho = \\frac{1}{16\\pi^2} \\cdot \\frac{2p}{\\sqrt{s}}\n\nUnitarity condition for partial wave amplitude: \n\\text{Im}\\, A_l(s) = \\rho(s) |A_l(s)|^2\n\nOptical theorem: \n\\text{Im}\\, A(s, t=0) = 2\\sqrt{s}\\, p\\, \\sigma_{\\text{tot}}(s)\n\n\n\n\n\nI’m surprised you need scattering. Most of the variables are defined for momentum, and then you use energy conservation. You can introduce Källén functions there—or tuning functions. The channel function is just a breakup momentum. I would consider this once you derive it, once you can consider this as common knowledge. It’s easy to remember, is it not? You have derived this one or you can look it up in the Bjorken and Drell book. I just don’t really see how to derive it.\nThe recipe is you start with the scattering kinematics, and that’s unique because you know that the ranges of the physical region correspond to the cosine going to plus one or minus one. But let’s not get distracted because there’s a lot of material here.\n\nOne thing to realize is that once you input this into Mathematica and ask what are the regions in the complex two-dimensional plane where this equation is satisfied, you don’t get only this—you also get the other regions that correspond to the crossed process. Here, I’m asking what other physical processes are connected by crossing and to draw the regions.\nThe answer here is that the regions will lie somewhere here, and these lines where the Kibble function is zero are the borders of the other process. The process asked to indicate here can be understood by looking at your original kinematics. If this is zero going to 1, 2, 3, that process. Here we define S so that process would correspond to—the variable S that you have here for any place inside the Dalitz plot is a positive quantity, and you want to keep it positive. For this area, it’s bigger but still positive, while the other one becomes negative.\nFor this area, these two particles are still in the physical space, so they are in either the initial state or final state, while the other one gets crossed to the other side. It’s a 0, \\bar{3} scattering into 1, 2. The process we talk about when we discuss this domain is \\pi^+ \\pi^- \\rightarrow \\pi^0 \\omega. Then we put it there by \\pi^+ \\omega \\rightarrow \\pi^0 \\pi^0, and we talk about this domain, but in terms of \\pi^- \\pi^0 \\rightarrow \\omega, what is 1, 2, and 3? One has to be always consistent—this is zero, depending on the axis, right?\nThe \\omega \\pi^0—I call this one, this is two, this is three. So the process here would be zero. But why? For the first process, we moved \\pi^- to the other side, not \\pi^0. Because one axis is \\omega \\pi^0 and another is \\pi^- \\pi^0, right? \\pi^-. No, it’s diagonal. No, it’s \\pi^- \\pi^0. So that one. If there are two \\pi^0s, shouldn’t we move \\pi^0 to the other side?\n\nI argue that way. I have here \\omega and here \\pi^0, and that’s what my channel represents. That’s the threshold for their channel. That’s the threshold for that channel, and that will be the production threshold for the initial state. I always have this S increasing along this axis.\nAre these asymptotes of the hyperbola? Well, yes. I’m asking because in the original Mandelstam paper, they’re not. They try to go over it—I never understood. No, going over is fine, but the question is, do they approach them in the infinite energy region, and that’s where you can neglect the masses? I think so. Asymptotes means not here, and they can cross. But when the energy gets high, do you approach this line? I think so.\nIs it that line or is it slightly offset by the threshold? Let me draw it here—what scattering process we are dealing with. The zero scatters with the particle: 1, 2; 1, 3; 2, 3. The energy is positive. Here is the zero with one bar. The process here is \\pi^+ \\omega \\rightarrow \\pi^- \\pi^0. You see that for the particle you cross to the other side, it becomes an antiparticle. But for \\omega and \\pi^0, particle and antiparticle are the same.\n\nThings get a bit trickier once you take into account that particles have spin. I should have written here \\lambda because \\omega has spin, and then helicities also have peculiar relations with respect to crossing. We don’t talk about that—for kinematic considerations, it’s enough. You can think of particles as scalars. To really relate the amplitudes with helicity indices requires more work.\nDraw regions, indicate process. I think we did draw roughly the regions together. More accurately, you have to open a computational notebook—otherwise, it’s intuitive.\n\n\n\n\n\n\n\nThe optical theorem relates the forward scattering amplitude to the total cross-section: \n\\text{Im}\\, A(s, t=0) = 2\\sqrt{s}\\, p\\, \\sigma_{\\text{tot}}(s)\n This is derived from unitarity and is valid in the elastic region.\n\n\n\nQuestions here quickly: the second unitarity equations. We consider the elastic scattering process. Shall we break this down? What does it mean? What are S and T? Maybe you guys follow—what is A? What is S for the scheme? A is the scattering amplitude, S is Mandelstam S. How do you compute this variable? It’s like P_0, P_1. And T is not a Mandelstam variable related to the—it’s computed by the difference of these two momenta, and then the scattering amplitude.\nI’m saying “process,” but in fact, it’s the scattering amplitude. What does elastic mean here? I don’t think we discussed elastic, but maybe you’ve heard of this. Energy is conserved—energy is always conserved. That we’re doing the same check? Exactly. The initial state and the final state are the same. That’s what I mean. Actually, here it’s somewhat slang—“elastic” is used in different contexts. But when we talk about an elastic process, we usually say the initial state is the same as the final state. Elastic scattering.\nFormulate A using explicit dependence. Formulate partial wave unitarity and the optical theorem. Alex, can you tell me the unitarity equation for A? Can you help me? Like, what should I relate? I don’t know how you’ve introduced amplitudes so far, but I always see that this—you know, normalized with regards to energy, which introduces a phase space factor. So I would say it’s like 1/(1 + i\\rho A).\nSorry, A times—phase space factor times A. Equals 1? Absolutely equals 1. So my first guess would be 1/(1 \\pm i\\rho A). I want to write the unitarity equation for A. How unitarity constrains my A? When you do this consideration, you get constraints on the imaginary part of A, and then the imaginary part of A is related to other amplitudes.\nMaybe I draw it diagrammatically first, and then you help me get the equations. But this is the optical theorem, right? No, that’s the unitarity equation for A.\nWhat is the difference between T, T', and T''? Very good. We are discussing the elastic process, and so S and T are defined. The imaginary part of the amplitude is related to the amplitude squared in the elastic region. This is the unitarity equation—one has to integrate over all intermediate states when you contract one amplitude to another. But since it’s elastic, it’s the same state that’s present here.\nThe same particles—the only difference is that the integration takes into account that the configuration of the particles in the intermediate states could be different. If I consider this process of the initial state scattering to the final state, the intermediate state that appears in unitarity equations—well, essentially what the unitarity equation says is that you have to sum over all intermediate states that appear for the elastic region.\nIt’s just different configurations—there is only one type of particle state present, and therefore the integration or phase space includes only different orientations of the particles. They are the same particles. Once you go to high energy, there will be many more intermediate states that come into the unitarity equations.\nTo clarify—maybe this drawing can clarify the configuration we have to integrate over. We fix the initial state, we fix the final state, and then T is the scattering angle between the initial and final state. We say they form a plane—this sits in the xz-plane. Here is the x-axis, here is the z-axis in the plane. The only variable we have, T, is connected to the scattering angle between the initial and final state.\nThis angle \\theta is connected to T. What appears in the intermediate state is the arbitrary configuration in the space of the vector of the intermediate state here. I call here P_1, P_2; here is Q_3, Q_4; and here is P_3, P_4. There are two scattering processes: first from P to Q, and then from Q to P again. We have to integrate—the plane is fixed, Q points somewhere in space, and we have to integrate over all possible configurations. That’s why T', T'' come in.\nThis is the unitarity equation for the full amplitude. Great simplification. Can you write phase space explicitly? Because T' and T'' are what they should be in the phase space. It’s a regular two-body phase space, and we have to integrate over all possible configurations of one vector and a second vector, but there is a constraint making them back-to-back.\nWriting this in terms of T' and T'' is a highly nontrivial task. The way to proceed—to get our regular unitarity that we know, where the amplitude is related to itself—requires partial wave expansion. The next step would be to do what we discussed last lecture: take each amplitude A here, A here, and A there, and expand them into partial waves.\nThe partial waves would involve the connection between the scattering angle in T or T'', and then we use properties of angular momentum and the angular functions to get rid of this integral essentially. Once we do, this equation becomes very simple. Instead of the integral over phase space, we simply have a phase space factor—it’s not an integral, it’s just an energy-dependent factor.\nSo this \\rho is: \n\\rho = \\frac{1}{16\\pi^2} \\cdot \\frac{2p}{\\sqrt{s}},\n where p is the breakup momentum in the system. It’s a function of energy. And then here A appears—that’s the unitarity equation for the partial wave. The partial wave amplitude has only one dependence, on S, and things drastically simplify. There is no T any longer.\nAnother thing I wanted to point out—coming back here—you see that there is T' and T''. The reason they are different is that T' is measured for the vector Q with respect to P_1. So it’s the amplitude computed between Q_3 and P_1—this vector and that vector. Then T'' is computed between Q_3 and P_3—the intermediate and final state. Either take these vectors or these vectors, and that’s what makes the variables different.\nThere is one limit where these variables become the same: when you have not just elastic scattering but forward scattering. When you have kinematics where you collide the particles and they just pass through—it’s forward scattering. In that case, A is equal to—sorry, T is equal to zero. And in that case, if P_1 = P_4, T' and T'' become the same.\nForget—there are two values for T: T = 0 and T = (some negative value). For this type of scattering, forward and backward—no, for backward, T equals its maximum value, it’s negative. But for forward scattering—you’re right, but it’s a different limit.\nNow I want to discuss the forward limit. In the forward limit, the point is that this amplitude becomes the same as this amplitude, and we can write the squared amplitude already right here.\nMy question was, isn’t it the same for backward as well? No, I don’t think so. The reason is—for forward, we have to consider this is P_1, and this is Q, and this is P_3, and this is Q, right? That’s forward scattering. And that’s why what makes this angle the same as this angle. If you consider backward scattering, P_3 will be in that direction, Q_3 will be this, and the angle is different.\nYou can identify T' and T'' to be the same in backward scattering, but—\nFinishing this, I wanted to give the final expression for the optical theorem that comes from this. It’s nothing else but the full cross-section. Here you have the optical theorem: the amplitude for forward scattering is equal to the flux times the full cross-section.\nIt’s important to acknowledge the fact here that this is valid in the elastic region. Forward scattering is really important because that’s when you identify—that’s how you get rid of one of the variables. The cross-section is a function of S only—the total cross-section doesn’t know anything about angular dependence. So you integrate this, and this happens once you identify two amplitudes to be the same and take into account the phase space.\nThis is from unitarity. This is the first condition written down, and what comes down—the dependence on the cross-section and the flux is the optical theorem for you.\nSo what is what here? The first is unitarity. The second is the optical theorem. I’m not sure we discussed this last time, but the optical theorem comes in quantum mechanics. Here is the analog of this, related to the transition matrix element amplitude we have in the scattering process.\n\n\nUnitarity for A: The imaginary part of the amplitude is related to the amplitude squared in the elastic region.\nPartial wave unitarity: \n\\text{Im}\\, A_l(s) = \\rho(s) |A_l(s)|^2\n Here, A_l(s) depends only on S, and partial waves don’t mix—only the same L appears on both sides.\nOptical theorem: Relates the forward scattering amplitude to the total cross-section: \n\\text{Im}\\, A(s, t=0) = 2\\sqrt{s}\\, p\\, \\sigma_{\\text{tot}}(s)\n\n\nGood. When we discussed the optical theorem last time—so that’s good that we discussed today. It’s important to relate the concepts to each other.\nToday in the lecture, we will go through analytic functions and see how scattering matrix principles are constrained by their properties. How scattering amplitudes are constrained by the properties of the scattering matrix, such as Dyson analyticity and partial analyticity. To consider this first part, I want to make the first part a little more mathematical by discussing analytic functions and real functions. I will say a few words about analytic continuation at the end.\n\n\n0.3 Analyticity and Singularities in Hadron Scattering Amplitudes\nIn hadron physics, we deal with interactions that are not driven by simple Lagrangian or Hamiltonian dynamics. We cannot proceed unless we restrict ourselves to specific near-threshold interactions. We cannot derive the scattering amplitudes from first principles. Instead, a different approach is used.\nWe study general principles of scattering theory, and it appears to be constraining enough to derive the general form of the amplitude, with parametric freedom around branches that could be fixed from the data. Therefore, the approach utilized in hadron physics to describe the scattering of hadrons, especially in experimental data, is to use general parameterizations of the amplitudes that satisfy unitarity and elasticity. Rarely, crossing symmetry can be satisfied—very rarely. Then, we fix the parametric freedom and obtain the scattering amplitude as a mathematical expression to study its properties.\nWhile studying the properties of the scattering amplitude, we obtain properties of the objects we describe—the resonances. So that’s the program of hadron spectroscopy: amplitude building and scattering theory tools to access the properties of the objects. We study resonances, and one of the constraints is analyticity—another one, especially, telling that the function constrains different regions in the variables. If a function depends on s and t, this function must extend in validity beyond the physical region, and extra constraints arise.\n\n\n\n\n\n\n\nKey Insight: The scattering amplitude \\mathcal{A}(s,t) is a complex function of Mandelstam variables s and t, which can be analytically continued into the complex plane. This allows us to study singularities (poles, branch cuts) that reveal physical properties like resonances.\n\n\n\n\nThen, we consider the amplitude as a complex function of its variables—the scattering amplitude. While observables are all real values (the cross-section is a real number), the amplitude itself is a complex number: it has an absolute value and a phase. This, in turn, gives important information about the scattering process.\nAs we saw in the problem we discussed earlier, the amplitude was a function of two variables, s and t. So it’s a multivariable function—a complex multivariable function. And it appears that we will gain some benefits and new insights into the amplitude if we consider it not just as a complex function, but also as a function of complex variables.\nWhat we are going to do, as part of the theoretical exploration, is take our s variable—something that used to be physical—and make it complex. We’ll see what the properties of the amplitude are when we make the energy of the scattering complex, or when we make the angle of the scattering a complex number—not 30 degrees, but 30^\\circ + i. And we’ll see that this gives us new insight.\nWhat’s important for us is not just that the amplitude is a complex function, but also that it’s an analytic function of its arguments everywhere in the complex plane, except for a few minor segments.\n\nBefore going into the details, I would like to discuss analytic functions. So here is the mathematical part of the lecture. I would like to remind you what analytic functions are, or as we also call them, holomorphic functions. Don’t be scared by this word—“holomorphic” simply means analytic.\nA holomorphic or analytic function is one that matches its Taylor series in the vicinity of every point of the complex plane or its domain. If you look in mathematics books, it says an analytic function is one that is complex differentiable everywhere in the complex plane. But we will consider domains of analyticity—the function doesn’t have to be analytic everywhere in the complex plane, just in certain domains. And in that domain, complex differentiability simply means it’s equal to its Taylor series. So there is a polynomial series that approximates the function exactly at every point in the plane or in the vicinity of every point.\nI could remind you what the Taylor series means, but you probably remember that expression with the derivatives—essentially a polynomial. That’s the point. The series can go to infinity, but the function must match it.\nNow let’s discuss examples. Clearly, if required, the function matches the polynomial series. Any polynomial function is analytic in \\mathbb{C}. Polynomials are excellent functions—analytic everywhere—but also simpler functions like \\sqrt{z} or \\log z are analytic everywhere except on a line segment. And this line segment is the branch cut. For every branch cut, we have two branch points. Sometimes one of the branch points is at infinity, and the other is in the finite range. And then the line that connects the branch points is called the branch cut.\n\n\n\n\n\n\n\nBranch Cut Examples:\n\n\\sqrt{z}: Branch cut along (-\\infty, 0]\n\\log z: Branch cut along (-\\infty, 0]\n\\sqrt{z - a}: Branch cut from a to -\\infty\n\\sqrt{(z - a)(z - b)}: Branch cut connecting a and b\n\n\n\n\n\nLet me draw a few examples convenient to discuss. The z-plane, where along the x-axis we have the real part of z and the y-axis is the imaginary part of z. And then for every point I can plot, it is the real part of z plus i times the imaginary part of z. So what I can do using this plot is to indicate where the function is analytic and where it is not. There are regions of analyticity.\nFor example, for a function like \\sqrt{z}, there is this branch point at zero and the branch cut going to the left, indicating that if I compute the function on one side and on the other side, I get different values. I think it’s pretty clear: if I compute \\sqrt{z} at -1 + i\\epsilon, I get +i. And if I compute this from that side, I get -i. It doesn’t happen on that side. If I compute the function at 1 + i\\epsilon, I get 1. And if I compute it from that direction, like 1 - i\\epsilon, I also get 1. But on that side, there is a discontinuity of the function.\nJust to demonstrate that the branch cut doesn’t have to be at 0. For the function \\sqrt{z - 1}, the branch cut starts at 1. We go backwards. For the function \\sqrt{-z}, the branch cut starts at 0 and goes upwards. For the function \\sqrt{(z - 1)z}, the branch cut starts at zero and goes to 1. For the function \\sqrt{z^2 - 1}, the branch cut is more complicated. It starts at 1, goes to 0, then to +\\infty, comes back from -\\infty again to -1. The last example is \\log z. In that case, the branch cut starts at zero and goes backwards.\nAnother class of functions and another type of singularity that is present is the poles. Poles are functions that have poles. They are isolated singularities where a function approaches infinity, complex infinity. For example, \\frac{1}{z - (1 - i)}. When z is equal to 1 - i, we approach the pole of the function; the function is undefined, it’s complex infinity. And then once we go away from this point, the amplitude value R is really high.\nWhat I want to say is that the pole value would be felt even away from it. We will see that all of the features that we see in the scattering, like bumps or cusps, are related to the singularities in the complex plane. The branch points are just doors to other domains of analyticity. The poles are something that makes the strength of the amplitude.\nI was thinking of this nice example of the complex multi-floor house that has many internet routers, and the poles are your routers. If you want to have strong internet, you better sit next to the router, and that provides you a signal. The closer you are to this internet router, the better internet you have—it’s a higher value of the absolute value of the amplitude. If you measure the process and you see that the cross section is large, something must be driving this large cross section. And it’s probably a pole somewhere in the complex plane.\nInteresting things about the cuts—those cuts. One thing to consider is that the cuts are the doors to other domains, every cut. You probably have heard it said that branch points are fixed, but the way you draw the cuts is arbitrary—up to you. I don’t like this way of putting it. But this is another way of saying that the way we draw the cut, you can draw the cut differently by extending the domain.\nThe way I’d like to think of this is that for every cut value, you can come up with different functions. You deal with a function that has a branch point at zero and the cuts towards the left. And the claim is you can come up with a different function, call it f_2, that’s defined in the complex plane such that it’s analytically continuous. The function above the cut analytically continues the domain of analyticity. So I can come up with a different function. In that case, it’s -\\sqrt{z}. But I can tell you the recipe to come up with this f_2 such that if you join the function—you take this domain and that domain—so there are no cuts any longer. The branch point is still there, but the function is analytic in this domain. If I take this and that, one can see this as the way to go beneath the cut. So you can analytically continue the function down. And that’s the perspective that I offer.\nThere is the function defined everywhere in \\mathbb{C} except a finite number of points, except a finite number of segments. And you can compute. If I ask you to compute a value, you can compute the value everywhere. The cut locations are fixed; we don’t touch them. We just come up with a different function f_2 that’s—for a start—it has nothing to do with f. It’s again another function that I define everywhere except a finite number of segments. But what makes us consider them together? The one analytically continues the other.\nIf you come back to this function, \\sqrt{z - 1} - \\sqrt{z}, shouldn’t there be two branches? Two branch cuts. One from 1 to -\\infty, from 0 to -\\infty. Yes, so what is this? This is minus. If it’s a minus, you’re right. But if it’s multiplied, then that’s different.\nFor all of the functions with the cuts, we can discuss what is f_2, what function makes analytic continuation. For any of them, you can ask me a question: What happens? What function would analytically continue the domain of analysis in that direction? And I know the answer. Just minus sign here or here or there. For any way you approach the cut, there should be analytic continuation.\n\n\n0.4 Riemann Sheets, Analytic Continuation, and Real Analyticity in Scattering Amplitudes\nThe other notion, the other way people talk about this, is Riemann sheets. Instead of saying there is one function, a second function, a third function, people say there is just one multivalued function with values on different Riemann sheets. But this is the same as saying there are F_1, F_2, F_3. So F_1 is the value of the function on the first Riemann sheet, F_2 on the second, and so on.\nQuestions? Now, while I’m cleaning this up, can you come up with the analytic continuation? What would F_2 look like here if I want to analytically continue? And here—quickly, what is F_2? What is F_3?\nFor the square root, the other sheet or branch is simply the negative value. You can convince yourself by computing \\sqrt{I + \\epsilon} and \\sqrt{I - \\epsilon} for F_3. They should match if the continuation is correct.\nNow, what is F_2 here? It can’t be the same, because I can demonstrate it with the logarithm. Here, F_1 is \\log(-1 + i\\epsilon), which equals i\\pi. On the other side, \\log(-1 - i\\epsilon) equals -i\\pi. No, it’s actually +i\\pi. We computed this already—it’s -i\\pi. If we add i\\pi, we get the value on that side: F_2 = +i\\pi, ensuring analytic continuation.\n\n\n\n\n\n\nThe logarithm function on different sheets: \nF_1(-1 + i\\epsilon) = \\log(-1 + i\\epsilon) = i\\pi\n \nF_2(-1 - i\\epsilon) = \\log(-1 - i\\epsilon) = -i\\pi\n\n\n\n\nWhy is it Sheet 3 right away? It’s not Sheet 3—it’s just labeling. You can call it 2 or whatever. The function going in that direction is F_3, which analytically continues from below, while F_2 continues from above. This is just my notation.\n\nAn interesting aspect closely related to the scattering amplitude is real analyticity. You can take “real analytic” literally—a function that is analytic and real. But I’d like to extend this definition: a function is real analytic if it has a segment along the real axis where it is real.\nImagine the z-plane with the real x-axis. If F is real for real z, it has peculiar properties. It satisfies the reflection principle. Analyticity is a special type of continuity—once you move away from the real axis, the imaginary part must appear. If the imaginary part grows in the positive direction above the axis, it must grow in the negative direction below. This is the Schwarz reflection principle:\n\nf(z^*) = f(z)^*\n\nThe star denotes complex conjugation—flipping the imaginary part.\nWe started with a function real on a segment. Analytic continuation shows the only way it stops being real is by developing a branch point. The function becomes non-real only when a branch point and cut appear. The imaginary part develops oppositely on either side of the cut: positive on one side, negative on the other. The discontinuity across the cut is twice the imaginary part because they grow in opposite directions.\n\n\\text{Disc}\\, f(z) = 2i\\,\\text{Im}\\, f(z)\n\nNow, the second branch cut opens. For analytic functions, the domain must be open. You can always extend a closed segment to an open one. But once you include an edge, you can’t guarantee smoothness in a small circle around it.\nScattering amplitudes are real analytic functions. For a 2-to-2 scattering amplitude A, we consider it on the Mandelstam plane with S and T. If we fix T (e.g., for 30-degree scattering), A becomes a function of S alone. In the complex S-plane, A has thresholds for the S- and U-channels and a real segment. The Schwarz reflection principle applies, linking the discontinuity to the imaginary part:\n\n\\text{Disc}\\, A(s) = 2i\\,\\text{Im}\\, A(s)\n\nThis connects to unitarity, as the imaginary part relates to the discontinuity.\nAnalyticity helps explain peaks in the amplitude. Branch points on the real axis cause kinks—derivatives don’t exist there. If the function spikes, there might be a pole beneath the cut. By examining F_2, you can trace the origin of such features.\n\nIn multichannel scattering, the amplitude has branch points for all coupled channels. A scattering channel is a subsystem of particles that can participate in initial or final states. Examples include:\n\n\\pi\\pi\nKK\n\\eta\\eta\n\\eta'\\psi\n\\pi\\Lambda\nKN\n\nThese channels scatter into each other and are labeled by indices a, b, c.\nAnother example is baryon scattering: \\Lambda K and \\Xi\\pi. These are coupled channels—their scattering problems are interconnected. The amplitude A_{ab}(s) describes scattering from channel a to b. For example:\n\nA_{\\Xi\\pi \\to \\Xi\\pi}(s), \\quad A_{\\Lambda K \\to \\Lambda K}(s), \\quad A_{\\Xi\\pi \\to \\Lambda K}(s)\n\nAll these amplitudes share the same complex plane with branch points for each channel.\nAway from the elastic region, more scattering possibilities arise. In unitary equations, you don’t just have a \\to a but also cross-channel terms. This reflects the coupled nature of the problem.\n\n\n0.5 Causality, Analyticity, and the Optical Theorem in Scattering Amplitudes\nThat brings me to the end questions. Having dared to use the word causality, I always assume that the answer for causality is connected to analyticity. That forces the amplitude to be real analytic.\nI didn’t prove it—I said it’s given that the amplitude has this form. But that actually originates from the causality principle: you must have only singularities along the real axis and only of the type of cuts and poles. There are no singularities in the complex plane allowed—cuts are not allowed to go upwards. This is the only configuration that’s allowed.\nIt’s because I had to demarcate the location of the poles or just the fact that the function is analytic. The function must be analytic everywhere in the complex plane away from the real axis. That’s what causality tells us.\nBut that seems more like a tool. In the end, you’re just trying to describe physics effects. You get the answer, but you don’t care where it comes from—it comes from probability considerations.\nThis causality seems like a decent thing to say: actions come afterwards. Causes come after actions. But this is always the case. You use microcausality.\nWith this hypothesis, you get disconnected regions, but then you can elevate it to complexify them. That’s what you’re doing here. At some stage, you also mentioned complex scattering. That would mean you make T complex as well. You can relate all these regions, and that’s the monodromy processes which transfer things more than just…\nThe introduction in the Martin experiment suggests that these are all interrelated objects—different amplitudes that look different but are connected.\n\n\n\n\n\n\nWhy complex angles? The function S is a complex function of two variables. It’s complicated to treat them simultaneously. T is almost an angle—it depends on the angle. It’s a function of two variables, and both could be complex.\n\n\n\nWe haven’t discussed this, but causality seems to relate all these different regions. Are you asking how causality acts in the multivariable case? How does it lead to analyticity?\nIn two dimensions, you go to four dimensions because s and t will be complex. Instead of talking about the domain of real analyticity where you have a segment of real values, you need a domain where the function is real and exists.\nWe demonstrated it here: for fixed t (negative, in a scattering region), the distance between branch points is controlled by t. Change t, and they get closer. If you look at a slice, there’s a region where they overlap and no longer have a segment of real analyticity. But since for different t there is a segment, we can discuss real analytic properties and analytically continue to the overlapping region.\nWhy do the branch points move when you change t? The threshold (m_1 + m_2)^2 is fixed, but the other point moves.\n\nNow, the optical theorem. It comes from unitarity. You have a transition matrix element \\langle B | T | A \\rangle, which equals the matrix element because these states don’t know about each other.\nThis is like squeezing in an intermediate state. The identity is the integral over the product of states. In continuous form, this is shorthand for summing over intermediate states C.\nThe optical theorem appears in this form:\n\n\\text{Im}\\, \\mathcal{M}(A \\to A) = \\sum_C \\int d\\Pi_C |\\mathcal{M}(A \\to C)|^2\n\nThis comes from unitarity S^\\dagger S = \\mathbb{1}, or equivalently i(T - T^\\dagger) = T^\\dagger T.\nThe left-hand side has a delta function, and the right-hand side has the phase space factor. The delta function cancels, leaving the basis.\nYou can wrap this into a partial wave expansion:\n\n\\mathcal{M}(s, t) = 16\\pi \\sum_\\ell (2\\ell + 1) a_\\ell(s) P_\\ell(\\cos\\theta)\n\nThe generalized optical theorem comes from Heisenberg and is derived from unitarity.\nThe amplitude \\mathcal{M} is written as \\tilde{\\mathcal{M}} times a kinematic factor. The singularity-free argument suppresses the volume divergence.\nFor less competitive momenta, the imaginary part of the amplitude is proportional to the matrix element squared.\n\nKey points:\n\nCausality restricts singularities to the real axis.\nAnalyticity is enforced by causality.\nThe optical theorem links unitarity and scattering amplitudes.\nPartial wave expansions decompose amplitudes into angular momentum components."
  },
  {
    "objectID": "2025-Lecture-11.html",
    "href": "2025-Lecture-11.html",
    "title": "(2025) Lecture 11",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Anna Zimmer\n\n0.1 Thresholds, Branch Points, and Resonances in ππ Scattering\nThe threshold for ππ scattering is at: \n\\sqrt{s} = 2m_\\pi\n Are there any other open channels? Let’s assume there are no other open channels.\nThe branch point is located at: \ns = (2m_\\pi)^2\n The branch cut should be drawn from the branch point to the right to satisfy causality.\nThere is also a cross channel to this reaction because ππ scattering isn’t the only possible process. If we consider backward scattering in energy, another π can scatter to another π. For example:\n\nπ⁺π⁻ can scatter to π⁺π⁻\nEquivalently, π⁺ can scatter with π⁺ if we rearrange the particles\n\nThis means there is another branch point on the left side from the cross channel.\n\nWhen discussing scattering amplitudes, we visualize a “blob” connecting the initial and final states. For ππ scattering:\n\nWe draw the blob with time moving in one direction\nInitial pions come together, interact, and then scatter out\nThis defines a physical process with well-defined kinematics\n\nFor this process to occur, the total energy must exceed the threshold: \n\\sqrt{s} &gt; 2m_\\pi\n The Mandelstam variables s and t describe the kinematics:\n\ns is positive above threshold\nt is negative for elastic scattering (represents momentum transfer)\n\n\n\n\n\n\n\nThe scattering amplitude is an analytic function of s and t, defined across the entire complex plane - even for unphysical values. This allows us to study processes like π₁π̄₃ → π₂π₄ through crossing symmetry by exchanging s and t.\n\n\n\n\nIn the s-plane:\n\nPositive s: s-channel scattering\nNegative s: t-channel or u-channel contributions\n\nThe amplitude has singularities, but we can analytically continue it to a second Riemann sheet, where resonant states appear. For example:\n\nIn ππ P-wave scattering, the ρ meson appears as a resonance\nIn the quark model, mesons are organized by energy levels (1s, 1p, 2s, etc.) with hyperfine splitting due to spin interactions\nThe ρ meson and pion differ only in their spin wavefunctions\n\nUnlike in electromagnetism (where transitions occur via photon emission), in QCD:\n\nThe ρ meson decays to ππ by emitting another pion\nThis occurs because quark-antiquark pairs can be easily created from the vacuum\n\n\nThe ρ meson appears as a resonance in ππ scattering - when two pions interact, they “feel” the ρ meson, leading to a peak in the cross section. This corresponds to a pole in the complex plane.\nTo fully understand resonances, we must analytically continue the amplitude to the second sheet. The classification includes:\n\nResonances: appear as peaks above threshold\nVirtual states: lie on unphysical sheets below threshold\nBound states: on the physical sheet below threshold\n\nThe cross section σ is related to the amplitude A by: \n\\sigma \\propto (2J + 1) |A_J(s)|^2 \\rho(s)\n where: \n\\rho(s) = \\frac{2k}{\\sqrt{s}}\n is the phase space factor, and k is the breakup momentum: \nk = \\frac{\\sqrt{s-4m_\\pi^2}}{2}\n\n\nFor different states:\n\nResonance: cross section peaks at the resonance mass\nBound state: signal appears below threshold\n\nThe analytic structure of the amplitude determines these features. In summary:\n\nSingularities in the scattering amplitude correspond to physical states\nTheir positions reveal whether they are resonances, virtual states, or bound states\n\n\n\n0.2 Poles, Bound States, and Virtual States in Scattering Amplitudes\nFor the bound state of the virtual state, there should be a peak below threshold. For the virtual state, there should just be a threshold cusp.\nThe peak below the threshold is divergent—it goes to infinity at this point. In contrast, the branch point has poles. The amplitude is undefined, or infinity, at the pole. Therefore, the poles are strong singularities. They truly make the amplitude infinite.\n\n\n\n\n\n\nKey distinction:\n\nBound state pole: E = -E_b (with E_b &gt; 0)\nVirtual state pole: E = -E_v + i0^+ (with E_v &gt; 0)\n\n\n\n\nImagine taking this pole and moving it closer to the physical axis, where measurements are made. The closer it gets, the higher the cross-section value becomes, and the smaller the width. This distance to the pole is called the width of the resonance, \\Gamma. The smaller the width, the narrower the peak, but also the stronger the resonance effect.\n\nE = E_0 - i\\frac{\\Gamma}{2}\n\nwhere E_0 is the resonance energy and \\Gamma is the width.\n\nThis also explains why bound states and virtual states produce enhancements. The pole creates an infinity at a specific point, but since measurements start above the threshold, the observed effect is finite. The physical values are still influenced by the pole’s proximity.\nFor a bound state, the amplitude is infinite below the threshold. However, since measurements occur above the threshold, we observe a finite effect. A bound state is a real particle that can propagate. Once produced, it does not decay—it lives forever because it has no width.\n\n\\sigma(E) \\sim \\frac{1}{(E - E_{\\text{th}} + E_b)^2 + \\Gamma^2/4}\n\n\nReal particles are like poles below the threshold but with a small imaginary part. They are slightly shifted due to other interaction channels. Think of these particles as bound states in the limit where other interactions—like electromagnetic or weak forces—are turned off.\nFor example, if weak interactions are absent, the B meson remains stable because flavor is conserved. In reality, weak interactions cause the B meson to decay. This is why the bound state pole in any amplitude is slightly shifted downward.\n\n\n\n\n\n\nFlavor conservation in strong interactions:  [H_{\\text{strong}}, Q_{\\text{flavor}}] = 0  where Q_{\\text{flavor}} is a flavor quantum number.\n\n\n\n\nThe discussion focuses on pole positions in the complex energy plane and their observable effects (peaks, cusps, and enhancements) in scattering amplitudes and cross-sections. The key distinction is between:\n\nBound states (poles on the real axis below threshold)\nVirtual states (poles on the second Riemann sheet near threshold)\nResonances (poles near the physical region with finite width).\n\nFor virtual states, the threshold cusp behavior is: \n\\sigma(E) \\sim \\frac{1}{\\sqrt{E - E_{\\text{th}}}} \\quad \\text{near } E = E_{\\text{th}}\n\n\n\n0.3 Breakup Momentum Plane and Lattice QCD in Scattering Theory\nAs a main part, another convenient representation of the singularity of the scattering amplitude is the breakup momentum plane. The scattering amplitude is only a function of the breakup momentum P in non-relativistic scattering. When you consider a non-relativistic problem, you don’t have a \\sqrt{s} anywhere. You only deal with P.\nIf you look at the expression for \\rho, where \\rho = \\frac{2P}{\\sqrt{s}}, in the non-relativistic limit this simplifies to \\rho = 2P. You can expand the amplitude near the threshold by expanding it in \\sqrt{s} near the first branch point. This leads to the expression for P:\n\nP = 2\\mu \\sqrt{E - E_{\\text{threshold}}}\n\nThis quantity has a branch point at s = s_{\\text{threshold}} because it’s under a square root, matching the branch point of the amplitude. You can also find the inverse relation and determine how s is related to P.\nThe key point is that if the amplitude is only a function of P, the square root will never appear. There is no \\sqrt{P} in the amplitude—it is actually an analytic function of P.\nWhat we discussed earlier with the first and second sheets now merges into a single complex plane.\n\nThe upper part represents A(P).\nThe lower part represents A_2(P). The real physics corresponds to P being real and positive, which maps to s.\n\n\n\n\n\n\n\nPole positions in the P-plane:\n\nBound state: On the positive imaginary axis (P = i|P|).\nVirtual state: On the negative imaginary axis (P = -i|P|).\nResonances: Complex-conjugate pairs in the lower-half plane (P = \\text{Re}(P) - i\\text{Im}(P)).\n\n\n\n\nLet me quickly sketch where the bound and virtual states are in this representation.\n\nThe bound state sits on the real axis just below the threshold.\nThe virtual state is also on the real axis but above the threshold.\nResonances appear as pairs in the complex plane.\n\nThis is the breakup momentum plane, often used in non-relativistic field theory. It simplifies visualization because all objects (bound states, virtual states, resonances) appear in one picture. However, this representation works as an expansion near the threshold and becomes complicated with higher thresholds or additional branch points.\n\nTo summarize: We’ve discussed scattering theory, widely used in hadron spectroscopy. Unlike field theory, this approach doesn’t start from a Lagrangian but uses general principles of scattering theory—unitarity, analyticity, and crossing symmetry.\n\nResonances are identified as poles of the scattering amplitude.\nThe amplitude is constructed to satisfy these principles and fitted to experimental data.\nOnce fixed, it reveals its analytic structure, including branch points and poles on unphysical sheets.\n\nIn experimental particle physics, we:\n\nConstrain this amplitude using data.\nAnalytically continue it.\nLocate poles.\n\nThe pole position gives the mass and width, while the residue (from the Cauchy integral) quantifies the coupling strength. These values are tabulated in the Particle Data Group for future reference.\nQuestion: What changes if we add spin to the particles? For example, in \\rho \\rho scattering compared to \\pi \\pi scattering? The short answer is: nothing fundamental changes except kinematics.\n\nThe amplitude gains extra kinematic factors at threshold.\nIt becomes a vector of amplitudes due to different helicity states. For \\rho \\rho scattering, there are multiple amplitudes corresponding to different helicity combinations. However, the analytic structure and pole interpretation remain the same.\n\n\nQCD is a gauge theory with an SU(3) gauge group, featuring confinement and asymptotic freedom.\n\nAt high momentum, quarks behave as free particles.\nAt low energy, they confine into hadrons.\n\nThis makes QCD difficult to study analytically because perturbation theory fails—the coupling is too strong at low energies.\nOne powerful method is lattice QCD, where we:\n\nDiscretize spacetime into a grid (e.g., a 2 \\text{ fm} \\times 2 \\text{ fm} \\times 2 \\text{ fm} box).\nSolve the equations numerically.\n\nWe use periodic boundary conditions, meaning the field at one edge matches the field at the opposite edge. This effectively tiles space with identical copies of the box.\nKey parameters:\n\nLattice spacing a (e.g., 0.1 \\text{ fm}).\nBox size L = N \\cdot a.\n\nWe want the lattice:\n\nLarge enough to fit hadrons (e.g., a meson of size \\sim 1 \\text{ fm}).\nSmall enough to avoid interactions with mirrored images.\n\nThe QCD Lagrangian has few parameters:\n\nThe coupling g.\nQuark masses m_f.\n\nOn the lattice, we tune these parameters to match physical observables (e.g., the pion mass m_\\pi). Interestingly, we can explore “alternative universes” by varying quark masses continuously. For example:\n\nA resonance like the \\rho-meson can become a bound state if quark masses are increased.\nThe pion mass follows m_\\pi \\propto \\sqrt{m_q} in the chiral limit.\n\nLattice QCD allows us to study these transitions numerically, providing insights into hadron structure and interactions.\n\n\nLighter quarks (e.g., up/down) make calculations more expensive because the pion mass appears in denominators, increasing uncertainty.\nThe lattice spacing a sets the scale—everything is measured in units of a.\nFiner lattices (smaller a) improve accuracy but raise computational costs.\n\nQuestion: How is quark mass defined on the lattice? It’s dimensionless, expressed as m_q a, where a is the lattice spacing. The optimal choice balances precision and computational feasibility.\n\nThis framework—combining scattering theory and lattice QCD—provides a powerful toolkit for understanding strong interactions. Next, we’ll dive deeper into the technical details of lattice calculations.\n\n\n0.4 Lagrangian, Path Integral, and Euclidean Time in QCD\nThe Lagrangian is a scalar quantity and it’s written as G_{\\mu \\nu} G^{\\mu \\nu}. This is the gluonic tensor energy momentum. This is a gluonic field tensor contracted with another one. All repeated indices are contracted among the gluonic fields. There are eight of them. And then \\mu \\nu everywhere. Here are Lorentz indices. They are relativistic properties, Lorentz group properties of the fields.\n\nThere is gauge invariance hidden here. That is enforced by extending the derivative with the gluonic field. So fermionic fields here are the quarks; they interact with the gluon. And the way they interact is given by the gauge theory through this term that appears in the combination of the quarks and gluons through these terms.\nThe mass term is present explicitly. And then here you see these three parameters or these N parameters of the field. g is one of them, and the quark masses are others. The rest is fixed.\n\nSo we have fields—we have the gluonic fields and the quark fields. When we say fields, what we really mean is that both \\psi and A_\\mu are functions of the point in spacetime. And any physical observable in this can be computed as an expectation value.\nThis is my notation. This is an operator. Let me write: this is an operator that can depend on it. What kind of operators are we talking about? You put together quarks and gluons, and you can measure the expectation of this operator by sandwiching it with the vacuum, and this is something you can observe.\n\n\n\n\n\n\n\nThe expectation value is something you can compute. It’s no longer a function of the coordinates. The operator can depend on the fields like \\psi and A_\\mu.\n\n\n\nThe notion of local and non-local operators is intuitive.\n\nLocal: Evaluated at a single spacetime point.\nNon-local: Evaluated across multiple spacetime points.\n\n\nStarting from Feynman’s ideas, the path integral technique has been developed, and it suggests a way to compute the expectation value of the operator using the functional integral. Let’s spend two minutes discussing what is actually written here, because this expression you might not have seen before, and conceptually this path integral is a difficult object.\nSo here, all is also a function of the fields—we just write explicitly \\psi and \\bar{\\psi}. Do we have a T-product yet? Time-ordered? Yes. And time ordering is needed here. If you have several operators, you have to time-order them. Here we have one. So if you want to proceed and expand this, you will get time-ordered. I don’t want to go into these details in this case.\n\nDid you cover path integrals? Let’s see if all symbols are clear. Let’s start with that. So S is called the action, and this is the integral over all coordinates of the Lagrangian. This action weights different configurations.\nSo this is the complex factor that, for any given configuration of the field—you tell me what A_\\mu in spacetime is, what \\psi and \\bar{\\psi} are—I can tell you what this weight factor is by just computing the Lagrangian integrated over spacetime.\nYou can think of spacetime, and at every point in spacetime, you know what your field is. Then you can compute this integral. You know the factor; you can weigh this configuration.\n\nSo the input to this weighting factor is a map of spacetime—how A_\\mu and \\psi look. You tell me not just a single value; you need to tell me what \\psi and \\bar{\\psi} are at every point in spacetime. Therefore, the input to this expression is the functional form. So what A_\\mu is as a function of spacetime, what \\psi is as a function of spacetime. And what I’m going to integrate over is all possible functional forms. So this is a functional integral.\n\\psi and \\bar{\\psi} are considered to be independent and are described by Grassmann variables (anti-commuting). I have another variable—I’ve been looking at Grassmann variables. It’s so much fun preparing this lecture. Maybe I have time to show you what that means.\n\nBut if you want to compute any expectation value, this is what you have to do. That’s how your expectation value is defined: integrate over all possible configurations of the fields, your operator that depends on the fields, and weight this with the complex factor.\nThis is not practical for many reasons, and one of them is that this factor oscillates rapidly. So it’s a complex function. And if you try to do this on a lattice, you can sample different field configurations, but you quickly realize that this doesn’t converge.\n\nAnd then the trick has been invented by changing real time to imaginary time. So we are going to go from Minkowski time and Minkowski metric to Euclidean time, equating the dimension of time with space. And then you will see that what happens is the action becomes a real number, and this e^{iS} will become just a real weighting factor—a real number saying that this configuration has a probability of 0.1, this configuration has a probability of 0.5.\nSo the trick—and this is super important, essential, the main trick on the lattice—is to compute with Euclidean time rather than Minkowski time. We’re going to change the time, introduce a different variable t_E, defined as t_E = it.\n\nHere is an example for the scalar field of how this transformation works. What I’m trying to demonstrate here is that the Lagrangian changes. So from the QCD Lagrangian, you’re going to get this Euclidean version of the QCD Lagrangian, because the time derivatives are going to pick up an i due to that, and the time derivative term changes sign.\nFor the fermion field, you have to do a little more. The gamma matrices also change, and you have to update them. Well, the derivative here changes, the gamma matrices change. But that’s—I think that’s it.\n\nBut what we are going to write, we find out that the Lagrangian for the action—the action—so what we see here is that for that simple example of the Lagrangian of the scalar field, there is an overall minus sign. So that term becomes a minus sum over four components of the new Euclidean coordinates minus the mass term.\nSo the whole expression is real and positive, and the action that we see here becomes real because of the integration variable. Here we had the action—the Minkowski action had a d^4x, but now time is imaginary. One of the i’s pops out and cancels the i here, and the whole weighting factor becomes real.\nMoreover, since the Lagrangian now is bound to be positive, the weight factor is bound to be less than 1—between 0 and 1. So I think I will.\n\n\n\n\n\n\n\nKey formulas discussed:\n\nGluon field strength tensor: G_{\\mu\\nu} G^{\\mu\\nu}\nEuclidean path integral weight: e^{iS_{\\text{Minkowski}}} \\rightarrow e^{-S_{\\text{Euclidean}}}\nWick rotation: t_E = it, S_{\\text{Minkowski}} \\rightarrow -S_{\\text{Euclidean}}\n\n\n\n\n\n\n0.5 Integrating Fermionic Fields and Quenched Calculations in Lattice QCD\nSo it’s quarter to four, guys. I need ten more minutes. I would really like to show you how we get to the quenched one-page calculations, and we’ll continue next time. I will clarify in a second. Let me just put it on the board.\n\nThe equations—another trick we are going to use is to get rid of the two fermionic fields. In the original equations, we had three types: \\bar{\\psi}, \\psi, and fermionic. Despite being correlated, they are the same field but conjugated. It’s convenient to consider them independent, taking into account that they have to be anticommuting. For this, we use a trick—a gluonic field.\nThe gluonic field turns out to be the one that leads to confinement and is the trickiest one. It’s not easy to get rid of, and it drives thermodynamics. The gluonic field is truly important and is going to remain. But we want to integrate over the fermionic fields.\n\nThe way to do this is to look at the original expression and notice that the fermionic fields are weighted with an exponential factor that has a Gaussian form. This integral is a well-known integral for Grassmann variables. When \\psi and \\bar{\\psi} are anticommuting, this integral is easy to evaluate. What you get is the determinant of M.\nHere, it’s kind of bizarre mathematics because everything here are operators, and you integrate not over \\psi but over all possible functional forms of it. But it works similarly to real variables.\n\n\n\n\n\n\nGrassmann variable anticommutation relation: \n\\{X, Y\\} = XY + YX = 0 \\quad \\Rightarrow \\quad XY = -YX\n \nX^2 = Y^2 = 0\n Exponential of Grassmann variables: \ne^{1 - XY} = 1 - XY\n\n\n\n\nI was trying to convince myself by looking at a simple example of Grassmann variables. Grassmann means they anticommute: XY = -YX. That means X^2 = Y^2 = 0. If you write XYXY and swap Y and X, you get X^2Y^2 = 0. So e^{1 - XY} = 1 - XY. Isn’t it amazing? You expand it just like that—no higher power terms are needed.\nFor Grassmann variables, integration is introduced as differentiation. It’s a definition: \n\\int dX \\, X = 1, \\quad \\int dX \\, 1 = 0\n and \n\\int dX \\, dY \\, e^{-XY} = 1\n If you put a constant A here, it becomes \n\\int dX \\, dY \\, e^{-A XY} = A\n After that, I thought I should spend more time learning about this—it’s fascinating that such a thing exists.\n\nYou didn’t explain how the determinant pops up, right? How from M you get a determinant. There is a formula. You can do this trick for two variables, say x_1, x_2, and even then, you get a determinant. It generalizes to higher dimensions.\n\n\n\n\n\n\nFermionic path integral and determinant: \n\\int \\mathcal{D}\\bar{\\psi} \\mathcal{D}\\psi \\, e^{-\\bar{\\psi} M \\psi} = \\det M\n where M is the fermionic operator (Dirac operator in QCD).\n\n\n\n\nIf we integrate over the gluonic fields, what remains is the integral over all configurations of the quark fields. The operator depends only on the gauge fields, and the determinant represents the effect of virtual quarks. All quarks are integrated, so the only effect is \\bar{Q}Q popping up from the vacuum. This expression only cares about gluons, and quarks only appear in loops.\nNow, important terminology: quenched and unquenched calculations refer to this term. When lattice physicists say “quenched calculations,” it means this term is ignored—no quarks in loops. This universe without quark loops is not so different and provides valuable insights into QCD. It’s also much cheaper computationally—the integral converges faster without this term. Remember: “quenched” means no fermion determinant.\n\n\n\n\n\n\nQuenched approximation: \nZ_{\\text{quenched}} = \\int \\mathcal{D}A \\, e^{-S_{\\text{gluon}}[A]}\n instead of the full (unquenched) partition function: \nZ_{\\text{unquenched}} = \\int \\mathcal{D}A \\, \\det M[A] \\, e^{-S_{\\text{gluon}}[A]}\n where S_{\\text{gluon}}[A] is the gluonic action and \\det M[A] represents virtual quark loops.\n\n\n\n\nThe last thing is how we compute this integral. Naively, you could discretize space and integrate over all lattice values. For a lattice of size 100 \\times 200 (time × space), how many variables are there? The gluon field A_\\mu has 4 × 9 components (plus complex parts). That’s 8,640 variables per site. If you replace DA with integrals over all these, you’d need 10^{10} evaluations. Even with 3 points per dimension, it’s 3^{10^{10}}—impossible to compute in the age of the universe.\nInstead, we use Monte Carlo sampling. Configurations are sampled randomly with a weight, making the calculation feasible. For lattice QCD, discretize spacetime, use Minkowski transformations, and apply Monte Carlo sampling for the weights. This trick also helps eliminate fermions.\n\n\n\n\n\n\nMonte Carlo sampling for lattice QCD: \n\\langle \\mathcal{O} \\rangle \\approx \\frac{1}{N} \\sum_{i=1}^N \\mathcal{O}[A_i]\n where A_i are gauge field configurations sampled with weight e^{-S[A_i]}.\n\n\n\n\nIf you take these five points home, I’d be happy. With that, I’ll take questions and finish here.\n\n\n0.6 Fermionic Action, Gluon Dependence, and Grassmann Variables in QCD\nQuestion: How is it possible that we can split the fermionic and gluon part of the action? If I look at the dimension, I can see that in the long derivative we have terms that give us gluon fields. But then how do we split it?\n\n\n\n\n\n\nThe fermionic part of the action in path integral representation is:  e^{-S_{\\text{fermionic}}} = \\int \\mathcal{D}\\psi \\mathcal{D}\\bar{\\psi} \\, e^{\\int d^4x \\, \\bar{\\psi}(D + M)\\psi} \n\n\n\nWe don’t split it. The fermionic part is this:  e^{-S_{\\text{fermionic}}} = \\int \\mathcal{D}\\psi \\mathcal{D}\\bar{\\psi} \\, e^{\\int d^4x \\, \\bar{\\psi}(D + M)\\psi} \nOnce you integrate over \\psi, the matrix M still depends on the gluon fields.\n\nQuestion: What is the meaning of M? It is A.\nQuestion: But then how did I split my Lagrangian? The full QCD Lagrangian is split as:  \\mathcal{L}_{\\text{QCD}} = \\mathcal{L}_D + \\mathcal{L}_F  Here, \\mathcal{L}_F is still dependent on A.\nQuestion: So M is a function of A? Yes, M is a function of A.\n\nIgnoring D_4, the fermionic part is:  e^{-S_{\\text{fermionic}}} = \\int \\mathcal{D}\\psi \\, e^{\\int d^4x \\, \\bar{\\psi}(D + M)\\psi} \nWhen you discretize this integral, it becomes:  S_{\\text{fermionic}} = \\bar{\\psi} M(A) \\psi  Here, \\psi in the lattice space appears as an inner matrix product. The discretized version of the Dirac operator D + M is M(A).\n\nQuestion: Why is it +M and not -M? The sign change happens when we flip the time derivative. The term D flips as:  \\partial_t \\rightarrow -\\partial_t  The gamma matrices are also flipped differently for space and time. For time, it doesn’t flip, but for space, it flips with an i. The overall effect is that D_4 gets a minus sign.\n\nQuestion: Is this what is called “passing”? These are paths in the gluon field.\nQuestion: Does a limit exist for quenched and unquenched calculations? Do they converge? I don’t see a limit. They are two independent approaches.\n\nQuestion: Historically, how was Grassmann algebra developed? How did it come to physics? It existed independently in mathematics before physics applications. I don’t know who introduced it to physics, but it turns out to be very useful.\nQuestion: Aren’t Grassmann variables limiting? You can’t square them, and series expansions truncate. Yes, but you can still use them to describe fields. It’s a trick, but it works.\n\n\n\n\n\n\nThe Dirac operator with mass term is:  D + M = \\gamma^\\mu(\\partial_\\mu - igA_\\mu) + M  where the sign change in the discretized version occurs due to flipping time derivatives and gamma matrices."
  },
  {
    "objectID": "2025-Lecture-13.html",
    "href": "2025-Lecture-13.html",
    "title": "(2025) Lecture 13",
    "section": "",
    "text": "Presenter: Mikhail Mikhasenko\nNote Taker: Anna Zimmer\n\n0.1 Hadronic Contributions to g-2: Vacuum Polarization and Light-by-Light\nLet’s start with recording.\nWhat are the two terms in g-2 that receive hadronic contributions? The contributions are vacuum polarization and light-by-light.\n\nVacuum polarization.\nLight-by-light.\n\n\nCan you draw the diagram for the vacuum polarization? For light-by-light? I think light-by-light.\nWhat is the vacuum polarization diagram? I would have to look it up. I have a guess, but I’m not sure. What’s your guess?\n\nThe solid lines represent the fermion. Which fermion is that? The fermion in this drawing is the muon. The wavy line is the photon.\nThat diagram shows how the probe—the muon—interacts with the photon. Here, something dresses the photon and changes the vertex. This is the hadronic vacuum polarization (HVP).\n\nThere is also another component—how it fits here. HVP sits here. Another contribution is hadronic light-by-light (HLbL), where hadrons enter.\nIf we expand further, where exactly in HLbL do hadrons enter? Precisely here. It could be a loop of pions or kaons. The particles that matter are those that couple to the photon.\n\nIn contrast, I don’t call it light-by-light if there’s a vertex with three photon legs. Mesons that couple to photons contribute the most—like \\eta and \\eta'. All mesons with a strong two-photon coupling are relevant.\n\n\n\n\n\n\nKey Formulas: The hadronic contributions to g-2 are decomposed as: \na_\\mu = a_\\mu^{\\text{QED}} + a_\\mu^{\\text{EW}} + a_\\mu^{\\text{HVP}} + a_\\mu^{\\text{HLbL}}\n For HVP: \na_\\mu^{\\text{HVP}} = \\left(\\frac{\\alpha}{\\pi}\\right)^2 \\int_{m_\\pi^2}^\\infty \\frac{ds}{s} K(s) R_{\\text{had}}(s)\n For HLbL (simplified): \na_\\mu^{\\text{HLbL}} \\propto \\sum_{M=\\pi^0,\\eta,\\eta',...} \\int d^4q_1 \\, d^4q_2 \\, \\frac{\\mathcal{F}_{M\\gamma^*\\gamma^*}(q_1^2,q_2^2)}{q_1^2 q_2^2 (q_1+q_2)^2}\n\n\n\n\n\nThis is important to realize: hadron physics provides the input for precision physics. Hadronic contributions are critical for g-2, which must be calculated to 10 digits, as shown last time. Experiments measure this, but theoretical calculations rely on understanding these vertices—what happens inside these blocks.\nThese days, there’s less discussion about hadronic light-by-light. The main uncertainty comes from hadronic vacuum polarization.\n\n\n0.2 Time Dependence in Euclidean Correlation Functions and Scattering in Quantum Mechanics with Pöschl-Teller Potential\nSecond question: What’s the time dependence of the correlation functions computed on the lattice once we transform Minkowski space to Euclidean? The expectation between the vacuum and any operator \\mathcal{O}_I for the state that’s not the vacuum as a function of q has an exponentially falling dependence.\nThe time dependence in the Heisenberg representation—the time dependence of the operator—is given by the commutation with the Hamiltonian. We change to Euclidean space, and then the time dependence is given by the exponent: \n\\langle 0 | \\mathcal{O}_I(t) | n \\rangle \\sim e^{-E_n t}.\n\n\n\n\n\n\n\nThis exponential decay in Euclidean time is a key feature of correlation functions in lattice field theory, where E_n represents the energy of the state |n\\rangle.\n\n\n\n\nToday in the lecture, I plan to connect the scattering problem and quantum mechanics. It is the scattering theory that we have discussed during the course, and we will see how resonance phenomena and complex structure emerge in quantum mechanics. Then, we will connect this to the methods used in lattice field theory in a finite box to obtain useful results on the scattering properties of particles.\n\nI’m going to start with the scattering problem in quantum mechanics on the basis of the wave functions. The main equation for scattering is the Hamiltonian operator acting on the wave function, giving an eigenvalue. The energy is the same wave function, and the Hamiltonian is the sum of two terms: \nH = -\\frac{d^2}{dx^2} + V(x),\n where:\n\nThe first term is the kinetic energy (mapping momentum to the derivative of the coordinate).\nThe potential V(x) is a function of the coordinate.\n\nMost of the time in quantum mechanics, we operate in the coordinate space, and the potential in that case is a function of a single variable—the one-dimensional x-coordinate. For simplicity, let’s drop all of the constants; at any moment, we can recover them from dimensionality. That’s why there is no good reason to track them.\nThat’s a differential equation that can be solved numerically. But there are several types of potentials for which there is a closed form for the solution in two dimensions later. I would like to look at one of them to gain insight.\n\nBefore that, let’s discuss what the variables are here. The energy of the system is a fixed parameter in this differential equation. This is something that we have to probe, fix, and then solve. The way to investigate the system with a given potential goes as follows:\n\nWe fix the energy or the breakup momentum in the system because they are one-to-one related to each other.\nBy solving this equation and finding eigenvalues P, we can look at what wave functions satisfy this equation.\n\nBut we can also look at the system differently. The solution won’t be present for certain values of E. You can consider this energy parameter as the parametric variable in this equation and ask: for which energy values does the equation have a solution? With that, we are going to realize what are possible regimes or possible values of the energy of the system.\n\nI look at the different potentials that have a closed-form solution. One of them is the Pöschl-Teller potential: \nV(x) = -U_0 \\text{sech}^2(\\alpha x).\n It’s nice when you have a closed form because then you can quickly visualize the code for this. The claim is that if you use this potential and do an algebraic change of variables, you can arrive at an analytic, almost analytic, closed-form expression for the wave function. The hyperbolic cotangent here is then related to the tangent. Everything is formulated in terms of hyperbolic functions, and the solution is given by the hypergeometric function.\nSpecial functions are not a problem once someone implements them. For any programming language—Mathematica, Python, Julia—you can find implementations of the hypergeometric functions. So we can plot everything.\n\nThe physics is very interesting for this system. That potential has two parameters:\n\nThe depth U_0\nThe size \\alpha of the well\n\n\\alpha controls how wide the well is, and U_0 is how deep it is. Depending on these two parameters, you can have many very nice physics phenomena.\nIn the scattering problem, the general features are the following. Let me also sketch next to this potential possible energies. The equation has solutions only for certain values of energies. Not every value would lead to a satisfactory wave function.\n\nOne of the conditions that makes certain energy values unacceptable is the asymptotic condition. We want our wave function to be finite—no exponential growth. The usual way we solve that is by saying \\psi has the term of the incoming wave and then the reflected term: \n\\psi(x) \\sim e^{ikx} + r e^{-ikx}.\n\n\nThe term with +ikx is the plane wave going in the positive x-direction.\nThe term with -ikx goes in the negative direction.\n\nThat asymptotic form is a good approximation for the well.\nFor that potential, the way to solve is to look at the differential equation and recognize it as a type of special function. But what’s important here is how the energy spectrum comes about. You take this equation, put it into the differential equation, and find the condition for certain k so that the wave function doesn’t grow exponentially at both infinities.\n\nWhy does the exponential appear in the first place? It appears when iK is real, as soon as we consider energy below zero. Energy has to be looked at on the scale of V. Here is zero. The levels will be for the energy positive, and here is the level for the energy negative.\nAs soon as you look at negative energy, K becomes imaginary, and iK becomes real—one of the exponentials blows up. To avoid this asymptotically, we have to limit some terms. That gives the normalization condition for the wave function. We want this to be finite or oscillating.\nYou immediately relate this picture to the energy spectrum. Energy is the lines in the potential. As soon as my energy is above the asymptotic values of the potential, all energy values are allowed. That’s called the continuum of states.\nFor energies below thresholds, the solution exists as bound states. These are bound states in the potential—something with exponential falloff. The wave function for the bound state will have peaks inside the well and exponentially suppressed tails outside.\nThe number of solutions inside the well is determined by an integer N. If the well is very deep (U_0 is high), N can have many values (0, 1, 2, …). As you make the well deeper, there are more states. There is a finite number because levels with energy much lower than the potential cannot exist.\nIf you make the well narrower (\\alpha increases), you can fit more states in it. The energy is related to the maximal energy, which is tied to the potential term: \nE_n = -U_0 + \\left( \\frac{\\hbar^2 \\alpha^2}{2m} \\right) \\left( n + \\frac{1}{2} - \\sqrt{\\frac{2m U_0}{\\hbar^2 \\alpha^2} + \\frac{1}{4}} \\right)^2.\n Energy is always negative here and limited by U_0.\n\nNow I want to look at this picture once again and then transform it to the way we have seen in the scattering theory. We used to write energy on the x-axis—the energy plane—where the continuum was represented by the cut, and bound states appeared as poles.\nIt’s very instructive to map the scattering amplitude in scattering theory to the potential problem. That analytic structure is exactly the same for the transmission and reflection coefficients.\nTo make it accurate, we have to consider a different problem where we have a clear incoming wave, reflected wave, and transmitted wave. For example, with a potential step:\n\nOn the left, an incoming wave 1 + e^{ikx} + r.\nOn the right, a transmitted wave T.\n\nHere, T is the transmission coefficient, and R is the reflection coefficient, with |T|^2 + |R|^2 = 1.\nWhen solving the scattering problem, that’s the setup you would use. You could also reverse it—send the wave from the right. But for symmetric potentials, it’s easier to use symmetric functions (cosine/sine).\nThe role of the scattering amplitudes is now played by these coefficients, which are functions of momentum and energy. We are dealing with the coordinate space x; e^{ikx} is a plane wave, and the scattering amplitude is the coefficient in front that depends on energy.\nWe can ask: what is the analytic structure of our transmission coefficients in the complex plane? We are going to discover exactly that. In the region where there is a continuum, any value of the energy is possible—you have a branch cut. And then there are bound states that are trapped in the well.\n\n\n\n\n\n\nThe analytic structure of scattering amplitudes—branch cuts for continuum states and poles for bound states—is a universal feature in quantum mechanics and quantum field theory. Understanding this helps connect lattice calculations to physical observables.\n\n\n\n\n\n0.3 Scattering Phase Shifts, Virtual States, and Resonances in a Symmetric Potential Well\nWhen we deal with the symmetric potential, instead of using that basis, we can switch to the cosine and sine basis. If we use kx + \\delta_0, then we can use either one or another. Let’s stick to the even wave functions that have positive parity.\n\nNow, I want to give you an intuitive understanding of \\delta_0. \\delta_0 is a scattering phase shift. We will look for the solution with positive energy.\nHere is our potential—that’s the energy that’s fixed. The wave function is given by: \n\\psi(x) \\sim \\cos(kx + \\delta_0).\n This satisfies the equations outside the well asymptotically.\n\nLet’s say the well is localized in the region of radius R. Outside the well, where the potential saturates at zero, the equation: \n\\psi'' + E \\psi = 0\n has a solution given by \\cos(kx + \\delta_0).\nIt’s clear that this is a solution. If you take the second derivative, the k^2 comes out of the cosine, and you still have a cosine with a minus sign. Since k^2 = E, you get exactly the terms.\n\n\n\n\n\n\n\nKey Insight: The phase shift \\delta_0 doesn’t spoil the solution—it’s still a solution, and it reflects, asymptotically, the properties of the potential. Despite the potential being localized, the functions at infinity feel the properties of the potential.\n\n\n\nThat’s the essence of scattering theory: you send a plane wave toward your potential from minus infinity, and then you check the properties of the wave on the other side asymptotically. It comes out as a plane wave, but with a slightly different phase. This difference in phase reflects the properties of the potential.\n\nMaybe this is also a good moment to think about what happens with these poles when we change the potential—that slider we had to move up and down the depth of the well. What happens with these poles? Clearly, they move continuously with the depth of the potential.\n\nWhen we make the potential shallower, these poles move toward the threshold and then disappear there.\nBut in fact, they don’t disappear. They wrap around the branch point and appear on the other analytic sheet.\nThe function, due to this branch cut, has several Riemann sheets.\nWhat they do is flip, go to the non-physical sheet, and become virtual states.\n\n\nAs soon as the potential is in a regime where it’s finite but cannot yet host a bound state, there are only virtual states. If the attraction is not strong enough, there are no bound states in the potential. But still, the attraction could be felt by particles, and that indicates there are virtual states. With a certain depth, there are one, two, and then even more states.\n\nWhat are resonances then? Following this logic, resonances are phenomena where particles with low energy feel the attraction more strongly. At certain energies, we want peaks in the amplitudes. This potential does not have any resonance phenomena. But resonance phenomena could be seen if the probability to pass through the well increases for certain energies.\nFor low energy, the probability to pass or the reflection—let’s think of the reflection coefficient—changes with the energy of the particle. Resonance phenomena correspond to cases where, when you change the energy, the probability of reflection gets lower or higher and then returns to the same value. If you have a strong dependence of the reflection probability on the frequency of the wave, you have a resonance phenomenon.\n\nNow we are ready to put this problem into the box. Solving it in the box is also an academic problem. I’m not sure this potential can be solved in closed form. But what changes is that you impose another condition.\nA particle at the value L has to be the same as the one on the other side, as well as the derivative: \n\\psi(L) = \\psi(-L) \\quad \\text{and} \\quad \\psi'(L) = \\psi'(-L).\n These conditions are strong and change the phenomena dramatically.\n\n\n0.4 Finite-Volume Spectrum and Phase Shifts in Scattering Theory\nThe first condition does nothing. The function is symmetric, so it is already the same on both sides of the lattice.\nBut the second condition is actually important. We are still dealing with energies that are positive. From this boundary condition, we realize that not every energy is possible—only those where E = k^2.\n\n\n\n\n\n\nThe energy-momentum relation E = k^2 is a key constraint here, limiting the allowed energy states.\n\n\n\n\nNow, in the continuum, certain values are forbidden, and only a discrete spectrum appears. Let me first draw it as a line. I will still have a bound state below the threshold crossing, but now even above the threshold, I have a discrete spectrum.\nYou can see how this transition happens. What effectively occurs is that our branch point and the cut starting from zero and extending to positive energies gets replaced by a set of poles. Every allowed value of energy—if it’s isolated—is a pole in the scattering amplitude.\nA bizarre thing happens: we used to have a cut in the complex plane, and now it becomes an infinite number of poles along the real axis.\n\nFor a larger lattice, in the limit L \\to \\infty, we recover the infinite box limit. In this limit, we resemble the infinite volume with a continuous spectrum. Here, I indicate the distance between poles in the region that used to be continuous, and you see they are very close to each other. The distance scales as \\Delta E \\sim \\frac{1}{L}.\n\n\n\n\n\n\nThe scaling \\Delta E \\sim \\frac{1}{L} shows how the discrete spectrum approaches continuity as L grows.\n\n\n\n\nAnother term in this formula I want to discuss is \\delta_0. That term, at infinity, remembers the properties of a potential with infinite range. As you see, this term changes the spectrum—the discrete spectrum we now have in the continuum, or the region that was once continuous.\nThere is a shift, related to the exercise we did earlier. We took our potential, made it flat by setting U = 0; the phase shift \\delta vanished, but \\delta_0 was still present. Then we pulled the potential down, and \\delta_0 became significant.\nWhat you observe is that the energy spectrum—both below and above the threshold—gets adjusted and shifted. This shift is related to the properties of the potential. That’s the key idea for our finite-volume scattering computations.\n\nHere, we have a way to numerically solve the Schrödinger equation and examine the asymptotic values to find \\delta_0. We don’t always need to do this, but it’s critical when the potential is not analytically solvable.\nIf we perform a scattering experiment and only have access to the asymptotic form and energy spectrum, we can deduce the potential’s properties from the spectrum. First, we check the allowed energies without interaction—a simple problem with the solution E_n = \\left( \\frac{2\\pi n}{L} \\right)^2. Then, we numerically determine the energies with interaction. By comparing these two cases, we can deduce \\delta_0.\nNote that \\delta_0 depends on energy. Here, you probe it with different values of k.\n\n\n\n\n\n\nThe phase shift \\delta_0(k) is energy-dependent and connects the finite-volume spectrum to the infinite-volume scattering properties.\n\n\n\n\n\n0.5 Connecting Quantum Scattering with Finite-Box Periodic Systems\nSo far, despite quantum mechanics being a very well-taught course, usually the connections with the material that comes after—quantum field theory and particle physics, or any other courses—are not that easy to grasp. I think it’s important to spell out and discuss all points that are shared and make a connection.\n\n\n\n\n\n\nThe periodic boundary conditions simplify the problem by making the system translationally invariant—only the relative distance between particles matters, not their absolute positions.\n\n\n\nI have an intuitive picture of this scattering in my head. If you think of a finite box with periodic boundary conditions, it’s like having two marbles interacting with each other on a ring. What plays the role of the lattice size is the full circumference of the ring. The variable x is the distance between the two marbles. If there is a potential V(x) with which they interact, that’s precisely the setup we have.\nPeriodic boundary conditions mean it doesn’t matter where they are—what matters is only the distance.\n\n\nSchrödinger equation for this system with periodic boundary conditions: \n\\left[ -\\frac{\\hbar^2}{2m} \\nabla^2 + V(\\mathbf{r}) \\right] \\psi(\\mathbf{r}) = E \\psi(\\mathbf{r})\n where V(\\mathbf{r} + \\mathbf{a}) = V(\\mathbf{r}) for lattice vector \\mathbf{a}.\nFor a short-range interaction (like the “marbles” analogy), the potential can be modeled as: \nV(x) = V_0 \\delta(x - x_0)\n where \\delta(x) is the Dirac delta function.\nThe scattering amplitude for two-particle interactions in a finite box: \nf(k, \\theta) = \\frac{1}{k} \\sum_{\\ell=0}^{\\infty} (2\\ell + 1) e^{i \\delta_\\ell} \\sin \\delta_\\ell P_\\ell (\\cos \\theta)\n where \\delta_\\ell is the phase shift and P_\\ell are Legendre polynomials.\nThe finite-volume quantization condition (e.g., Lüscher’s formula for \\ell = 0): \n\\cot \\delta_\\ell(k) = \\frac{n_\\ell(kL/2\\pi)}{\\sqrt{(kL/2\\pi)^2 - \\ell(\\ell+1)}}\n where L is the box size.\n\n\nThe formulas above tie directly to the intuitive analogy of marbles on a ring, bridging conceptual understanding with mathematical rigor.\n\n\n\n\n\n\nWhen working with periodic boundary conditions, always check for translational symmetry—it often simplifies calculations!\n\n\n\n\nLet me know if you’d like further refinements or clarifications!\n\n\n0.6 Extracting Meson and Baryon Spectra from Lattice Correlation Functions\nThe last thing I would like to do is to properly connect this to what is actually computed on the lattice. At the bottom, the first thing one introduces is the quantity and the main computation.\n\n\n\n\n\n\nThe fundamental quantity computed in lattice QCD is the correlation function between operators, which contains all the spectral information of the theory.\n\n\n\nThe only thing that the lattice computes is the correlation of operators. This is the vacuum expectation value of the product of two operators:\n\nC_{ij}(t) = \\langle 0 | \\mathcal{O}_i(t) \\mathcal{O}_j^\\dagger(0) | 0 \\rangle\n\nHere, i is the index that labels the operator. Say we have 5 of them, or 1, or 2, or 3, or 4. We compute the 5 \\times 5 matrix of the correlation coefficients as a function of time.\n\nThe spectral decomposition of this correlation function is:\n\nC_{ij}(t) = \\sum_{n} \\frac{\\langle 0 | \\mathcal{O}_i | n \\rangle \\langle n | \\mathcal{O}_j^\\dagger | 0 \\rangle}{2E_n} e^{-E_n t}\n\nIt is a combination of exponential functions with coefficients.\n\nTo get the meson spectrum, you would take \\mathcal{O} as the combination of the quark and antiquark fields:\n\n\\mathcal{O}_M(x) = \\bar{q}(x) \\Gamma q(x)\n\nwhere \\Gamma represents Dirac matrices that determine the meson’s quantum numbers.\n\nYou take this operator and compute its correlation as a function of time\nThen extract E, which gives you the energy of the system—all possible energies of the meson system\nThese are the ground states—the mesons\n\n\nPractical example: Do this exercise: take U \\bar{U}, run this correlation on the lattice, and the lowest energy that appears will be the mass of the pion.\n\nAn important aspect to realize is that:\n\nThe higher the energy, the faster the state’s contribution dies\nIf you wait long enough, all higher states’ contributions exponentially drop, leaving only the ground state:\n\n\n\\lim_{t \\to \\infty} C_{ij}(t) \\approx \\frac{\\langle 0 | \\mathcal{O}_i | 0 \\rangle \\langle 0 | \\mathcal{O}_j^\\dagger | 0 \\rangle}{2E_0} e^{-E_0 t}\n\nIn the meson case, this works well. In the baryon case, it’s a problem.\n\nThere is more information stored in these correlations if your operator basis is larger. Consider the generalized eigenvalue problem:\n\nC(t) \\vec{v}_n(t, t_0) = \\lambda_n(t, t_0) C(t_0) \\vec{v}_n(t, t_0)\n\nThis technique:\n\nExtracts exponentials of the higher states\nFor every value of t, you can find an eigenvalue\nThe diagonal elements of the diagonalized correlation matrix give the time dependence of those higher states\nThis optimizes the overlap of different operators to isolate excited states\n\n\n\n0.7 Lattice QCD Spectroscopy: Extracting Meson Spectra and Resonances via Finite-Volume Phase Shifts\nThe eigenvalue problem and that technique are quite different. There is a class of algorithms that makes calculations less noisy related to what we use. For this operator that contracts mesons, the operators we input have different condition numbers.\nIf you want to calculate the spectrum of the light spin-1 mesons, the operators have quantum numbers of 0^+, and you obtain one spectrum. For other quantum numbers, you need different operators.\n\n\n\n\n\n\nThe Hamiltonian diagonalization for the meson spectrum is given by: \nH|\\psi_n\\rangle = E_n|\\psi_n\\rangle\n\n\n\n\nWhat actually happens—without going into details—is that in a finite volume, spin is no longer a good quantum number because you lack full rotational symmetry. The lattice has a reduced symmetry group, not SU(2), so different spins mix.\nIn practice, the entire meson spectrum is computed simultaneously. The matrix has a large dimensionality—hundreds of sectors with overlapping non-diagonal elements between different spins. This yields many eigenenergy values (eigenvalues), which are mapped and presented as meson masses.\n\nThere is one problem with this approach. Most resonances above decay thresholds are not bound states but resonances. Excited mesons like the \\rho meson and higher states are resonances in \\pi-\\pi scattering.\nStrictly speaking, extracting eigenvalues above the two-pion threshold is not reliable. Below the threshold, you measure the pion mass accurately. Above it, you observe two-pion states rather than stable mesons. These extracted values are not trustworthy, so another method is needed to study resonances, such as those in \\pi-\\pi scattering.\nInstead of using two core capacitors, four coil capacitors represent meson-meson combinations. The energy spectrum of these combinations is rich and resembles what we see in a finite box.\n\nWhen two particles are confined in a box, their energy spectrum is discrete above the threshold. The spacing between levels is roughly 2\\pi/L. The deviation of these discrete states from the non-interacting case gives the phase shift \\delta(k). This is how the phase shift is extracted.\nA classic example is \\pi-\\pi scattering in the P-wave. The extracted phase shift shows a resonant behavior, peaking at the \\rho meson mass. For physical quark masses, the pion mass is 140 \\text{ MeV}, and the \\rho meson resonance occurs at 770 \\text{ MeV}, where the phase shift passes \\pi/2.\n\n\n\n\n\n\nThe energy shift between interacting and non-interacting systems is key: \n\\Delta E_n = E_n^{\\text{interacting}} - E_n^{\\text{free}}\n This connects to Lüscher’s formula for phase shifts: \n\\tan\\delta(k) = \\frac{\\pi^{3/2} q}{\\mathcal{Z}_{00}(1;q^2)}, \\quad q = \\frac{kL}{2\\pi}\n\n\n\n\n\nThis is how modern hadron spectroscopy works on the lattice. The method is accurate if all relevant channels are included. The steps are:\n\nConstruct a large basis of operators with different quantum numbers.\nExtract eigenvalues to get the energy spectrum of the interacting system.\nCompare it to the non-interacting spectrum.\nThe difference \\Delta E_n = E_n^{\\text{interacting}} - E_n^{\\text{free}} gives the phase shift \\delta(k).\n\nPlotting \\delta(k) versus the breakup momentum reveals resonances. Experienced theorists can spot resonances just from the spectrum.\nFrom first principles—the QCD Lagrangian—we obtain scattering spectra and resonance properties. Complications arise from coupled channels. Above thresholds like 2\\pi or 3\\pi, additional operators (K\\pi, \\eta', etc.) must be included.\nThe key idea is using the energy shift between interacting and non-interacting systems. This connects to experimental particle physics, where phase shifts and cross-sections are analyzed similarly.\n\nFurther analysis involves parameterizing phase shifts, extracting pole positions, and couplings—the final results shared in the PDG. The K-matrix formalism is often used for coupled-channel cases: \nK = \\tan\\delta \\quad \\text{or} \\quad T = \\frac{K}{1-iK}.\n\nLüscher’s formula links finite-volume energy shifts to infinite-volume phase shifts: \n\\tan\\delta(k) = \\frac{\\pi^{3/2} q}{\\mathcal{Z}_{00}(1;q^2)}, \\quad q = \\frac{kL}{2\\pi},\n where \\mathcal{Z}_{00} is the Lüscher zeta function.\nThis machinery bridges lattice calculations and experimental resonance physics."
  },
  {
    "objectID": "2025-lecture-overview.html",
    "href": "2025-lecture-overview.html",
    "title": "",
    "section": "",
    "text": "0.1 2025-Lecture-01: Introduction to Hadron Physics and Course Logistics\nMain Content:\n\nIntroduction to hadron physics: what it is, its origin, and its role in the early universe.\nTimeline of the universe from the Big Bang to hadronization, with key epochs and physical scales.\nOverview of matter composition, the Standard Model, and the SU(3) group.\nDiscussion of confinement and the formation of hadrons.\nAtomic structure: nucleus, electrons, atomic and nuclear scales.\nThe Standard Model’s structure, its successes, and open questions (e.g., naturalness, unexplained parameters).\nIntroduction to quantum chromodynamics (QCD) and the concept of color charge.\n\nPractical skills:\n\nExplain the timeline of the early universe and identify when hadrons formed.\nDescribe the basic structure of the Standard Model and the role of QCD.\nRelate atomic and nuclear scales and understand the concept of confinement.\n\n\n\n\n0.2 2025-Lecture-02: Classification of Hadrons and the Role of Isospin\nMain Content:\n\nHistorical context: discovery of many new particles (“particle zoo”) and the need for classification.\nIntroduction and mathematical structure of isospin symmetry (SU(2)), its analogy to spin, and its role in strong interactions.\nConstruction of isospin multiplets for mesons and baryons.\nIntroduction of strangeness as a quantum number and its conservation in strong/weak interactions.\nGell-Mann–Nishijima formula relating charge, isospin, and hypercharge.\nThe Eightfold Way: SU(3) flavor symmetry, baryon and meson multiplets, and the prediction/discovery of new particles.\nUse of group theory in organizing hadrons.\n\nPractical skills:\n\nClassify hadrons using isospin and strangeness.\nApply the Gell-Mann–Nishijima formula to relate charge, isospin, and hypercharge.\nIdentify multiplet structures and predict missing particles.\nSolve problems involving the construction of isospin multiplets and conservation laws in reactions.\n\n\n\n\n0.3 2025-Lecture-03: Symmetry, Hadron Structure, and Isospin in Meson and Baryon Systems\nMain Content:\n\nDeeper exploration of symmetry in hadron physics.\nExperimental evidence for hadron substructure (quark model).\nCalculation of cross sections and kinematics in scattering.\nForm factors and their relation to hadron properties (charge distributions, magnetic moments).\nSpin-flavor wave functions for nucleons and their observables.\nIsospin assignments for various hadrons, including those with heavy quarks.\nDecay analysis and combinatorics for multi-particle final states.\nIntroduction to deep inelastic scattering as a probe of hadron structure.\n\nPractical skills:\n\nCalculate cross sections and analyze kinematics for simple scattering processes.\nUse form factors to extract information about hadron structure.\nAssign isospin to hadrons and analyze decay channels.\n\n\n\n\n0.4 2025-Lecture-04: Electron-Photon Scattering, Form Factors, and Baryon Magnetic Moments\nMain Content:\n\nReview of electron-photon scattering and key cross sections (Rosenfeld, Mott, Rosenbluth).\nIntroduction to form factors and their physical interpretation (Fourier transform of charge density).\nExtraction of proton and neutron radii from scattering data.\nCalculation of baryon magnetic moments in the quark model.\nSpin-flavor symmetry and SU(6) multiplets.\nLight baryon spectroscopy: radial and orbital excitations, mass differences, and experimental methods for hadron production.\nOverview of experimental facilities and production mechanisms.\n\nPractical skills:\n\nAnalyze electron scattering data to extract form factors and radii.\nCalculate magnetic moments of baryons using the quark model.\nClassify baryon states by spin, flavor, and excitation.\n\n\n\n\n0.5 2025-Lecture-05: Production Mechanisms and Kinematic Analysis in Hadron Spectroscopy\nMain Content:\n\nOverview of hadron production mechanisms: fixed target vs. collider experiments.\nKinematics of particle production and decay, including invariant mass and phase space.\nExclusive vs. inclusive processes in experiments.\nPractical aspects of experimental setups (targets, beams, safety).\nUse of bremsstrahlung for photon production.\nAnalysis of strangeness production and gluon distributions in the proton.\n\nPractical skills:\n\nDistinguish between exclusive and inclusive processes.\nAnalyze kinematic setups for different experimental configurations.\nCalculate phase space and invariant mass for multi-particle final states.\n\n\n\n\n0.6 2025-Lecture-07: Helicity Formalism, Weak Decays, and Isospin Violation in Λ⁰ Decay\nMain Content:\n\nHelicity formalism and partial wave expansion.\nMandelstam plane and analyticity in scattering amplitudes.\nWeak decays (e.g., Λ⁰ → p + π⁻), conservation and violation of isospin and parity.\nDalitz plot analysis for multi-body decays.\nConservation laws and resonance identification in decay processes.\nIsospin operators and rotations in isospin space.\n\nPractical skills:\n\nApply helicity and partial wave formalism to decay and scattering problems.\nAnalyze weak decays and identify conservation law violations.\nUse Dalitz plots to interpret decay kinematics and resonance structures.\n\n\n\n\n0.7 2025-Lecture-08: Scattering Matrix, Unitarity, and the K-Matrix Formalism\nMain Content:\n\nScattering matrix (S-matrix) and transition matrix (T).\nUnitarity and conservation of probability in quantum mechanics.\nPhase space integrals and their role in scattering.\nPartial wave expansion and Legendre polynomials.\nResonance description: Breit-Wigner and K-matrix formalism.\nUnitarity constraints for overlapping resonances.\n\nPractical skills:\n\nWrite and manipulate the S-matrix and T-matrix for scattering processes.\nApply unitarity constraints to scattering amplitudes.\nUse partial wave expansion to analyze angular distributions.\nSolve problems involving resonance parameterization and unitarity in scattering.\n\n\n\n\n0.8 2025-Lecture-09: Kinematics of Decay and Scattering: The Källén Function and Dalitz Plot\nMain Content:\n\nKinematic boundaries for decay and scattering: Dalitz plot and Källén function.\nMandelstam variables and their geometric interpretation.\nScattering angle and its relation to invariants.\nOptical theorem and unitarity in elastic scattering.\nCrossing symmetry and analytic continuation in scattering amplitudes.\nIntroduction to lattice QCD and its role in studying hadron properties.\n\nPractical skills:\n\nUse the Källén function to determine kinematic boundaries.\nApply the optical theorem and unitarity to scattering problems.\nInterpret Dalitz plots for both decay and scattering.\nSolve problems involving kinematic analysis and basic lattice QCD concepts.\n\n\n\n\n0.9 2025-Lecture-11: Thresholds, Branch Points, and Resonances in ππ Scattering\nMain Content:\n\nThresholds and branch points in scattering amplitudes.\nAnalytic structure of amplitudes: Riemann sheets, poles, and branch cuts.\nResonances, virtual states, and bound states in the complex plane.\nCross section and phase space factors.\nLattice QCD: discretization, boundary conditions, and parameter tuning.\nPhysical interpretation of pole positions and their relation to observables.\n\nPractical skills:\n\nAnalyze the analytic structure of scattering amplitudes.\nDistinguish between resonances, bound states, and virtual states.\nUse lattice QCD concepts to interpret hadron properties.\nSolve problems involving analytic continuation and resonance identification.\n\n\n\n\n0.10 2025-Lecture-13: Hadronic Contributions to (g-2): Vacuum Polarization and Light-by-Light\nMain Content:\n\nHadronic contributions to the muon anomalous magnetic moment (g-2): vacuum polarization and light-by-light scattering.\nFeynman diagrams for hadronic corrections.\nImportance of hadron physics in precision measurements.\nTime dependence of correlation functions in Euclidean space (lattice QCD).\nScattering theory in quantum mechanics: potential wells, bound states, and phase shifts.\nAnalytic structure of scattering amplitudes and their connection to quantum mechanics.\n\nPractical skills:\n\nIdentify and draw Feynman diagrams for hadronic contributions to g-2.\nUnderstand the role of hadronic uncertainties in precision physics.\nAnalyze time dependence in lattice QCD correlation functions.\nSolve problems involving quantum mechanical scattering, bound states, and phase shifts."
  }
]